[
  {
    "objectID": "posts/2023-02-11-iv-risk-ratio/index.html",
    "href": "posts/2023-02-11-iv-risk-ratio/index.html",
    "title": "Causal Risk Ratios in AB Tests with One Sided Non-Compliance",
    "section": "",
    "text": "Sometimes we run AB tests where engaging with the treatment (and hence being treated) is optional. Since assignment to treatment is random, we can use the randomization as an instrumental variable (assuming there is a strong correlation between the instrument and the treatment, and that there are no backdoor paths between randomization and the final outcome).\nThere are a few libraries to do estimate the LATE from an instrumental variable. However, none of them report a causal risk ratio, which is usually our choice of causal contrast for better or worse.\nIn this post, I’m putting together some much appreciated advice from @guilhermejd1 and dimitry on cross validated on how to estimate a causal risk ratio in a randomized experiment with one sided non-compliance. I’ll first demonstrate how to do this with a simulation where we will actually have potential outcomes with which to compare. Then, I’ll apply this approach to a real experiment I helped run at Zapier."
  },
  {
    "objectID": "posts/2023-02-11-iv-risk-ratio/index.html#simulation",
    "href": "posts/2023-02-11-iv-risk-ratio/index.html#simulation",
    "title": "Causal Risk Ratios in AB Tests with One Sided Non-Compliance",
    "section": "Simulation",
    "text": "Simulation\nLet \\(A\\) be a binary indicator for assignment to treatment (1) or control (0). Let \\(d_j^a\\) be the potential outcome for engaging with the treatment under treatment \\(A=a\\) for user \\(j\\). Due to construction of the experiment, \\(d_j^0 = 0 \\forall j\\) because users in control can not engage in the treatment by design. Dimitry writes\n\n“This is different than the typical experiment in labor economics, where people can take up job training somewhere else even if they are in the control group”.\n\nThis means that we have two types of users in treatment group: \\(d_j^1 = 1\\) is a “complier” and \\(d_j^1 = 0\\) is a “never taker”. If our outcome is \\(y\\), then LATE is the ATE for compliers and is also the ATT.\nLet’s set up a simulation. Here are some details:\n\n\\(A\\) is decided by a coinflip (i.e. bernoulli with probability of success 0.5)\nThere is an unmeasured confounder \\(w\\), which is also distributed in the population via a coinflip.\nThe confounder and the treatment effect the probability of engaging with the treatment (being a complier). \\(P\\left(d_j^1 = 1 \\mid A=1, w\\right) = 0.4 - 0.2w\\). Because of the one sided compliance, \\(P(d_j^0=1) = 0\\).\nProbability of the outcome is \\(P\\left( y^a_j=1 \\mid d^{a}_j, w \\right) = 0.1 + 0.5w + 0.1d_j^{a}\\). So the instrument only effects the outcome through compliance.\n\nLet’s simuilate this in R\n\nlibrary(tidyverse)\nlibrary(kableExtra)\nmy_blue <- rgb(45/250, 62/250, 80/250, 1)\ntheme_set(theme_classic()) \n\nupdate_geom_defaults(\"point\",   list(fill = my_blue, shape=21, color = 'black'))\n\n\nset.seed(0)\nN<- as.integer(1e6) # Lots of precision\nA <- rbinom(N, 1, 0.5)\nw <- rbinom(N, 1, 0.5)\n\n# Potential outcomes\nd_0 <- 0\nd_1 <- rbinom(N, 1, 0.3 - 0.2*w + 0.1*A)\ny_0 <- rbinom(N, 1, 0.1 + 0.5*w)\ny_1 <- rbinom(N, 1, 0.1 + 0.5*w + 0.1*d_1)\n\n# and now the observed data via switching equation\ny <- A*y_1 + (1-A)*y_0\nd <- A*d_1 + (1-A)*d_0\ncomplier <- d==1\n\nFrom our setup, \\(LATE = 0.1\\). Let’s compute that from our potential outcomes and estimate it using \\(A\\) as an instrument.\n\nsample_late <- mean(y_1[d_1==1]) - mean(y_0[d_1==1])\nest_late <- cov(y, A)/cov(d, A)\n\n\n\n\n\n\n\n\nTrue LATE\n\n\nSample LATE\n\n\nIV Estimate of LATE\n\n\n\n\n\n\n0.1\n\n\n0.101\n\n\n0.103\n\n\n\n\n\nTrue LATE, sample LATE, and estimated LATE. All 3 agree to within 3 decimal places and any differences are just sampling variability."
  },
  {
    "objectID": "posts/2023-02-11-iv-risk-ratio/index.html#estimating-the-causal-risk-ratio",
    "href": "posts/2023-02-11-iv-risk-ratio/index.html#estimating-the-causal-risk-ratio",
    "title": "Causal Risk Ratios in AB Tests with One Sided Non-Compliance",
    "section": "Estimating The Causal Risk Ratio",
    "text": "Estimating The Causal Risk Ratio\nIn order to compute the causal risk ratio we need two quantities:\n\nAn estimate of \\(E[y^1_j \\mid d^1]\\), and\nAn estimate of \\(E[y^0_j \\mid d^1]\\).\n\n\\(E[y^1_j \\mid d^1]\\) is easy to estimate; just compute the average outcome of those users in treatment who engaged with the treatment. Now because \\(LATE = E[y^1_j \\mid d^1] - E[y^0_j \\mid d^1]\\), the second estimate we need is obtained from some algebra.\n\n# Estimate from the data\nE_y1 <- mean(y[d==1])\nE_y0 <- E_y1 - est_late # use the estimate, not the truth\n\nE_y1/E_y0\n\n[1] 1.389267\n\n\nLet’s compare this to the true estimate of the causal risk ratio using potential outcomes\n\nmean(y_1[complier])/mean(y_0[complier])\n\n[1] 1.381259\n\n\nWhich is pretty damn close."
  },
  {
    "objectID": "posts/2023-02-11-iv-risk-ratio/index.html#oh-shit-i-forgot-a-bound",
    "href": "posts/2023-02-11-iv-risk-ratio/index.html#oh-shit-i-forgot-a-bound",
    "title": "Causal Risk Ratios in AB Tests with One Sided Non-Compliance",
    "section": "Oh Shit, I Forgot a Bound",
    "text": "Oh Shit, I Forgot a Bound\nNote that there is no bound on the LATE because it is estimated via OLS (sure, there are realistic bounds on how big this can be, but OLS isn’t enforcing those). In particular, what if \\(LATE = E[y^1 \\mid d^1]\\)? Then the denominator of the causal risk ratio would be 0. That’s…bad.\nMore over, what if \\(LATE \\approx E[y^1 \\mid d^1]\\) so that the denominator was really small? Then the causal risk ratio would basically blow up (that’s a technical term for “embiggen”).\nThe only reason I bring this up is because it happens in this example. Let’s bootstrap the estimated causal risk ratio (what we call “lift”) and look at the distribuion of bootstrap replicates.\n\n\n\n\n\nLMAO look at that tail! The long tail us due to the problems I’ve highlighted. In fact, we can highlight the “Oh shit zone” on a plot of the bootstraps (below in the figure below). The red line is where the tail behavior comes from; if you have a bootstrap replicate on that line, you should be saying “oh shit”.\n\n\n\n\n\nIn fact, there are some estimates from the bootstrap which yield \\(E[y^0\\mid d^1]<0\\) so… what was the point if this?"
  },
  {
    "objectID": "posts/2023-02-11-iv-risk-ratio/index.html#what-was-the-point-of-this-post",
    "href": "posts/2023-02-11-iv-risk-ratio/index.html#what-was-the-point-of-this-post",
    "title": "Causal Risk Ratios in AB Tests with One Sided Non-Compliance",
    "section": "What Was The Point Of This Post",
    "text": "What Was The Point Of This Post\nOk, so estimating the causal risk ratio in a randomized experiment with one sided non-compliance is technically possible, but the math can get…weird. In particular, bootstrapping the standard errors (which is probably the most sensible way of estimating the standard errors unless you’re a glutton for delta method punishment) shows that we can get non-nonsensical bootstrapped estimates of the counterfacutal average outcome for compliers.\nHonestly…I’m not sure where to go from here. Point estimates are possible but incomplete. Bootstrapping is a sanest way to get standard errors, but have no way of ensuring the estimates are bounded appropriately. All is not lost, its nice to know this sort of thing can happen. Maybe the most sensible thing to say here is “do not ask for causal risk ratios for these types of experiments” and that is worth its weight in gold."
  },
  {
    "objectID": "posts/2019-05-21-odes/index.html",
    "href": "posts/2019-05-21-odes/index.html",
    "title": "Gradient Descent with ODEs",
    "section": "",
    "text": "Gradient descent usually isn’t used to fit Ordinary Differential Equations (ODEs) to data (at least, that isn’t how the Applied Mathematics departments to which I have been a part have done it). Nevertheless, that doesn’t mean that it can’t be done. For some of my recent GSoC work, I’ve been investigating how to compute gradients of solutions to ODEs without access to the solution’s analytical form. In this blog post, I describe how these gradients can be computed and how they can be used to fit ODEs to synchronous data with gradient descent."
  },
  {
    "objectID": "posts/2019-05-21-odes/index.html#up-to-speed-with-odes",
    "href": "posts/2019-05-21-odes/index.html#up-to-speed-with-odes",
    "title": "Gradient Descent with ODEs",
    "section": "Up To Speed With ODEs",
    "text": "Up To Speed With ODEs\nI realize not everyone might have studied ODEs. Here is everything you need to know:\nA differential equation relates an unknown function \\(y \\in \\mathbb{R}^n\\) to it’s own derivative through a function \\(f: \\mathbb{R}^n \\times \\mathbb{R} \\times \\mathbb{R}^m \\rightarrow \\mathbb{R}^n\\), which also depends on time \\(t \\in \\mathbb{R}\\) and possibly a set of parameters \\(\\theta \\in \\mathbb{R}^m\\). We usually write ODEs as\n\\[y' = f(y,t,\\theta) \\quad y(t_0) = y_0\\]\nHere, we refer to the vector \\(y\\) as “the system”, since the ODE above really defines a system of equations. The problem is usually equipped with an initial state of the system \\(y(t_0) = y_0\\) from which the system evolves forward in \\(t\\). Solutions to ODEs in analytic form are often very hard if not impossible, so most of the time we just numerically approximate the solution. It doesn’t matter how this is done because numerical integration is not the point of this post. If you’re interested, look up the class of Runge-Kutta methods."
  },
  {
    "objectID": "posts/2019-05-21-odes/index.html#computing-gradients-for-odes",
    "href": "posts/2019-05-21-odes/index.html#computing-gradients-for-odes",
    "title": "Gradient Descent with ODEs",
    "section": "Computing Gradients for ODEs",
    "text": "Computing Gradients for ODEs\nIn this section, I’m going to be using derivative notation rather than \\(\\nabla\\) for gradients. I think it is less ambiguous.\nIf we want to fit an ODE model to data by minimizing some loss function \\(\\mathcal{L}\\), then gradient descent looks like\n\\[ \\theta_{n+1} = \\theta_n - \\alpha \\dfrac{\\partial \\mathcal{L}}{\\partial \\theta} \\]\nIn order to compute the gradient of the loss, we need the gradient of the solution, \\(y\\), with respect to \\(\\theta\\). The gradient of the solution is the hard part here because it can not be computed (a) analytically (because analytic solutions are hard AF), or (b) through automatic differentiation without differentiating through the numerical integration of our ODE (which seems computationally wasteful).\nThankfully, years of research into ODEs yields a way to do this (that is not the adjoint method. Surprise! You thought I was going to say the adjoint method didn’t you?). Forward mode sensitivity analysis calculates gradients by extending the ODE system to include the following equations:\n\\[ \\dfrac{d}{dt}\\left( \\dfrac{\\partial y}{\\partial \\theta} \\right) = \\mathcal{J}_f \\dfrac{\\partial y}{\\partial \\theta} +\\dfrac{\\partial f}{\\partial \\theta} \\]\nHere, \\(\\mathcal{J}\\) is the Jacobian of \\(f\\) with respect to \\(y\\). The forward sensitivity analysis is just another differential equation (see how it relates the derivative of the unknown \\(\\partial y / \\partial \\theta\\) to itself?)! In order to compute the gradient of \\(y\\) with respect to \\(\\theta\\) at time \\(t_i\\), we compute\n\\[ \\dfrac{\\partial y}{\\partial \\theta} = \\int_{t_0}^{t_i} \\mathcal{J}_f \\dfrac{\\partial y}{\\partial \\theta} + \\dfrac{\\partial f}{\\partial \\theta} \\, dt \\]\nI know this looks scary, but since forward mode sensitivities are just ODEs, we actually just get this from what we can consider to be a black box\n\\[\\dfrac{\\partial y}{\\partial \\theta} = \\operatorname{BlackBox}(f(y,t,\\theta), t_0, y_0, \\theta)\\]\nSo now that we have our gradient in hand, we can use the chain rule to write\n\\[\\dfrac{\\partial \\mathcal{L}}{\\partial \\theta} =\\dfrac{\\partial \\mathcal{L}}{\\partial y} \\dfrac{\\partial y}{\\partial \\theta} \\]\nWe can use automatic differentiation to compute \\(\\dfrac{\\partial \\mathcal{L}}{\\partial y}\\).\nOK, so that is some math (interesting to me, maybe not so much to you). Let’s actually implement this in python."
  },
  {
    "objectID": "posts/2019-05-21-odes/index.html#gradient-descent-for-the-sir-model",
    "href": "posts/2019-05-21-odes/index.html#gradient-descent-for-the-sir-model",
    "title": "Gradient Descent with ODEs",
    "section": "Gradient Descent for the SIR Model",
    "text": "Gradient Descent for the SIR Model\nThe SIR model is a set of differential equations which govern how a disease spreads through a homogeneously mixed closed populations. I could write an entire thesis on this model and its various extensions (in fact, I have), so I’ll let you read about those on your free time.\nThe system, shown below, is parameterized by a single parameter:\n\\[ \\dfrac{dS}{dt} = -\\theta SI \\quad S(0) = 0.99 \\]\n\\[ \\dfrac{dI}{dt} = \\theta SI - I \\quad I(0) = 0.01 \\]\nLet’s define the system, the appropriate derivatives, generate some observations and fit \\(\\theta\\) using gradient descent. Here si what you’ll need to get started:\nimport autograd\nfrom autograd.builtins import tuple\nimport autograd.numpy as np\n\n#Import ode solver and rename as BlackBox for consistency with blog\nfrom scipy.integrate import odeint as BlackBox\nimport matplotlib.pyplot as plt\nLet’s then define the ODE system\ndef f(y,t,theta):\n    '''Function describing dynamics of the system'''\n    S,I = y\n    ds = -theta*S*I\n    di = theta*S*I - I\n\n    return np.array([ds,di])\nand take appropriate derivatives\n#Jacobian wrt y\nJ = autograd.jacobian(f,argnum=0)\n#Gradient wrt theta\ngrad_f_theta = autograd.jacobian(f,argnum=2)\nNext, we’ll define the augmented system (that is, the ODE plus the sensitivities).\ndef ODESYS(Y,t,theta):\n\n    #Y will be length 4.\n    #Y[0], Y[1] are the ODEs\n    #Y[2], Y[3] are the sensitivities\n\n    #ODE\n    dy_dt = f(Y[0:2],t,theta)\n    #Sensitivities\n    grad_y_theta = J(Y[:2],t,theta)@Y[-2::] + grad_f_theta(Y[:2],t,theta)\n\n    return np.concatenate([dy_dt,grad_y_theta])\nWe’ll optimize the \\(L_2\\) norm of the error\ndef Cost(y_obs):\n    def cost(Y):\n        '''Squared Error Loss'''\n        n = y_obs.shape[0]\n        err = np.linalg.norm(y_obs - Y, 2, axis = 1)\n\n        return np.sum(err)/n\n\n    return cost\nCreate some observations from which to fit\n\nnp.random.seed(19920908)\n## Generate Data\n#Initial Condition\nY0 = np.array([0.99,0.01, 0.0, 0.0])\n#Space to compute solutions\nt = np.linspace(0,5,101)\n#True param value\ntheta = 5.5\n\nsol = BlackBox(ODESYS, y0 = Y0, t = t, args = tuple([theta]))\n\n#Corupt the observations with noise\ny_obs = sol[:,:2] + np.random.normal(0,0.05,size = sol[:,:2].shape)\n\nplt.scatter(t,y_obs[:,0], marker = '.', alpha = 0.5, label = 'S')\nplt.scatter(t,y_obs[:,1], marker = '.', alpha = 0.5, label = 'I')\n\n\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7f8e9d0b3d30>\n\n\n\n\n\nPerform Gradient Descent\n\ntheta_iter = 1.5\ncost = Cost(y_obs[:,:2])\ngrad_C = autograd.grad(cost)\n\nmaxiter = 100\nlearning_rate = 1 #Big steps\nfor i in range(maxiter):\n\n    sol = BlackBox(ODESYS,y0 = Y0, t = t, args = tuple([theta_iter]))\n\n    Y = sol[:,:2]\n\n    theta_iter -=learning_rate*(grad_C(Y)*sol[:,-2:]).sum()\n\n    if i%10==0:\n        print(\"Theta estimate: \", theta_iter)\n\nTheta estimate:  1.697027594337629\n\n\nTheta estimate:  3.9189060278370365\n\n\nTheta estimate:  4.810038385538704\n\n\nTheta estimate:  5.251499985105974\n\n\nTheta estimate:  5.427206219478129\n\n\nTheta estimate:  5.46957706068474\n\n\nTheta estimate:  5.47744643541383\n\n\nTheta estimate:  5.4792194685272095\n\n\nTheta estimate:  5.479636817124458\n\n\nTheta estimate:  5.47973599525063\n\n\nAnd lastly, compare our fitted curves to the true curves\n\nsol = BlackBox(ODESYS, y0 = Y0, t = t, args = tuple([theta_iter]))\ntrue_sol = BlackBox(ODESYS, y0 = Y0, t = t, args = tuple([theta]))\n\n\nplt.plot(t,sol[:,0], label = 'S', color = 'C0', linewidth = 5)\nplt.plot(t,sol[:,1], label = 'I', color = 'C1', linewidth = 5)\n\nplt.scatter(t,y_obs[:,0], marker = '.', alpha = 0.5)\nplt.scatter(t,y_obs[:,1], marker = '.', alpha = 0.5)\n\n\nplt.plot(t,true_sol[:,0], label = 'Estimated ', color = 'k')\nplt.plot(t,true_sol[:,1], color = 'k')\n\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7f8e9de4e5e0>"
  },
  {
    "objectID": "posts/2019-05-21-odes/index.html#conclusions",
    "href": "posts/2019-05-21-odes/index.html#conclusions",
    "title": "Gradient Descent with ODEs",
    "section": "Conclusions",
    "text": "Conclusions\nFitting ODEs via gradient descent is possible, and not as complicated as I had initially thought. There are still some relaxations to be explored. Namely: what happens if we have observations at time \\(t_i\\) for one part of the system but not the other? How does this scale as we add more parameters to the model? Can we speed up gradient descent some how (because it takes too long to converge as it is, hence the maxiter variable). In any case, this was an interesting, yet related, divergence from my GSoC work. I hope you learned something."
  },
  {
    "objectID": "posts/2017-12-29-coins/index.html",
    "href": "posts/2017-12-29-coins/index.html",
    "title": "Coins and Factors",
    "section": "",
    "text": "I love Fivethirtyeight’s Riddler column. Usually, I can solve the problem with computation, but on some rare occasions I can do some interesting math to get the solution without having to code. Here is the first puzzle I ever solved. It is a simple puzzle, yet it has an elegant computational and analytic solution. Let’s take a look.\nThe puzzle says:"
  },
  {
    "objectID": "posts/2017-12-29-coins/index.html#computing-the-solution",
    "href": "posts/2017-12-29-coins/index.html#computing-the-solution",
    "title": "Coins and Factors",
    "section": "Computing the Solution",
    "text": "Computing the Solution\nThis is really easy to program. Here is a little python script to compute the solution:\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom itertools import product\n\n\n# Array of 100 True.  True is Heads up\nNcoins = 100\ncoins = np.ones(Ncoins,dtype = bool)\nindex = np.arange(1,Ncoins+1)\n\n#Go through the coins\nfor N in range(1,Ncoins+1):\n    \n    coins[index%N==0] = ~coins[index%N==0]  #Flip the coin.  \nShown below is the solution. In dark blue are the coins face down (I’ve arranged them in a 10 by 10 grid and annotated them with their position for clarity). When we take a look at the coins variable, we see that those coins in positions which are perfect squares pop out. That is an interesting result, but what is more interesting is reasoning out the solution without doing any computation at all!\n\n\n\n\n\nFigure 1: Coins after flipping. Dark squares are face down coins."
  },
  {
    "objectID": "posts/2017-12-29-coins/index.html#reasoning-out-the-solution.",
    "href": "posts/2017-12-29-coins/index.html#reasoning-out-the-solution.",
    "title": "Coins and Factors",
    "section": "Reasoning Out the Solution.",
    "text": "Reasoning Out the Solution.\nFirst, let’s think why a coin would end up face down. If all coins start heads up, then it would take an odd number of flips for the coin to end up face down. Since coins are only flipped when we pass a factor of a coin’s position, then those coins in positions with an odd number of factors will be heads down at the end.\nSo 9 would end up heads down because it has factors 1 (flip face down), 3 (flip face up), and 9 (flip face down), while 6 would be heads up because it has factors 1, 2, 3, and 6.\nSo which numbers have an odd number of factors? Here is where we get to do some interesting math. The Fundamental Theorem of Arithmetic says that every integer \\(N>1\\) is either prime or can be uniquely factored as a product of primes\n\\[N = \\prod_{j} p_j^{a_j} \\>.\\]\nIf \\(N\\) can be factored like this, that means it has\n\\[\\prod_{j} (a_j +1)\\]\nunique factors.\nIt is straight forward to argue that a composite odd number must be the product of odd numbers, so we know that the \\(a_j+1\\) must be odd \\(\\forall j\\), and so that means the \\(a_j\\) are even and can be written as \\(a_j = 2n_j\\). Thus, our factorization becomes\n\\[N = \\prod_j p_j^{2n_j} = \\prod_j (p_j^{n_j})^2 = \\left(\\prod_j p_j^{n_j} \\right)^2 \\>,\\]\nwhich means that if \\(N\\) has an odd number of factors, it must be a perfect square! All done.\nI love trying to solve the Riddler’s puzzle without coding. It makes me draw upon knowledge I haven’t used in a while, and may force me to learn something new."
  },
  {
    "objectID": "posts/2022-12-06-journeyman/index.html",
    "href": "posts/2022-12-06-journeyman/index.html",
    "title": "Journeyman Statistics",
    "section": "",
    "text": "A number of people have asked me how to learn statistics. I don’t have a good answer for them. I find all books are deficient in some way: some too theoretical, others not theoretical enough. Some too focused on pen and paper calculation, others provide code that was very likely written decades ago and do not use modern packages or tidy principles (e.g they begin their analysis with rm(list=ls() or use attach and detatch). I can’t recommend my own path to learning statistics either, mostly because I did a Ph.D in statistics and had the benefit of a Masters in Applied Mathematics.\nI want to write a book for people who are not statisticians but need to make use of statistics anyway. The medical residents who need to do logistic regression for a research project, the social science grad student (save econometricians, I’d say we could learn something from them but we probably know all the same things by different names) who is more interested in their research than their statistical models, the business intelligence analyst who has to to analyze a (poorly planned) A/B test and would love nothing than to improve the experiment the next time around.\nThis sequence of blog posts is going to be a sort of first go at that book. Not even an alpha or a rough draft, but rather somewhere to put some thoughts that might eventually make it into the book. This first post is about the motivation for the book, which I am tentatively calling “Journeyman Statistics”."
  },
  {
    "objectID": "posts/2022-12-06-journeyman/index.html#what-is-journeyman-statistics",
    "href": "posts/2022-12-06-journeyman/index.html#what-is-journeyman-statistics",
    "title": "Journeyman Statistics",
    "section": "What is Journeyman Statistics?",
    "text": "What is Journeyman Statistics?\nA journeyperson/woman/man is\n\n“a worker, skilled in a given building trade or craft, who has successfully completed an official apprenticeship qualification. Journeymen are considered competent and authorized to work in that field as a fully qualified employee. They earn their license by education, supervised experience and examination. Although journeymen have completed a trade certificate and are allowed to work as employees, they may not yet work as self-employed master craftsmen.\n\nWhat do I mean then by “journeyman statistics” and who are “journeyman statisticians”? Borrowing heavily from the definition above, journeyman statisticians are people who are trained in some field and are currently doing statistics in service of someone or something else. Journeyman statistics are then the statistical analyses performed by these people. I don’t think its tough to pick our journeyman statisticians; they are to a first approximation those who perform statistical analysis but are not statisticians first and foremost. They are biologists, medical residents, sociologists, analysts of several varietys, etc. To them, statistics is the means whereas statistics are the end to research (perhaps “pure”) statisticians.\nIts important to further distinguish journeyman statisticians from applied statisticians. Applied statisticians can, as Tukey once said, “play in everyone’s backyard”. They possess the necessary mathematical maturity and statistical expertise to move from field to field. A journeyman statistician, though they may be well versed in statistics, would likely stay in their own backyard (to continue the metaphor) in order to tackle problems there.\nOf course, I don’t mean to place people in boxes. You don’t need to subscribe to my taxonomy of statisticians (in fact, outside of this book I don’t think its a particularly useful taxonomy), and I think there are edge cases which threaten the taxonomy as a whole. The taxonomy is simply a model, and this model is useful for one thing: understanding motivations for learning statistics, and designing a path through statistical literature so as to serve those motivations."
  },
  {
    "objectID": "posts/2022-12-06-journeyman/index.html#why-do-we-need-a-book-on-journeyman-statistics",
    "href": "posts/2022-12-06-journeyman/index.html#why-do-we-need-a-book-on-journeyman-statistics",
    "title": "Journeyman Statistics",
    "section": "Why Do We Need a Book on Journeyman Statistics?",
    "text": "Why Do We Need a Book on Journeyman Statistics?\nThe taxonomy allows us to understand who journeyman statisticians are, what their intentions are, what they may lack in terms of statistical understanding, what is enough to satiate their desire to learn statistics, and what details contribute “noise” rather than “signal”. As an example, I don’t think biology grad students need to know what \\(\\operatorname{plim}\\) means, or any of the other topics adjacent to mathematical analysis in Casella and Berger’s Statistical Inference. They do need to go slightly beyond their sophmore classes which tell them they can use the t-test when \\(N>30\\). Likewise, medical residents need to go beyond Martin Bland’s An Introduction to Medical Statistics and need to be able to confidently say “we shouldn’t do that” when their supervisors or superiors insist on a clearly flawed mode of analysis. However, they may get bogged down by the integrals in Frank Harrell’s Regression Modelling Strategies (as well as the sea of references to methodological papers). There are few books for people like that. I find that books are typically for sophmore students learning statistics for the first time, or applied statisticians. Journeyman statisticians need something in the middle. I hope this book is that something."
  },
  {
    "objectID": "posts/2022-12-06-journeyman/index.html#what-will-this-book-contain",
    "href": "posts/2022-12-06-journeyman/index.html#what-will-this-book-contain",
    "title": "Journeyman Statistics",
    "section": "What Will This Book Contain?",
    "text": "What Will This Book Contain?\nStatisticians like to joke “its all regression”. There is truth in that phrase, and so this book will take the perspective that regression is the primary means of estimation. We’ll cover all the typical analyses as regression methods. This includes estimation of the mean, its just a one parameter regression. I want to get to GLM’s as quickly as possible while not getting bogged down by mathematical details, like whatever “mild regularity conditions” means. GLMs are the workhorse of applied statistics, and I see no point in leaving GLMs to later chapters. One thing that will be absent from the book is p values. The book will take an estimation approach and report only confidence intervals.\nThe book will also contain code in both python and R, though I encourage readers to use R rather than python. Python’s statistical tools have a distinct econometrics flavor."
  },
  {
    "objectID": "posts/2022-12-06-journeyman/index.html#what-benefit-is-there-to-reading-this-book",
    "href": "posts/2022-12-06-journeyman/index.html#what-benefit-is-there-to-reading-this-book",
    "title": "Journeyman Statistics",
    "section": "What Benefit is There to Reading This Book?",
    "text": "What Benefit is There to Reading This Book?\nBefore discussing why you should read this book, I want to discuss what I call “The Precision-Usefulness Tradeoff”, as I anticipate I will refer to this many times. In short, the tradeoff states\n\nPerfectly precise statements are completely useless. In order to become useful, the statement must be made less precise. The less precise, the more useful.\n\nAgain, this is a model rather than a law. As an example, consider the definition for a 95% confidence interval.\n\nA 95% confidence interval is an interval estimate which, upon repeated construction under identical and ideal conditions, contains the estimand 95% of the time.\n\nI can make this statement more precise by writing it down mathematically\n\\[ P \\left( \\bar{x} - z_{0.975} \\sigma/\\sqrt{n} \\leq \\mu \\leq \\bar{x} + z_{0.975} \\sigma/\\sqrt{n} \\right) = 0.95 \\]\nbut it loses usefulness. This doesn’t really tell me what a confidence interval is, when to use one, or how to interpret one. However, the definition I initially presented perhaps permits some pathological cases. We can make the definition more useful by removing precision further\n\nA 95% confidence interval contains parameters consistent with the data.\n\nNow, we have a better idea how to interpret the interval, but the 95% is opaque to us.\nThe reason I bring up this trade off is because the book is intended to give journeyman statisticians the tools required to move in Precision-Usefulness space. My hope is that when the time comes, you will be able to artfully trade precision for usefulness (e.g. to be pedantic when it is necessary, or to break the rules precisely because you know how and when they can be broken safely). This is the primary benefit of the book. Obviously, I can’t tell people when or where to move within that space. I will have to leave that to their best judgement.\nThe second benefit is to solve the problem we began with; to answer “how to I go about learning statistics” in a satisfactory way."
  },
  {
    "objectID": "posts/2022-07-20-pooling-experiments/index.html",
    "href": "posts/2022-07-20-pooling-experiments/index.html",
    "title": "Forecasting Experimental Lift Using Hierarchical Bayesian Modelling",
    "section": "",
    "text": "You’re part of a team at a company who is tasked with improving conversion on some web page. You’ve run a few experiments already with mixed results and now it is time to set some goals for the next year. Here is a question:\nMaybe your approach for your end of year targets would look like\n\\[ \\Big( \\mbox{Average Lift}\\Big)^{\\mbox{Number of Planned Experiments}} \\]\nIts a good back-of-the-napkin approach to the problem. But if you come up short is that neccesarily a failure? Or, could it be well within expectation?\nThis post is forecasting how much a given team can move a metric within some time frame. You’re going to forecast the lift the team can generate given some of their past performance. The forecasting is Bayesian, but assumes the team works within a frequentist framework."
  },
  {
    "objectID": "posts/2022-07-20-pooling-experiments/index.html#assumptions",
    "href": "posts/2022-07-20-pooling-experiments/index.html#assumptions",
    "title": "Forecasting Experimental Lift Using Hierarchical Bayesian Modelling",
    "section": "Assumptions",
    "text": "Assumptions\nYour team can run approximately 1 experiment per month or 12 in a calendar year (but the method we develop can be extended to an arbitrary number of experiments per month). Let’s say you start experimenting on January 1 and will evaluate your performance December 31. In addition to this, assume:\n\nAll your experiments are A/B tests with two and only two groups: test and control.\nYour main metric is a conversion rate and the baseline value is 1%.\nEvery intervention has an effect, though it may be small. The null is never true.\nYour site sees 100,000 unique users per month. You split all 100,000 into two groups at random, and\nYou measure lift in a relative sense (this is sometimes called relative risk in epidemiology).\n\nLet’s make some additional assumptions about experiments:\n\nYour team is relatively reliable. They don’t get better at thinking up interventions over time, so the effects they generate do not change over time, except for random variation.\nExperiments effects are independent of one another, so the implementation of one change does not alter the effect of the next experiment."
  },
  {
    "objectID": "posts/2022-07-20-pooling-experiments/index.html#scenario",
    "href": "posts/2022-07-20-pooling-experiments/index.html#scenario",
    "title": "Forecasting Experimental Lift Using Hierarchical Bayesian Modelling",
    "section": "Scenario",
    "text": "Scenario\nShown in the table below are your results over the last year. Nice job, lots of wins, a few failures to reject the null, but overall very good. Using the estimated relative lifts where you did , you managed to increase conversion by 80%. Now, you’re PM is asking you to shoot for 2x conversion this year.\nIs that reasonable1? How probable are you to generate at least 2x lift over 12 months given your past performance? I mean, it’s only a little better than you did this past year, right? Luckily, you’re a good data scientist. Even though your team uses frequentism to evaluate their A/B tests, you are not beholden to one ideology over another. So, you decide to use a hierarchical Bayesian model to estimate what kinds of lifts your team is likely to generate in the future.\n\n\n\n\n \n  \n    N \n    Treatment Conversions \n    Control Conversions \n    Relative Lift \n    p \n  \n \n\n  \n    50,000 \n    541 \n    496 \n    1.09 \n    0.08 \n  \n  \n    50,000 \n    557 \n    524 \n    1.06 \n    0.16 \n  \n  \n    50,000 \n    559 \n    486 \n    1.15 \n    0.01 \n  \n  \n    50,000 \n    556 \n    500 \n    1.11 \n    0.04 \n  \n  \n    50,000 \n    530 \n    516 \n    1.03 \n    0.34 \n  \n  \n    50,000 \n    532 \n    475 \n    1.12 \n    0.04 \n  \n  \n    50,000 \n    516 \n    507 \n    1.02 \n    0.40 \n  \n  \n    50,000 \n    532 \n    475 \n    1.12 \n    0.04 \n  \n  \n    50,000 \n    528 \n    490 \n    1.08 \n    0.12 \n  \n  \n    50,000 \n    544 \n    506 \n    1.08 \n    0.13 \n  \n  \n    50,000 \n    519 \n    512 \n    1.01 \n    0.43 \n  \n  \n    50,000 \n    552 \n    489 \n    1.13 \n    0.03"
  },
  {
    "objectID": "posts/2022-07-20-pooling-experiments/index.html#hierarchical-model",
    "href": "posts/2022-07-20-pooling-experiments/index.html#hierarchical-model",
    "title": "Forecasting Experimental Lift Using Hierarchical Bayesian Modelling",
    "section": "Hierarchical Model",
    "text": "Hierarchical Model\nLet \\(\\widehat{RR}_i\\) be the estimated relative lift2 from experiment \\(i\\). The sampling distribution of relative lift is asymptotically normally distributed on the log scale. Assuming we know the standard error exactly (using the delta rule), this means\n\\[ \\log \\Big(\\widehat{RR}_i \\Big) \\sim \\mathcal{N}(\\log(\\theta_i), \\sigma)\\]\nHere, \\(\\log(\\theta_i)\\) is the relative lift on the log scale for experiment \\(i\\) (whereas \\(\\widehat{RR}_i\\) is just the estimated relative lift). We can model the \\(\\theta\\) hierarchically as\n\\[ \\log(\\theta_i) \\sim \\mathcal{N}(\\mu, \\tau) \\]\nNow, you just need to place priors on \\(\\mu\\) and \\(\\tau\\) (assume you used good priors)."
  },
  {
    "objectID": "posts/2022-07-20-pooling-experiments/index.html#forcasting-lift",
    "href": "posts/2022-07-20-pooling-experiments/index.html#forcasting-lift",
    "title": "Forecasting Experimental Lift Using Hierarchical Bayesian Modelling",
    "section": "Forcasting Lift",
    "text": "Forcasting Lift\nOnce you fit your model, you can generate hypothetical relative lifts by sampling from the model. Let \\(\\psi\\) be a relative lift, so that\n\\[ \\log(\\psi) \\sim \\mathcal{N}(\\mu, \\sigma) \\>. \\]\nIf your team were to implement an experiment for which had a relative lift of \\(\\psi\\), you would get an estimated relative lift. Depending on the size of that estimate, you may or may not reject the null hypothesis. The probability you reject the null hypothesis is when it is false (and it is always false by assumption) is known as the statistical power. Since you have a fixed sample size in each experiment, and every experiment is a 50/50 split, you can calculate the statistical power that you detect a relative lift of \\(\\psi\\). Call that \\(p_{\\psi}\\).\nNow for the fun part. Say you run \\(n\\) experiments per month for \\(K\\) months. The lift you generate in month \\(k\\), \\(LG_k\\), would be\n\\[ LG_k = \\exp\\Bigg( \\sum_{j=1}^n \\log(\\psi_j) p_{\\psi, j} \\Bigg) \\]\nand the forecasted lift, \\(FL\\), up to and including month \\(k\\) is\n\\[ FL_k = \\prod_{i=1}^{k} LG_i \\]\nThink this through. If you were to implement every intervention, your lift would simply be \\(\\prod_{j=1}^k \\psi_j\\), or on the log scale \\(\\sum_j \\log(\\psi_j)\\). But you don’t detect every effect. The probability you detect the effect of the \\(j^{th}\\) intervention is \\(p_{\\psi, j}\\). So \\(\\sum_j \\log(\\psi_j) p_{\\psi, j}\\) is the expected lift you would accrue over the \\(k\\) experiments. Take the exponential to convert this sum back to a product and you’ve got a generated lift after \\(n\\) experiments in a given month. Multiply the lift month over month to get a forecasted lift. Now, because there is uncertainty in the \\(\\psi\\), there is uncertainty in the forecasted lift. However, your hierarchical model will make it more or less easy to integrate over that uncertainty. Just sample from the model and average over the samples."
  },
  {
    "objectID": "posts/2022-07-20-pooling-experiments/index.html#modelling",
    "href": "posts/2022-07-20-pooling-experiments/index.html#modelling",
    "title": "Forecasting Experimental Lift Using Hierarchical Bayesian Modelling",
    "section": "Modelling",
    "text": "Modelling\n\n\n\nLuckily, all of the computation above – even the power calculation – can be done inside Stan (and you’re pretty good at writing Stan code3).\nShown in Figure 1 is the forecasted lift as compared to baseline after the \\(k^{th}\\) month Good job, if you keep doing things as you’re doing, you’re going to probably increase conversion rate by a little more than 50% (a little less than the 80% but still nothing to sneeze at). The shaded blue regions indicate the uncertainty in that estimate. Note that although your forecasted lift seems to always be increasing, that isn’t necessarily the case. You could implement a change which hurts our conversion because of chance, so if you were to plot simulated trajectories you might see some decreases in the metric.\n\n\n\n\nFigure 1: Forecasted lift after the 12 months. Shown in blue are credible interval estimates. The conditional distirbution is log-normal since the forecasted lift is the sum of normals on the log scale.\n\n\n\n\n\n\n\nClick to see individual trajectories\n\n\n\n\n\n\nThe red lines are draws where you would have implemented a change to hurt the conversion rate. See how sometimes those lines actually decrease? Such is life, can’t win em all!\n\n\n\n\n\n\n\nNow, what about that goal of increasing conversion by 2x? Well, it isn’t looking good. Looks like there is only a 12% chance you meet or exceed the 2x goal. Could it be your performance last year was just extreme? The distribution of forecasted lifts is long tailed. Maybe you’re just going to regress to the mean. Sounds like a good time to push back on your boss and come prepared with data.\n\n\n\n\nFigure 2: Conditional posterior distirbution of forecasted lifts after completing the 12 experiments you had planned this year. Indicated point/interval is mean and 95% credible interval."
  },
  {
    "objectID": "posts/2022-07-20-pooling-experiments/index.html#conclusion",
    "href": "posts/2022-07-20-pooling-experiments/index.html#conclusion",
    "title": "Forecasting Experimental Lift Using Hierarchical Bayesian Modelling",
    "section": "Conclusion",
    "text": "Conclusion\nYou’re clever and realized you could use a hierarchical model to simulate future experiment results and use those to forecast your team’s performance. Your boss’ goal of a 2x increase is nice in so far as it shows they have confidence in you and your team, but the model says it isn’t super achievable.\nIf 2x isn’t achievable, what is a better target? Or maybe, what is a better range of targets. I’m not sure, that isn’t the point of the post. The post was to equip you with a means of answering that question yourself, and I know you’re capable of answering it. I mean…look at all this cool modelling you did."
  },
  {
    "objectID": "posts/2022-07-20-pooling-experiments/index.html#post-script",
    "href": "posts/2022-07-20-pooling-experiments/index.html#post-script",
    "title": "Forecasting Experimental Lift Using Hierarchical Bayesian Modelling",
    "section": "Post Script",
    "text": "Post Script\nOk, breaking away from the narrative for a moment…this is a continuous approximation to a discrete process. We should simulate this to see how real experiments would stack up against my forecast. I’ve gone ahead and actually simulated running the 12 tests and computed the lift after the 12 tests. Shown below is the forecasted lift versus relative error as compared to simulation. I’ll let you come to your own conclusion about the quality of the approximation."
  },
  {
    "objectID": "posts/2023-03-31-optimal-mde/index.html#introduction",
    "href": "posts/2023-03-31-optimal-mde/index.html#introduction",
    "title": "Choosing the Optimal MDE for Experimentation",
    "section": "Introduction",
    "text": "Introduction\nPlanning experiments means deciding how long to run the experiment (by doing a sample size calculation). A big factor in this decision is agreeing on a minimal detectable effect or MDE. Smaller MDEs mean longer experiments and hence fewer of them in a given window of time. Larger MDEs mean shorter experiments, but they also mean there is a chance we could fail to reject smaller effects 1. Clearly, there is a sweet spot for the MDE; not so large that we are passing over interventions which would improve a given metric, but not so small that we are wasting our time collecting samples.\nThis blog post is intended to demonstrate how data scientists can empirically estimate the “optimal” MDE for experimenting teams. Here, “optimal” refers to the MDE which optimizes the long run cumulative improvement to the metric (i.e. the MDE which is estimated to improve the metric the most in a given window of time) under some mild and some strong assumptions. I begin by assuming teams have a model of likely effect sizes for their interventions vis a vis a Bayesian model like I have done in my past post Forecasting Experimental Lift Using Hierarchical Bayesian Modelling.\nI begin with an illustration of the larger idea, at a level I hope would be appropriate for a product manager. Then, I formally describe the procedure and implement it in R, making some illustrative assumptions about a hypothetical team running experiments. Lastly, I demonstrate how various assumptions can effect the estimate of the optimal MDE and estimate a linear model for the optimal MDE as a function of the assumptions.\nAlthough imperfect, I believe this approach offers a superior approach to determining the MDE for experimenting teams, and also has the benefit of being re-estimable experiment over experiment thanks to the Bayesian modelling driving the approach."
  },
  {
    "objectID": "posts/2023-03-31-optimal-mde/index.html#big-idea",
    "href": "posts/2023-03-31-optimal-mde/index.html#big-idea",
    "title": "Choosing the Optimal MDE for Experimentation",
    "section": "Big Idea",
    "text": "Big Idea\nLet’s consider how many experiments a team can run in a year. Teams usually have some upper limit for the number of experiments because they need time to tend to their other responsibilities. Let’s assume a team can run 24 experiments in a year (~2 experiments a month on average). While the team has a maximum number of experiments they can run, the number of experiments they actually run will depend on how the experiments are planned. For example, if the team has on average 10,000,000 unique visitors each year, and each experiment needs 200, 000 users, then the team can run 50 experiments (you can’t run half an experiment so you have to round down to the nearest number). So the number of experiments that can be run for this team is the smaller of the ratio of unique visitors per year to total sample size per experiment and 24. The key insight here is that because the MDE determines the sample size per experiment (again, larger/smaller MDEs mean smaller/larger sample sizes), then the MDE implicitly determines the number of experiments we can.\nEach change to the product has some true effect that is unknown to us. In fact, the whole reason we run an experiment is to estimate that effect. Because the MDE determines the sample size per experiment, it also determines the probability we detect the effect. Increase the MDE, and the probability we detect a given effect decreases (because the sample size for the experiment decreases). Conversely, decreasing the MDE increases the probability we detect an effect.\nWe need to choose an MDE so that we can run lots of experiments and have a good chance of detecting positive effects in those experiments. The problem is that we don’t know what kinds of effects our experiments are going to have, which is why we use an MDE. The MDE basically is a stand in for what we think the effect of the change is going to be at its smallest. If the effect of the change is bigger than the MDE, then we have a really good chance to detect the effect.\nHowever, we actually can estimate what kinds of effects our changes will produce. It isn’t worth getting into, but we can estimate a distribution of likely effect sizes, meaning we can reasonably guess what kinds of effects we are going to see in future experiments. This means that we can use this distribution of plausible future effects to simulate future experiments. These simulations can then be used to determine the MDE which strikes the balance we need. A good way to determine which MDE is best is to consider the “cumulative impact” on the metric. Think of it this way; if we run lots of experiments and they all have a reasonable chance of detecting effects then the metric we’re seeking to improve is going to change in a big way. So our goal is to find the MDE which results in the largest improvement to our metric over a given window of time in which we can experiment. This MDE which results in the largest improvement is called the “optimal MDE”.\nLet’s formalize the optimization problem using some math."
  },
  {
    "objectID": "posts/2023-03-31-optimal-mde/index.html#mathematical-details",
    "href": "posts/2023-03-31-optimal-mde/index.html#mathematical-details",
    "title": "Choosing the Optimal MDE for Experimentation",
    "section": "Mathematical Details",
    "text": "Mathematical Details\nLet \\(N_{uuid}\\) be the number of unique subjects available for experimentation in a given time frame, let \\(\\delta \\in \\mathbb{R}_+\\) be an arbitrary MDE, and let \\(K\\) be the maximum number of experiments for a given team. Let \\(n_{ss}: \\mathbb{R}_+ \\to \\mathbb{N}\\) be a function which maps MDEs to sample sizes for experiments. The number of experiments which can be run in a given time frame is \\(n_{\\exp}(\\delta) = \\min\\left(K, \\lfloor\\frac{N_{uuid}}{n_{ss}(\\delta)} \\rfloor \\right)\\). Here, I have included an explicit dependency of \\(n_{\\exp}\\) on \\(\\delta\\) to remind us that the MDE implicitly determines the number of experiments. I’ve made the assumption that experiments can be run back to back.\nLet \\(\\theta_k\\) be a latent effect from an intervention, and let \\(\\psi(\\theta_k; \\delta)\\) be the statistical power to detect an effect of \\(\\theta\\) when the experiment is designed with an MDE of \\(\\delta\\). I assume that: a) All interventions are independent of one another, and the effect of one intervention is not changed by the implementation of another, b) effects of interventions are additive (on the appropriate scale), and c) effects persist through time (there is now decay of an effect once implemented).\nThe objective function we seek to optimize is the expected cumulative improvement to the metric we are opting to experiment on. We get \\(n_{\\exp}\\) draws from our population distribution of effects (because we are running that many experiments), and the expected cumulative improvement is the sum of the products of the effects and the probability we detect the effect\n\\[ C(\\delta) = \\sum_{k=1}^{n_{\\exp}(\\delta)} \\theta_k \\psi(\\theta_k; \\delta)  \\] Note here that the MDE \\(\\delta\\) determines both the number of experiments run (\\(n_{\\exp}\\)) and the probability those experiments detect an effect. The optimal MDE is then\n\\[ \\delta_{\\mbox{opt}} = \\underset{\\delta \\in \\mathbb{R}_+}{\\arg\\max}  \\Big\\{ C(\\delta) \\Big \\}\\] While the \\(\\theta\\) are latent, this quantity can still be optimized by using draws from a Bayesian model for experimental effects. See this previous post of mine for an example of what I mean.\nIn the next section, I demonstrate how to estimate \\(\\delta_{\\mbox{opt}}\\) using simulation. To fill in some missing information (e.g. the maximum number of experiments run by the team), I explicitly write out some assumptions."
  },
  {
    "objectID": "posts/2023-03-31-optimal-mde/index.html#additional-assumptions",
    "href": "posts/2023-03-31-optimal-mde/index.html#additional-assumptions",
    "title": "Choosing the Optimal MDE for Experimentation",
    "section": "Additional Assumptions",
    "text": "Additional Assumptions\nImagine a team who runs experiments. I make the following assumptions about the team:\n\nThe team’s entire job is running experiments. Due to resourcing constraints, they can only run a finite number experiments per year. I assume the team can run 24 experiments a year (or 2 per month on average). The team can run experiments back to back.\nThe team works in a frequentist framework, and they always run 2 tailed tests because there is a chance they could hurt the product, and they would want to know that.\nThe main causal contrast is relative risk. In industry, we call this the “lift”.\nThe outcome is a binary outcome, and the baseline rate is 8%.\n10,000,000 unique visitors to your website per year.\nThe team generates lift fairly reliably and these lifts sustain through time. There is no decay of the effect, no interaction between experiments, nor is there any seasonality. These are blatantly false, but they simplify enough for us to get traction.\nThe population level lift distribution is log normal, with parameters \\(\\mu=\\log(1.01)\\) and \\(\\sigma=0.1\\) on the log scale. This means the team increases the metric by approximately 1% on average.\nThe team is really only interested in positive effects (lift > 1) so they will not implement anything with lift < 1, and if the null fails to be rejected they will stick with the status quo.\nThe same MDE is used to plan all experiments.\n\nUnder these assumptions, a procedure can be devised to optimize the cumulative improvement to the metric of interest."
  },
  {
    "objectID": "posts/2023-03-31-optimal-mde/index.html#results-from-a-simulation-experiment",
    "href": "posts/2023-03-31-optimal-mde/index.html#results-from-a-simulation-experiment",
    "title": "Choosing the Optimal MDE for Experimentation",
    "section": "Results from a Simulation Experiment",
    "text": "Results from a Simulation Experiment\nShown in the code cell below is simulation of the process for finding the optimal MDE under the assumptions listed above. Rather than simulate every experiment (e.g. by drawing random numbers and performing a statistical test), we can draw a binomial random variable with probability of success equal to the statistical power of detecting the latent lift with the indicated MDE and hence sample size.\nThe optimal lift is somewhere between 5% and 6%. Explicit optimization methods could be used find the optima, but I think for the purposes of experimentation you just want to be in the right ballpark, so a plot is more than enough.\n\n\nCode\none_sided_power = function(real_lift, n_per_group){\n  # Only interested in the case when the estimated lift is\n  # Greater than one, which corresponds to a one sided test.\n  # However, you always run 2 tailed tests, so the significance level\n  # is half of what is typically is.\n  pwr.2p.test(h = ES.h(real_lift*baseline, baseline), \n              n = n_per_group,\n              alternative = 'greater',\n              sig.level = 0.025\n              )$power\n}\n\nf = function(mde, baseline=0.08, n_uuids=2500000, latent_lift = 1.01){\n  \n  # Draw lifts for experiments from this distribution\n  lift_dist <-\\(n) rlnorm(n, log(latent_lift), 0.1)\n  \n  # Given the MDE, here is how many users you need per group in each experiment.\n  n_per_group = ceiling(pwr.2p.test(h = ES.h(mde*baseline, baseline), power = 0.8)$n)\n  \n  # Here is how many experiments you could run per year\n  # Why the factor of 2?  Because the computation above is the szie of each group.\n  n_experiments_per_year <- pmin(24, floor(n_uuids/(2*n_per_group)))\n  \n  # Here is a grid of experiments.  Simulate \n  # Running these experiments 1000 times\n  # each experiment has n_per_group users in each group\n  simulations <- crossing(\n    sim = 1:4000, \n    experiment = 1:n_experiments_per_year,\n    n_per_group = n_per_group\n  )\n  \n  simulations %>% \n    mutate(\n      # draw a real lift for each experiment from your lift distribution\n      real_lift = lift_dist(n()),\n      # Compute the power to detect that lift given the sample size you have\n      actual_power = map2_dbl(real_lift, n_per_group, one_sided_power),\n      # Simulate detecting the lift\n      detect = as.logical(rbinom(n(), 1, actual_power)),\n      # Did you implement the result or not?\n      # If you didn't, this is equivalent to a lift of 1\n      # and won't change the product.\n      result = if_else(detect, real_lift, 1),\n    ) %>% \n    group_by(sim) %>% \n    #finally, take the product, grouping among simulations.\n    summarise(lift = prod(result)) \n  \n}\n\n\nmdes <- tibble(mde = seq(1.01, 1.2, 0.01)) %>% \n        mutate(mde_id = as.character(seq_along(mde)))\n\nresults = map_dfr(mdes$mde, f, .id = 'mde_id')  %>% \n          left_join(mdes)\n\n\nresults %>% \nggplot(aes(mde, lift)) + \n  stat_summary(fun.data = \\(x) mean_se(x, 2)) + \n  scale_x_continuous(labels = \\(x) scales::percent(x-1, 0.01)) + \n  scale_y_continuous(labels = \\(x) scales::percent(x-1, 0.01)) +\n  labs(x='MDE', y='Cumulative Improvement Over all Experiments',\n       title = 'Swing for the Fences',\n       subtitle = 'The optimal MDE is not the expected lift the team generates')"
  },
  {
    "objectID": "posts/2023-03-31-optimal-mde/index.html#how-do-the-various-parts-of-the-problem-change-the-objective-function",
    "href": "posts/2023-03-31-optimal-mde/index.html#how-do-the-various-parts-of-the-problem-change-the-objective-function",
    "title": "Choosing the Optimal MDE for Experimentation",
    "section": "How Do The Various Parts of The Problem Change The Objective Function?",
    "text": "How Do The Various Parts of The Problem Change The Objective Function?\nChanging the baseline of the metric moves the optima, with larger MDEs being considered optimal for smaller baselines.\n\n\n\n\n\nAs the number of unique visitors to the website increases, the optimal MDE decreases, but only slightly.\n\n\n\n\n\nAs the expectation of the latent lift increases, the optima does not move but the expected cumulative improvement to the metric increases. This is unsurprising."
  },
  {
    "objectID": "posts/2022-05-07-flippin/index.html",
    "href": "posts/2022-05-07-flippin/index.html",
    "title": "Flippin’ Fun!",
    "section": "",
    "text": "I wrote an answer to a question about sequences of coin flips a couple days back that I was quite chuffed with. In short, the question asked for statistical ways to determine if a sequence of coin flips was from an unbiased coin or a human trying to appear random. The resulting model turned into a fun game on twitter centered around determining if people who follow me could simulate a sequence of coin flips that looked random (without using a real RNG, or some funny workaround. I have a lot of faith in my twitter followers…maybe too much).\nAnyway, then I thought “I should fit a hierarchical model to this data”. So that’s what I’m doing"
  },
  {
    "objectID": "posts/2022-05-07-flippin/index.html#initial-model",
    "href": "posts/2022-05-07-flippin/index.html#initial-model",
    "title": "Flippin’ Fun!",
    "section": "Initial Model",
    "text": "Initial Model\nTo understand the hierarchical model, we first need to understand the model I initially built. Let me give you a quick rundown on that.\nLet \\(S\\) be a sequence of bernoulli experiments so that \\(S_i\\) is either 1 or a 0 (a heads or a tails if you wish). The question I answered concerns detecting if a given sequence \\(S\\) could have been created by a human (or by a non-random process posing as random). I interpreted that as a call to estimate the lag-1 autocorrelation of the flips. The hypothesis being that humans probably perceive long streaks of one outcome or the other as signs of non-randomness and will intentionally switch the outcome if they feel the run is too long. I initially chose a Bayesian approach because I’m a glutton for punishment and someone else already gave a pretty good answer.\nThe model is quite straight forward to write down. Let \\(\\rho\\) be the correlation between \\(S_i\\) and \\(S_{i+1}\\), and let \\(q\\) be the expected number of heads in the sequence. We can write down the conditional probabilities that we see a 0/1 given the last element in the sequence was a 1/0. Those conditional probabilities are derived here and they are…\n\\[ P(1 \\vert 1) = q + \\rho(1-q) \\]\n\\[ P(1 \\vert 0) = q(1-\\rho) \\]\n\\[ P(0 \\vert 1) = (1-q)(1-\\rho) \\]\n\\[ P(0 \\vert 0) = 1 - q + \\rho \\cdot q \\]\nThe trick is to then count the subsequences of (1, 1), (1, 0), (0, 1), and (0, 0). Let \\(p_{i\\vert j} = P(i \\vert j)\\). We can then consider the count of each subsequence as multinomial\n\\[ y \\sim \\mbox{Multinomial}(\\theta) \\>. \\]\nHere, \\(\\theta\\) is the multinomial parameter, wherein each element is \\(\\theta = [p_{1 \\vert 1}, p_{1\\vert 0}, p_{0 \\vert 1}, p_{0\\vert 0}]\\). Equip this with a uniform prior on both \\(\\rho\\) and \\(q\\) and you’ve got yourself a model."
  },
  {
    "objectID": "posts/2022-05-07-flippin/index.html#the-stan-code",
    "href": "posts/2022-05-07-flippin/index.html#the-stan-code",
    "title": "Flippin’ Fun!",
    "section": "The Stan Code",
    "text": "The Stan Code\nThe Stan code for this model is quite easy to understand. The data block will consist of the counts of each type of subsequence. We can then concatenate those counts into an int of length 4 via the transformed data block. The concatenated counts will be what we pass to the multinomial likelihood.\ndata{\n  int y_1_1; //number of concurrent 1s\n  int y_0_1; //number of 0,1 occurences\n  int y_1_0; //number of 1,0 occurences\n  int y_0_0; //number of concurrent 0s\n}\ntransformed data{\n    int y[4] = {y_1_1, y_0_1, y_1_0, y_0_0};\n}\nThe only parameters we are interested in estimating are the autocorrelation rho and the coin’s bias q.\nparameters{\n  real<lower=-1, upper=1> rho;\n  real<lower=0, upper=1> q;\n}\nWe can derive the probabilities we need via the equations above, and that is a job for the transformed parameters block. We can then concatenate the conditional probabilities into a simplex data type object, theta to pass to the multinomial likelihood. Be careful though, we need to multiply theta by 0.5 since we are working with conditional probabilities. Note \\(p_{1\\vert 1} + p_{0 \\vert 1 } + p_{1\\vert 0} + p_{0 \\vert 0 } = 2\\), hence the scaling factor.\ntransformed parameters{\n  real<lower=0, upper=1> prob_1_1 = q + rho*(1-q);\n  real<lower=0, upper=1> prob_0_1 = (1-q)*(1-rho);\n  real<lower=0, upper=1> prob_1_0 = q*(1-rho);\n  real<lower=0, upper=1> prob_0_0 = 1 - q + rho*q;\n  simplex[4] theta = 0.5*[prob_1_1, prob_0_1, prob_1_0, prob_0_0 ]';\n}\nThe model call is then quite simple\nmodel{\n  q ~ beta(1, 1);\n  rho ~ uniform(-1, 1);\n  y ~ multinomial(theta); \n}\nand we can even generate new sequences based off the estimated parameters as a sort of posterior predictive check.\ngenerated quantities{\n    vector[300] yppc;\n    \n    yppc[1] = bernoulli_rng(q);\n    \n    for(i in 2:300){\n        if(yppc[i-1]==1){\n            yppc[i] = bernoulli_rng(prob_1_1);\n        }\n        else{\n        yppc[i] = bernoulli_rng(prob_1_0);\n        }\n    }\n}\nAll in all the model is\ndata{\n\n  int y_1_1; //number of concurrent 1s\n  int y_0_1; //number of 0,1 occurences\n  int y_1_0; //number of 1,0 occurences\n  int y_0_0; //number of concurrent 0s\n  \n}\ntransformed data{\n    int y[4] = {y_1_1, y_0_1, y_1_0, y_0_0};\n}\nparameters{\n  real<lower=-1, upper=1> rho;\n  real<lower=0, upper=1> q;\n}\ntransformed parameters{\n  real<lower=0, upper=1> prob_1_1 = q + rho*(1-q);\n  real<lower=0, upper=1> prob_0_1 = (1-q)*(1-rho);\n  real<lower=0, upper=1> prob_1_0 = q*(1-rho);\n  real<lower=0, upper=1> prob_0_0 = 1 - q + rho*q;\n  simplex[4] theta = 0.5*[prob_1_1, prob_0_1, prob_1_0, prob_0_0 ]';\n}\nmodel{\n  q ~ beta(1, 1);\n  rho ~ uniform(-1, 1);\n  y ~ multinomial(theta);\n  \n}\ngenerated quantities{\n    vector[300] yppc;\n    \n    yppc[1] = bernoulli_rng(q);\n    \n    for(i in 2:300){\n        if(yppc[i-1]==1){\n            yppc[i] = bernoulli_rng(prob_1_1);\n        }\n        else{\n        yppc[i] = bernoulli_rng(prob_1_0);\n        }\n    }\n}"
  },
  {
    "objectID": "posts/2022-05-07-flippin/index.html#run-from-python",
    "href": "posts/2022-05-07-flippin/index.html#run-from-python",
    "title": "Flippin’ Fun!",
    "section": "Run From Python",
    "text": "Run From Python\nWith the model written down, all we need to do is add some python code to create the counts of each subsequence and then run the stan model. Here is teh python code I used to create the response tweets for that game I ran on twitter.\n\nimport cmdstanpy\nimport matplotlib.pyplot as plt\n\ny_1_1 = 0 # count of (1, 1)\ny_0_0 = 0 # count of (0, 0)\ny_0_1 = 0 # count of (0, 1)\ny_1_0 = 0 # count of (1, 0)\n\nsequence = list('TTHHTHTTHTTTHTTTHTTTHTTHTHHTHHTHTHHTTTHHTHTHTTHTHHTTHTHHTHTTTHHTTHHTTHHHTHHTHTTHTHTTHHTHHHTTHTHTTTHHTTHTHTHTHTHTTHTHTHHHTTHTHTHHTHHHTHTHTTHTTHHTHTHTHTTHHTTHTHTTHHHTHTHTHTTHTTHHTTHTHHTHHHTTHHTHTTHTHTHTHTHTHTHHHTHTHTHTHHTHHTHTHTTHTTTHHTHTTTHTHHTHHHHTTTHHTHTHTHTHHHTTHHTHTTTHTHHTHTHTHHTHTTHTTHTHHTHTHTTT'.upper())\n\n# Do a rolling window trick I saw somewhere on twitter.\n# This implements a rollowing window of 2\n# In python 3.10, this would be a great use case for match\nfor pairs in zip(sequence[:-1], sequence[1:]):\n    if pairs == ('H','H'):\n        y_1_1 +=1\n    elif pairs == ('T','H'):\n        y_0_1 +=1\n    elif pairs == ('H', 'T'):\n        y_1_0 +=1\n    else:\n        y_0_0 +=1\n\n# Write the stan model as a string.  We will then write it to a file\nstan_code = '''\ndata{\n\n  int y_1_1; //number of concurrent 1s\n  int y_0_1; //number of 0,1 occurences\n  int y_1_0; //number of 1,0 occurences\n  int y_0_0; //number of concurrent 0s\n  \n}\ntransformed data{\n    int y[4] = {y_1_1, y_0_1, y_1_0, y_0_0};\n}\nparameters{\n  real<lower=-1, upper=1> rho;\n  real<lower=0, upper=1> q;\n}\ntransformed parameters{\n  real<lower=0, upper=1> prob_1_1 = q + rho*(1-q);\n  real<lower=0, upper=1> prob_0_1 = (1-q)*(1-rho);\n  real<lower=0, upper=1> prob_1_0 = q*(1-rho);\n  real<lower=0, upper=1> prob_0_0 = 1 - q + rho*q;\n  simplex[4] theta = 0.5*[prob_1_1, prob_0_1, prob_1_0, prob_0_0 ]';\n}\nmodel{\n  q ~ beta(1, 1);\n  rho ~ uniform(-1, 1);\n  y ~ multinomial(theta);\n  \n}\ngenerated quantities{\n    vector[300] yppc;\n    \n    yppc[1] = bernoulli_rng(q);\n    \n    for(i in 2:300){\n        if(yppc[i-1]==1){\n            yppc[i] = bernoulli_rng(prob_1_1);\n        }\n        else{\n        yppc[i] = bernoulli_rng(prob_1_0);\n        }\n    }\n}\n'''\n\n\n\n# Write the model to a temp file\nwith open('model_file.stan', 'w') as model_file:\n    model_file.write(stan_code)\n    \n# Compile the model\nmodel = cmdstanpy.CmdStanModel(stan_file='model_file.stan')\n\n# data to pass to Stan\ndata = dict(y_1_1 = y_1_1, y_0_0 = y_0_0, y_0_1 = y_0_1, y_1_0 = y_1_0)\n\n# Plotting stuff.\nfig, ax = plt.subplots(dpi = 120, ncols=2, figsize = (15, 5))\n\nax[0].set_title('Auto-correlation')\nax[1].set_title('Bias')\n\nax[0].set_xlim(-1, 1)\nax[1].set_xlim(0, 1)\n\nax[0].axvline(0, color = 'red')\nax[1].axvline(0.5, color = 'red')\n\nax[0].annotate('Uncorrelated Flips', xy=(0.475, 0.5), xycoords='axes fraction', rotation = 90)\nax[1].annotate('Unbiased Flips', xy=(0.475, 0.5), xycoords='axes fraction', rotation = 90)\n\n# MCMC go brrrr\nfit = model.sample(data)\n\nax[0].hist(fit.stan_variable('rho'), edgecolor='k', alpha = 0.5)\nax[1].hist(fit.stan_variable('q'), edgecolor='k', alpha = 0.5)\n\nautocorr = fit.stan_variable('rho').mean()\nbias = fit.stan_variable('q').mean()\n\ntweet = f\"Your flips have an expected correlation of {autocorr:.2f} and your coin's bias is about {bias:.2f}\"\n\nprint(f\"Your sequence was {''.join(sequence)}\")\nprint(tweet)\n\nINFO:cmdstanpy:compiling stan file /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file.stan to exe file /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file\n\n\nINFO:cmdstanpy:compiled model executable: /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file\n\n\nWARNING:cmdstanpy:Stan compiler has produced 1 warnings:\n\n\nWARNING:cmdstanpy:\n--- Translating Stan model to C++ code ---\nbin/stanc  --o=/Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file.hpp /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file.stan\nWarning in '/Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file.stan', line 11, column 4: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.32.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\n\n--- Compiling, linking C++ code ---\nclang++ -std=c++1y -Wno-unknown-warning-option -Wno-tautological-compare -Wno-sign-compare -D_REENTRANT -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.3.9 -I stan/lib/stan_math/lib/boost_1.75.0 -I stan/lib/stan_math/lib/sundials_6.0.0/include -I stan/lib/stan_math/lib/sundials_6.0.0/src/sundials    -DBOOST_DISABLE_ASSERTS          -c -include-pch stan/src/stan/model/model_header.hpp.gch -x c++ -o /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file.o /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file.hpp\nclang++ -std=c++1y -Wno-unknown-warning-option -Wno-tautological-compare -Wno-sign-compare -D_REENTRANT -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.3.9 -I stan/lib/stan_math/lib/boost_1.75.0 -I stan/lib/stan_math/lib/sundials_6.0.0/include -I stan/lib/stan_math/lib/sundials_6.0.0/src/sundials    -DBOOST_DISABLE_ASSERTS                -Wl,-L,\"/Users/demetri/.cmdstan/cmdstan-2.29.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/Users/demetri/.cmdstan/cmdstan-2.29.2/stan/lib/stan_math/lib/tbb\"      /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file.o src/cmdstan/main.o        -Wl,-L,\"/Users/demetri/.cmdstan/cmdstan-2.29.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/Users/demetri/.cmdstan/cmdstan-2.29.2/stan/lib/stan_math/lib/tbb\"   stan/lib/stan_math/lib/sundials_6.0.0/lib/libsundials_nvecserial.a stan/lib/stan_math/lib/sundials_6.0.0/lib/libsundials_cvodes.a stan/lib/stan_math/lib/sundials_6.0.0/lib/libsundials_idas.a stan/lib/stan_math/lib/sundials_6.0.0/lib/libsundials_kinsol.a  stan/lib/stan_math/lib/tbb/libtbb.dylib stan/lib/stan_math/lib/tbb/libtbbmalloc.dylib stan/lib/stan_math/lib/tbb/libtbbmalloc_proxy.dylib -o /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file\nrm -f /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file.o\n\n\n\nINFO:cmdstanpy:CmdStan start processing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \n\n\n                                                                                \n\n\n                                                                                \n\n\n                                                                                \n\n\nINFO:cmdstanpy:CmdStan done processing.\n\n\n\n\n\nYour sequence was TTHHTHTTHTTTHTTTHTTTHTTHTHHTHHTHTHHTTTHHTHTHTTHTHHTTHTHHTHTTTHHTTHHTTHHHTHHTHTTHTHTTHHTHHHTTHTHTTTHHTTHTHTHTHTHTTHTHTHHHTTHTHTHHTHHHTHTHTTHTTHHTHTHTHTTHHTTHTHTTHHHTHTHTHTTHTTHHTTHTHHTHHHTTHHTHTTHTHTHTHTHTHTHHHTHTHTHTHHTHHTHTHTTHTTTHHTHTTTHTHHTHHHHTTTHHTHTHTHTHHHTTHHTHTTTHTHHTHTHTHHTHTTHTTHTHHTHTHTTT\nYour flips have an expected correlation of -0.36 and your coin's bias is about 0.49"
  },
  {
    "objectID": "posts/2022-05-07-flippin/index.html#stack-layerswait-thats-a-deep-learning-thing",
    "href": "posts/2022-05-07-flippin/index.html#stack-layerswait-thats-a-deep-learning-thing",
    "title": "Flippin’ Fun!",
    "section": "Stack Layers…Wait, That’s a Deep Learning Thing",
    "text": "Stack Layers…Wait, That’s a Deep Learning Thing\nNow it’s time to write a hierarchical model, and for that we need to be a little more careful. I initially thought I could just put priors on the population level autocorrelation and bias, but I quickly ran into a problem there, which I will illustrate below.\nSuppose the coin’s bias is 0 (we always get tails). Could the autocorrelation be -1? No, it couldn’t be, because that would mean our next flip would have to be a 1, but the coin’s bias is 0! This illustrates some nuance to the problem I had failed to consider but luckily did not suffer from. The autocorrelation for two binary random variables, \\(X, Y\\), is defined as\n\\[ \\rho = \\dfrac{\\alpha - q}{q(1-q)} \\]\nWhere \\(E(XY) = \\alpha\\). You see, \\(\\alpha\\) can only be so big depending on the value of \\(q\\), and if you place uniform priors on both \\(\\rho\\) and \\(q\\) you can quickly get conditional probabilities outside the unit interval. That’s exactly what happened, and I was banging my head against the wall for a night trying to figure out the bounds on \\(\\rho\\) given \\(q\\) and it quickly became a mess.\nThere is another way. Rather than place priors on \\(\\rho\\) and \\(q\\), we could place priors on the multinomial parameter and then do algebra (two equations, two unknowns) to find out expressions for \\(q\\) and \\(\\rho\\) in terms of \\(p_{1\\vert1}\\) and \\(p_{1\\vert0}\\). This isn’t ideal, because I have very good prior information on what \\(\\rho\\) and \\(q\\) should be, not on what \\(\\theta\\) should be. Whatever, let’s proceed and see how our priors look like with a prior predictive check.\nThe model is actually simpler to write in Stan than the previous model. We will place a Dirichlet prior on the multinomial parameters (one for each person who responded with a sequence), and then each sequence is multinomial with that multinomial parameter.\n\\[ \\theta_j \\sim \\mbox{Dirichlet}(\\alpha) \\]\n\\[ y_j \\sim \\mbox{Multinomial}(\\theta_j)  \\]\nThe quantities we care about can be expressed in terms of \\(\\theta\\)\n\\[ \\rho = p_{1\\vert 1} - p_{1\\vert 0}  \\]\n\\[ q = \\dfrac{2p_{1\\vert 0}}{1 - p_{1\\vert 1} + p_{1\\vert 0}}\\]\nHere is the model in Stan\ndata{\n\n    int N; //How many sequences do we have\n    int y[N, 4]; // matrix of counts of co-occurences of (1,1), (1,0), (0, 1), (0,0)\n    int do_sample; // Flag to do a prior predictive check\n\n}\nparameters{\n    vector<lower=0>[4] a;\n    simplex[4] theta[N];\n}\nmodel{\n  \n  a ~ cauchy(0, 2.5);\n  \n  if(do_sample>0){\n      for(i in 1:N){\n          theta[i] ~ dirichlet(a);\n          y[i] ~ multinomial(theta[i]);\n      }\n    }\n  \n}\ngenerated quantities{\n\n    vector[4] theta_ppc = dirichlet_rng(a);\n    real rho = theta_ppc[1] - theta_ppc[2];\n    real q = 2* theta_ppc[2]/(1 - theta_ppc[1] + theta_ppc[2]); \n    \n    real yppc[N, 4];\n    \n    for(i in 1:N){\n        yppc[i] = multinomial_rng(theta[i], sum(y[i]));\n    }\n}\nShown below are the priors for the autocorrelation and bias based on the priors I’ve used. They are a little too uncertain for my liking. Humans are pretty smart, and I don’t expect for the population average bias to be very far from 0.5. I would prefer that the prior for \\(q\\) be very tightly centered around 0.5, and that the prior for \\(\\rho\\) be tightly centered on 0, but that’s life. The model runs the 76 sequences in about 12 seconds (4 chains, 1000 warmups, 1000 samples) and diagnostics don’t indicate any pathological behavior. Let’s look at the joint posterior.\n\n\n\n\n\n\n\n(a) Priors\n\n\n\n\n\n\n\n(b) Posteriors\n\n\n\n\nFigure 1: Prior and Posterior distributions for the model.\n\n\nThe take home here is that the sequences are largely consistent with an unbiased and uncorrelated coin. The expected correlation is negative (-0.05) meaning humans are more likely to switch from heads to tails, or tails to heads, and the expected bias is 0.53 meaning people seem to favor heads for some reason. The results are largely unsurprising, and I really wish I could place priors on \\(\\rho\\) and \\(q\\) directly so that my model really does reflect the state of my knowledge, but this is good enough for now."
  },
  {
    "objectID": "posts/2022-05-07-flippin/index.html#conclusion",
    "href": "posts/2022-05-07-flippin/index.html#conclusion",
    "title": "Flippin’ Fun!",
    "section": "Conclusion",
    "text": "Conclusion\nI’ve been thinking about this problem for a while after I watched a talk by Gelman where he mentioned estimating the autocorrelation between bernoulli experiments in passing (he was talking about model criticism and offering examples of other things to check our model with). I’m moderately happy with the hierarchical model, and the results make a lot of sense. Humans are pretty smart, and we have an intuitive sense for what random looks like. I’m willing to bet that if people submitted longer sequences, we would have a more precise estimate of the bias/correlation.\nOne thing I’ve shirked is really detailed model criticism, though I have an idea of how I would do that. In that answer on cross validated, COOLSerdash posts a REALLY COOL way to visualize the sequence data. They plot the number of runs (sequences of consecutive heads or tails) against the longest run in the sequence. I think this would make for an excellent way to check that our model has learned the correct autocorrelation for each individual who participated in the game (though I think the sequences were too short to have a precise estimate)."
  },
  {
    "objectID": "posts/2018-08-31-combinatorics/index.html",
    "href": "posts/2018-08-31-combinatorics/index.html",
    "title": "Neat Litle Combinatorics Problem",
    "section": "",
    "text": "I’ll cut right to it. Consider the set \\(S = (49, 8, 48, 15, 47, 4, 16, 23, 43, 44, 42, 45, 46 )\\). What is the expected value for the minimum of 6 samples from this set?\nWe could always just sample form the set to estimate the expected value. Here is a python script to do just that.\n\nimport numpy as np\nx = np.array([49, 8, 48, 15, 47, 4, 16, 23, 43, 44, 42, 45, 46])\n\nmins = []\nfor _ in range(1000):\n    mins.append(np.random.choice(x,size = 6, replace = False).min())\n\nprint(np.mean(mins))\n\n9.098\n\n\nBut that is estimating the mean. We can do better and directly compute it. Here is some python code to create all subsets from \\(S\\) of size 6. Then, we simply take out the minimum from each subset and compute the mean.\n\nimport numpy as np\nfrom itertools import combinations, groupby\n\nx = np.array([49, 8, 48, 15, 47, 4, 16, 23, 43, 44, 42, 45, 46])\nx = np.sort(x)\n\nc = list(combinations(x,6))\n\nmins = list(map(lambda x: x[0], c))\n\ns = 0\nfor k, g in groupby(sorted(mins)):\n    s+=k*(len(list(g))/len(mins))\n\nprint( s )\n\n8.818181818181818\n\n\nThe script returns 8.18 repeating. Great, but we can do even better! If we can compute the probability density function, we can compute the mean analytically. Let’s consider a smaller problem to outline the solution.\nLet our set in question be \\((1,2,3,4,5)\\). Let the minimum of a sample of 3 numbers from this set be the random variable \\(z\\). Now, note there are \\(\\binom{5}{3} = 10\\) ways to choose 3 elements from a set of 5.\nHow many subsets exist where the minimum is 1? Well, if I sampled 1, then I would still have to pick 2 numbers from a possible 4 numbers larger than 1. There are \\(\\binom{4}{2}\\) ways to do this. So \\(p(z=1) = \\binom{4}{2} / \\binom{5}{3}\\).\nIn a similar fashion, there are \\(\\binom{3}{2}\\) subsets where 2 is the minimum, and \\(\\binom{2}{2}\\) subsets where 3 is the minimum. There are no subsets where 4 or 5 are the minimum (why?). So that means the expected minimum value for this set would be\n\\[\\operatorname{E}(z) = \\dfrac{ \\sum_{k = 1}^{3} k\\binom{5-k}{2} }{\\binom{5}{3}}  \\]\nWhatever that sum happens to be. Here is how you could code up the analytic solution to our problem.\n\nimport numpy as np\nfrom scipy.special import binom\n\nx = np.array([ 4, 8, 15, 16, 23, 42, 43, 44, 45, 46, 47, 48, 49])\nx = np.sort(x)\n\nsample_size =6\nsample_space = x[:-(sample_size-1)]\nE = 0\nfor i,s in enumerate(sample_space,start = 1):\n\n    E+= s*binom(x.size-i,sample_size-1)\n\nprint(E/binom(x.size, sample_size))\n\n8.818181818181818\n\n\nFull disclosure, this was on a job application (literally, on the job application), so sorry KiK for putting the answer out there, but the question was too fun not to write up!"
  },
  {
    "objectID": "posts/2022-06-22-new-blog/index.html",
    "href": "posts/2022-06-22-new-blog/index.html",
    "title": "This Is A Quarto Blog",
    "section": "",
    "text": "This is a quarto blog.\nThat means I can write code in either R or python directly in the blog post and have it execute. So when you see something like\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nt = np.linspace(0, 1)\nplt.plot(t, np.sin(2*np.pi*t), color='k')\nplt.plot(t, np.cos(2*np.pi*t), color='red')\nplt.title(\"Here is a plot\")\n\n\n\n\nFigure 1: Whoa, check it out!\n\n\n\n\nThat is the code that is actually executed. That means the blog is more reproducible and will have fewer errors. It also means you can go directly to the repo for my blog and clone the post to start tinkering. No more linking to other gitrepos, no more copying and pasting code with errors.\nDid I mention I can write both R and python?\n\nt = seq(0, 1, 0.01)\nplot(t, sin(2*pi*t), main='Here is another plot!', xlab='', ylab='', type='l')\nlines(t, cos(2*pi*t), col='red')\n\n\n\n\nFigure 2: Whoa, check it out again!\n\n\n\n\nI can also reference figures (like Figure 2 and Figure 1)"
  },
  {
    "objectID": "posts/2022-07-06-gsd/index.html",
    "href": "posts/2022-07-06-gsd/index.html",
    "title": "Interim Analysis & Group Sequential Designs Pt 1",
    "section": "",
    "text": "Special thanks to Jacob Fiksel for writing a great blog post which inspired me to write my own.\nAt Zapier, AB testing kind of has a bad rap. AB testing is perceived as slow – sometimes taking up to a month to complete a single test– with the chance that we don’t get a definitive result (i.e. we fail to reject the null). One of our priorities (and hence my priority) is to find a way to speed up AB testing so we can learn faster.\nPeeking is one way to do that. Peeking involves testing the experimental data before the end of the experiment (“peeking” at the results to see if they indicate a change). As you may know from other popular posts on the matter, or from sophomore stats, this can inflate the type one error. That’s a real shame, because peeking is a really attractive way to end an experiment early and save some time. Additionally, people are curious! They want to know how things are going. Fortunately, there are ways to satisfy the urge to peek while preserving the type one error rate.\nOne way to peek while preserving the type one error rate is through Group Sequential Designs (GSDs). This series of blog posts is intended to delve into some of the theory of GSDs. To me, theoretical understanding – knowing why something works, or at least being able to understand how in principle I could do this myself – is the key to learning. I’m happy to just do this in isolation, but I bet someone else may benefit too.\nI’m working mainly from this book, but I don’t anticipate I will discuss the entirety of the book. I really want to know a few key things:"
  },
  {
    "objectID": "posts/2022-07-06-gsd/index.html#goal-for-this-post",
    "href": "posts/2022-07-06-gsd/index.html#goal-for-this-post",
    "title": "Interim Analysis & Group Sequential Designs Pt 1",
    "section": "Goal For This Post",
    "text": "Goal For This Post\nWe know that under “peeking conditions” – just testing the data as they roll in – inflates the type one error rate. In this post, I want to understand why that happens. Like…where is the problem exactly? Where will be our theoretical basis for attacking the problem of controlling the type one error rate?\nBut first, a little background on GSDs."
  },
  {
    "objectID": "posts/2022-07-06-gsd/index.html#background",
    "href": "posts/2022-07-06-gsd/index.html#background",
    "title": "Interim Analysis & Group Sequential Designs Pt 1",
    "section": "Background",
    "text": "Background\nThe “G” in GSD means that the hypothesis test is performed on groups of observations. Given a maximum number of groups \\(K\\), the sample size of \\(k^{th}\\) each group is \\(n_k\\).\nThe “S” in GSD means the test is performed sequentially. If after observing the \\(k^{th}\\) group the test statistic (computed using all the data observed up to that point) is beyond some threshold, then the null is rejected and the experiment is finished. If not, the next group of observations is made and added to the existing data, wherein the process continues until the final group has been observed. If after observing the final group the test statistic does not exceed the threshold, then we fail to reject the null. The process for \\(K=2\\) is illustrated in Figure 1.\n\n\nCode\nflowchart TD\n  A[Observe Group k=1] --> B[Perform Test]\n  B --> C{Data From k=1 \\n Significant?}\n  C -- Yes --> D[Reject Null]\n  C -- No --> E[Observe Group k=2]\n  E --> G{Data From k=1 and \\n k=2 Significant?}\n  G -- Yes --> D\n  G -- No --> H[Fail To Reject Null]\n\n\n\n\nFigure 1: A GSD for \\(K=2\\)\n\nflowchart TD\n  A[Observe Group k=1] --> B[Perform Test]\n  B --> C{Data From k=1 \\n Significant?}\n  C -- Yes --> D[Reject Null]\n  C -- No --> E[Observe Group k=2]\n  E --> G{Data From k=1 and \\n k=2 Significant?}\n  G -- Yes --> D\n  G -- No --> H[Fail To Reject Null]"
  },
  {
    "objectID": "posts/2022-07-06-gsd/index.html#some-math-on-means",
    "href": "posts/2022-07-06-gsd/index.html#some-math-on-means",
    "title": "Interim Analysis & Group Sequential Designs Pt 1",
    "section": "Some Math on Means",
    "text": "Some Math on Means\nMeans are a fairly standard place to start for a statsitical test, so we will start there too. Let \\(X_{k, i}\\) be the \\(i^{th}\\) observation in the \\(k^{th}\\) group. Then the mean of group \\(k\\) is\n\\[ \\bar{X}_k = \\dfrac{1}{n_k} \\sum_{i=1}^{n_{k}} X_{k, i} \\]\nSince we are accumulating data, let’s write the cumulative mean up to and including group \\(k\\) as \\(\\bar{X}^{(k)}\\), and let the cumulative standard deviation up to and including group \\(k\\) be \\(\\sigma^{(k)}\\). We can actually write \\(\\bar{X}^{(k)}\\) in terms of the group means \\(\\bar{X}_{k}\\) using some algebra. Its just a weighted mean of the previous \\(\\bar{X}_{k}\\) weighted by the sample size.\n\\[\n\\bar{X}^{(k)} =  \\dfrac{\\sum_{\\tilde{k} = 1}^{k^\\prime} n_{\\tilde{k}} \\bar{X}_{\\tilde{k}}}{\\sum_{\\tilde{k} = 1}^{k^\\prime} n_{\\tilde{k}}}\n\\tag{1}\\]"
  },
  {
    "objectID": "posts/2022-07-06-gsd/index.html#a-simple-example",
    "href": "posts/2022-07-06-gsd/index.html#a-simple-example",
    "title": "Interim Analysis & Group Sequential Designs Pt 1",
    "section": "A Simple Example",
    "text": "A Simple Example\nRemember that our goal is to understand why the type one error rate increases when we peek as data accumulates, as we might do in an AB test. Answering how much is a little easier, so let’s do that first. Let’s do so by analyzing a \\(K=2\\) GSD where we assume:\n\nThat each group has the same sample size \\(n_1 = n_2 = n\\).\nThat the data we observe are IID bernoulli trials \\(X_{k, i} \\sim \\operatorname{Bernoulli}(p=0.5)\\) for \\(k=1, 2\\) and \\(j=1, \\dots, n\\).\nThat our false postie rate \\(\\alpha = 0.05\\)\n\n\nHow Much Does The Type One Inflate?\nLet’s just simulate data under the assumptions above. At each stage, let’s test the null that \\(H_0: p=0.5\\) against \\(H_A: p \\neq 0.5\\) and see how frequently we reject the null. In our simulation, we will assume “peeking” conditions, meaning we’re just going to do a test of proportions at each stage.\n\nlibrary(tidyverse)\n\nset.seed(0)\n\n# Simulation Parameters\np<- 0.5\nn<- 250\nnsims <- as.integer((1/0.01)^2)\n\n# Run the simulation\nsims<-rerun(nsims, {\n  # K=1\n  x1 <- rbinom(1, n, p)\n  # K=2, accumulating data from each state\n  x2 <- x1 + rbinom(1, n, p)\n  \n  # Compute some various quntities we will need, like the Z score\n  K <- str_c('K=',1:2)\n  X <- c(x1, x2) / ((1:2)*n)\n  mu <- p\n  sds <- sqrt(p*(1-p)/(n*1:2))\n  Z <- (X-p)/sds\n  reject <- abs(Z)>1.96\n  \n  tibble(K, X, mu, sds, Z, reject)\n}) %>% \n  bind_rows(.id='sim')\n\nfpr<-sims %>% \n  group_by(sim) %>% \n  summarise(result=any(reject)) %>% \n  summarise(fpr = mean(result)) %>% \n  pull(fpr)\n\nFrom our simulation, we reject the null around 8.6% of the time. That is certainly higher than the nominal 5%, but if we recall our sophomore stats classes, isn’t there a 9.8% (\\(1-0.95^2\\)) chance we reject the null?\nThe 8.6% isn’t simulation error. We forgot that \\(\\bar{X}^{(1)}\\) and \\(\\bar{X}^{(2)}\\) are correlated. The correlation between \\(\\bar{X}^{(1)}\\) and \\(\\bar{X}^{(2)}\\) makes intuitive sense. If the sample mean for the first group is small, then the accumulated mean is also likely to be small than if we were to just take a new sample. Let’s take a more detailed look at Equation 1. Note that\n\\[ \\bar{X}^{(2)} = \\dfrac{n_1 \\bar{X}_1 + n_2\\bar{X}_2}{n_1 + n_2} \\>.\\]\n\\(\\bar{X}^{(1)}\\) (which is just \\(\\bar{X}_1\\) ) appears in the expression for \\(\\bar{X}^{(2)}\\). In the extreme case where \\(n_2=1\\), the stage 2 mean is going to be \\(n_1 \\bar{X}_1/(n_1+1) + X_{2, 1}/(n_1+1)\\). How much could a single observation change the sample mean? It depends on the observation, but also on how big that sample is. The stuff you learned in sophmore stats about type one error inflating like \\(1 - (1-\\alpha)^k\\) assumes the test statistics are independent. So where does the 8.6% come from? To answer that, we need to understand the joint distribution of the \\(\\bar{X}^{(k)}\\).\n\n\nWhy The Type One Inflates\nThe assumptions we made above allow us to get a little analytic traction. We know that the sampling distribution of \\(\\bar{X}^{(1)}\\) and \\(\\bar{X}^{(2)}\\) are asymptotic normal thanks to the CLT\n\\[ \\bar{X}^{(1)} \\sim \\operatorname{Normal}\\left(p, \\dfrac{p(1-p)}{n}\\right)  \\]\n\\[ \\bar{X}^{(2)}\\sim \\operatorname{Normal}\\left( p, \\dfrac{p(1-p)}{2 n} \\right)  \\]\n\n\nCode\nmy_blue <- rgb(45/250, 62/250, 80/250, 1)\ntheme_set(theme_classic())\n\n\nsims %>% \n  ggplot(aes(X))+\n  geom_histogram(aes(y=..density..), fill = 'light gray', color = 'black')+\n  facet_wrap(~K) + \n  geom_line(aes(y = dnorm(X,\n                          mean = mu,\n                          sd = sds[PANEL])),\n            color = my_blue, \n            size = 1)+\n  theme(\n    panel.grid.major = element_line()\n  )+\n  labs(y='Density',\n       x = expression(bar(X)^(k)))\n\n\n\n\n10,000 simulations of a \\(K=2\\) GSD. Each group has 250 observations. Note that \\(\\bar{X}^{(2)}\\) has smaller standard error due to the fact that 500 (250 + 250) observations are used in the computation. Sampling distributions show in blue.\n\n\n\n\n\nConsider the random vector \\(\\theta = \\left(\\bar{X}^{(1)}, \\bar{X}^{(2)}\\right)\\). Since each components has a normal marginal then the joint must be multivariate normal\n\\[ \\theta \\sim \\mathcal{N}(\\mathbf{p}, \\Sigma) \\]\nwith mean \\(\\mathbf{p} = (p,p)\\) and covariance1\n\\[ \\Sigma= p(1-p)\\begin{bmatrix}\n\\dfrac{1}{n_1} &  \\dfrac{1}{n_1 + n_2} \\\\\n\\dfrac{1}{n_1 + n_2} & \\dfrac{1}{n_1 + n_2}\n\\end{bmatrix}\n\\]\n\n\nCode\nsigma_1 <- sqrt(qchisq(0.95, 2))\nsigma_2 <- sqrt(qchisq(0.99, 2))\nsig<- p*(1-p) * matrix(c(1/n, 1/(2*n), 1/(2*n), 1/(2*n) ), nrow = 2)\ntt <- seq(0, 1, 0.01)\nx <- cos(2*pi*tt)\ny <- sin(2*pi*tt)\nR <- cbind(x,y)\ne = eigen(sig)\nV = sqrt(diag(e$values))\n\nlevel_curve_1 <- sigma_1*R %*% (e$vectors %*% V %*% t(e$vectors)) + p\ncolnames(level_curve_1) <- c(\"X1\", \"X2\")\nlevel_curve_1 <- as_tibble(level_curve_1)\nlevel_curve_2 <- sigma_2*R %*% (e$vectors %*% V %*% t(e$vectors)) + p\ncolnames(level_curve_2) <- c(\"X1\", \"X2\")\nlevel_curve_2 <- as_tibble(level_curve_2)\n\njoint <-sims %>% \n  select(sim, K, X) %>% \n  pivot_wider(names_from='K', values_from='X') %>% \n  rename(X1 = `K=1`, X2=`K=2`) %>% \n  select(-sim) %>% \n  sample_n(1000)\n\njoint %>% \n  ggplot(aes(X1, X2))+\n  geom_point(color = 'dark gray', fill='gray', alpha = 0.5, shape=21)+\n  geom_path(data=level_curve_1, aes(X1, X2), color = my_blue, linetype='dashed')+\n  geom_path(data=level_curve_2, aes(X1, X2), color = my_blue)+\n  lims(x=c(.4, .6), y=c(.4, .6))+\n  theme(\n    panel.grid.major = element_line(),\n    aspect.ratio = 1\n  )+\n  labs(x=expression(bar(X)^(1)),\n       y=expression(bar(X)^(2)))\n\n\n\n\nFigure 2: 1000 samples from the density of \\(\\theta\\). Dashed line indicates where region of 95% probability, solid line indicates region of 99% probability.\n\n\n\n\n\nNow that we know the joint sampling distribution for our statistics of interest (namely \\(\\bar{X}^{(1)}\\) and \\(\\bar{X}^{(2)}\\)), let’s examine when we would reject the null under “peeking” conditions. For brevity, let’s call \\(Z^{(k)}\\) the standardized cumulative means. Then we would reject the null under “peeking” conditions if \\(\\Big\\vert Z^{(k)} \\Big\\vert > 1.96\\) for at least one \\(k=1, 2\\). As a probabilistic statement, we want to know\n\\[ Pr\\left( \\Big\\vert Z^{(1)} \\Big\\vert > 1.96 \\cup 1.96 < \\Big\\vert Z^{(2)} \\Big\\vert \\right) \\>. \\]\nBecause the joint is multivariate normal, we can compute this probability directly. However, I’m just going to simulate it.\n\n# Standardize the MVN by converting covariance matrix into a correlation matrix\nD <- solve(diag(sqrt(diag(sig))))\ncormat <- D %*% sig %*% D\nZ<- MASS::mvrnorm((1/0.001)^2, rep(0, 2), cormat)\n\n\nz1 = abs(Z[, 1])>1.96\nz2 = abs(Z[, 2])>1.96\n\nfpr <- mean(z1|z2)\n\nand we get something like 8.3%. But that doesn’t answer why, that just means I did my algebra correctly. As always, a visualization might help. take a look at Figure 3. The shaded regions show the areas where the null would be rejected. These are the areas we would make a false positive. The dots indicate the standardized draws from the density of \\(\\theta\\). Remember, this distribution is the null distribution for our GSD – these are draws from \\(\\theta\\) when \\(H_0\\) is true. And now here is the important part…\n\nThe shaded region depends on critical values we use for each test in the sequence. If we naively use \\(Z_{1-\\alpha/2}\\) as the critical value for each group as in “peeking” conditions, then the shaded region is too big!\n\n\n\nCode\nsims %>% \n  select(sim, K, Z) %>% \n  pivot_wider(names_from='K', values_from='Z') %>% \n  rename(Z1 = `K=1`, Z2=`K=2`) %>% \n  select(-sim) %>% \n  sample_n(1000) %>% \n  ggplot(aes(Z1, Z2))+\n  geom_point(color = 'dark gray', fill='gray', alpha = 0.5, shape=21)+\n  scale_x_continuous(limits = c(-5, 5), expand=c(0,0))+\n  scale_y_continuous(limits = c(-5, 5), expand=c(0,0))+\n  annotate(\"rect\", xmin = -5, xmax = -1.96, ymin = -5, ymax = 5, alpha = .5, fill=my_blue)+\n  annotate(\"rect\", xmin = 1.96, xmax = 5, ymin = -5, ymax = 5, alpha = .5, fill=my_blue)+\n  annotate(\"rect\", xmin = -1.96, xmax = 1.96, ymin = 1.96, ymax = 5, alpha = .5, fill=my_blue)+\n  annotate(\"rect\", xmin = -1.96, xmax = 1.96, ymin = -1.96, ymax = -5, alpha = .5, fill=my_blue)+\n  geom_hline(aes(yintercept=-1.96), linetype='dashed')+\n  geom_hline(aes(yintercept=1.96), linetype='dashed')+\n  geom_vline(aes(xintercept=-1.96), linetype='dashed')+\n  geom_vline(aes(xintercept=1.96), linetype='dashed')+\n  theme(\n    panel.grid.major = element_line(),\n    aspect.ratio = 1\n  )+\n  labs(x=expression(Z^(1)),\n       y=expression(Z^(2)))\n\nfind_region <- function(za){\n  z1 = abs(Z[, 1])>za\n  z2 = abs(Z[, 2])>za\n\n  fpr <- mean(z1|z2)\n  \n  (fpr - 0.05)^2\n}\n\nresult<-optimize(find_region, interval=c(0, 3))\n\n\n\n\nFigure 3: Standardized draws from the density of \\(\\theta\\). Shaded regions indicate where the null hypothesis would be rejected under “peeking” conditions. The shaded region has approximately 8.5% probability mass and represents the false positive rate. We need to select a different region so that the shaded region has probability mass closer to 5%.\n\n\n\n\n\n\n\n\nThat is the why! When we naively just run our test each time we peek, we are defining a region in \\(\\theta\\) space which has too much probability. Peeking is fine, you just have to be careful in defining your rejection region in \\(\\theta\\) space. Defining a better rejection region isn’t too hard, and we can do it using a numerical search. When we do so, we find that using a critical value of 2.18 results in a type one error closer to the desired 5%. However, we’re implicitly restricted ourselves to having the threshold be the same for each group. That doesn’t have to be the case as we will see eventually."
  },
  {
    "objectID": "posts/2022-07-06-gsd/index.html#conclusion",
    "href": "posts/2022-07-06-gsd/index.html#conclusion",
    "title": "Interim Analysis & Group Sequential Designs Pt 1",
    "section": "Conclusion",
    "text": "Conclusion\nWe’ve done algebra, and it wasn’t for nothing. It have us insight into exactly what is going on and why the type one error increases under peeking. We also know that there is a way to fix it, we just need to define the shaded region a little more carefully. This will lead us to talk about alpha spending and various alpha spending functions."
  },
  {
    "objectID": "posts/2022-07-06-gsd/index.html#appendix",
    "href": "posts/2022-07-06-gsd/index.html#appendix",
    "title": "Interim Analysis & Group Sequential Designs Pt 1",
    "section": "Appendix",
    "text": "Appendix\n\nCovariance Calculation\nThe diagonals of the covariance matrix \\(\\Sigma\\) are simply the variances of the marginal distributions.\n\\[ \\Sigma_{1, 1} = \\dfrac{p(1-p)}{n_1} \\]\n\\[ \\Sigma_{2, 2} = \\dfrac{p(1-p)}{n_1 + n_2} \\]\nWhat remains is the covariance, which can be obtained with some covariance rules\n\\[ \\begin{align} \\operatorname{Cov}\\left(\\bar{X}^{(1)}, \\bar{X}^{(2)}\\right) &= \\operatorname{Cov}\\left(\\bar{X}_1, \\dfrac{n_1\\bar{X}_1 + n_2\\bar{X}_2}{n_1 + n_2}\\right)\\\\\n&=\\dfrac{n_1}{n_1 + n_2}\\operatorname{Var}(\\bar{X_1}) + \\dfrac{n_2}{n_1+n_2}\\operatorname{Cov}(\\bar{X}_1, \\bar{X}_2)\n\\end{align}\\]\nSince the groups are independent, the sample means are also independent (but the cumlative means are not). Meaning \\(\\operatorname{Cov}(\\bar{X}_1, \\bar{X}_2)=0\\) so\n\\[ \\operatorname{Cov}\\left(\\bar{X}^{(1)}, \\bar{X}^{(2)}\\right) = \\dfrac{p(1-p)}{n_1 + n_2} \\]"
  },
  {
    "objectID": "posts/2021-11-23-bootstrap/index.html",
    "href": "posts/2021-11-23-bootstrap/index.html",
    "title": "Hacking Sklearn To Do The Optimism Corrected Bootstrap",
    "section": "",
    "text": "Its late, I can’t sleep, so I’m writing a blog post about the optimism corrected bootstrap.\nIn case you don’t know, epidemiology/biostatistics people working on prediction models like to validate their models in a slightly different way than your run-in-the-mill data scientist. Now, it should be unsurprising that this has generated some discussion between ML people and epi/biostats people, but I’m going to ignore this for now. I’m going to assume you have good reason for wanting to do the optimism corrected bootstrap in python, especially with sklearn, and if you don’t and want to discuss the pros and cons fo the method instead then lalalalalala I can’t hear you."
  },
  {
    "objectID": "posts/2021-11-23-bootstrap/index.html#the-optimism-corrected-bootstrap-in-7-steps",
    "href": "posts/2021-11-23-bootstrap/index.html#the-optimism-corrected-bootstrap-in-7-steps",
    "title": "Hacking Sklearn To Do The Optimism Corrected Bootstrap",
    "section": "The Optimism Corrected Bootstrap in 7 Steps",
    "text": "The Optimism Corrected Bootstrap in 7 Steps\nAs a primer, you might want to tread Alex Hayes’ pretty good blog post about variants of the bootstrap for predictive performance. It is more mathy than I care to be right now and in R should that be your thing.\nTo do the optimism corrected bootstrap, follow these 7 steps as found in Ewout W. Steyerberg’s Clinical Prediction Models.\n\nConstruct a model in the original sample; determine the apparent performance on the data from the sample used to construct the model.\nDraw a bootstrap sample (Sample*) with replacement from the original sample.\nConstruct a model (Model) in Sample, replaying every step that was done in the original sample, especially model specification steps such as selection of predictors. Determine the bootstrap performance as the apparent performance of Model* in Sample.\nApply Model* to the original sample without any modification to determine the test performance.\nCalculate the optimism as the difference between bootstrap performance and test performance.\nRepeat steps 1–4 many times, at least 200, to obtain a stable mean estimate of the optimism.\nSubtract the mean optimism estimate (step 6) from the apparent performance (step 1) to obtain the optimism-corrected performance estimate.\n\nThis procedure is very straight forward, and could easily be coded up from scratch, but I want to use as much existing code as I can and put sklearn on my resume, so let’s talk about what tools exist in sklearn to do cross validation and how we could use them to perform these steps."
  },
  {
    "objectID": "posts/2021-11-23-bootstrap/index.html#cross-validation-in-sklearn",
    "href": "posts/2021-11-23-bootstrap/index.html#cross-validation-in-sklearn",
    "title": "Hacking Sklearn To Do The Optimism Corrected Bootstrap",
    "section": "Cross Validation in Sklearn",
    "text": "Cross Validation in Sklearn\nWhen you pass arguments like cv=5 in sklearn’s many functions, what you’re really doing is passing 5 to sklearn.model_selection.KFold. See sklearn.model_selection.cross_validate which calls a function called ‘check_cv’ to verify this. KFold.split returns a generator, which when passed to next yields a pair of train and test indicides. The inner workings of KFold might look something like\nfor _ in range(number_folds):\n    train_ix = make_train_ix()\n    test_ix = make_test_ix()\n    yield (trian_ix, test_ix)\nThose incidies are used to slice X and y to do the cross validation. So, if we are going to hack sklearn to do the optimisim corrected bootstrap for us, we really just need to write a generator to give me a bunch of indicies. According to step 2 and 3 above, the train indicies need to be resamples of np.arange(len(X)) (ask yourself “why?”). According to step 4, the test indicies need to be np.arnge(len(X)) (again….”why?“).\nOnce we have a generator to do give us our indicies, we can use sklearn.model_selection.cross_validate to fit models on the resampled data and predict on the original sample (step 4). If we pass return_train_score=True to cross_validate we can get the bootstrap performances as well as the test performances (step 5). All we need to do then is calculate the average difference between the two (step 6) and then add this quantity to the apparent performance we got from step 1.\nThat all sounds very complex, but the code is decieptively simple."
  },
  {
    "objectID": "posts/2021-11-23-bootstrap/index.html#the-code-i-know-you-skipped-here-dont-lie",
    "href": "posts/2021-11-23-bootstrap/index.html#the-code-i-know-you-skipped-here-dont-lie",
    "title": "Hacking Sklearn To Do The Optimism Corrected Bootstrap",
    "section": "The Code (I Know You Skipped Here, Don’t Lie)",
    "text": "The Code (I Know You Skipped Here, Don’t Lie)\n\nimport numpy as np\nfrom numpy.core.fromnumeric import mean\nfrom sklearn.model_selection import cross_validate, RepeatedKFold\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.utils import resample\n\n# Need some data to predict with\ndata = load_diabetes()\nX, y = data['data'], data['target']\n\nclass OptimisimBootstrap():\n\n    def __init__(self, n_bootstraps):\n\n        self.n_bootstraps = n_bootstraps\n\n    def split(self, X, y,*_):\n\n        n = len(X)\n        test_ix = np.arange(n)\n\n        for _ in range(self.n_bootstraps):\n            train_ix = resample(test_ix)\n            yield (train_ix, test_ix)\n\n# Optimism Corrected\nmodel = LinearRegression()\nmodel.fit(X, y)\napparent_performance = mean_squared_error(y, model.predict(X))\n\nopt_cv = OptimisimBootstrap(n_bootstraps=250)\nmse = make_scorer(mean_squared_error)\ncv = cross_validate(model, X, y, cv=opt_cv, scoring=mse, return_train_score=True)\noptimism = cv['test_score'] - cv['train_score']\noptimism_corrected = apparent_performance + optimism.mean()\nprint(f'Optimism Corrected: {optimism_corrected:.2f}')\n\n# Compare against regular cv\ncv = cross_validate(model, X, y, cv = 10, scoring=mse)['test_score'].mean()\nprint(f'regular cv: {cv:.2f}')\n\n# Compare against repeated cv\ncv = cross_validate(model, X, y, cv = RepeatedKFold(n_splits=10, n_repeats=100), scoring=mse)['test_score'].mean()\nprint(f'repeated cv: {cv:.2f}')\n\nOptimism Corrected: 2999.04\nregular cv: 3000.38\n\n\nrepeated cv: 3009.02\n\n\nThe three estimates (optimism corrected, 10 fold, and repeated 10 fold) should be reasonably close together, but uh don’t run this code multiple times. You might see that the optimism corrected estimate is quite noisy meaning I’m either wrong or that twitter thread I linked to might have some merit."
  },
  {
    "objectID": "posts/2022-11-16-bootstrapping-in-sql/index.html",
    "href": "posts/2022-11-16-bootstrapping-in-sql/index.html",
    "title": "Bootstrapping in SQL",
    "section": "",
    "text": "Remember the “Double Down” from KFC? It was bacon and cheese sandwiched between two deep fried pieces of chicken. I’m willing to bet we all conceived of it independently (as in “LOL wouldn’t it be crazy if…”), realistically could have made it ourselves, but were smart enough not to because “sure we could but… why?”.\nThis blog post is the Double Down of Statistics."
  },
  {
    "objectID": "posts/2022-11-16-bootstrapping-in-sql/index.html#bootstrapping-in-sql.-no-really.",
    "href": "posts/2022-11-16-bootstrapping-in-sql/index.html#bootstrapping-in-sql.-no-really.",
    "title": "Bootstrapping in SQL",
    "section": "Bootstrapping in SQL. No, Really.",
    "text": "Bootstrapping in SQL. No, Really.\nTwo things which have made my stats like easier:\n\nBootstrapping, and\nTidy data\n\nR’s rsample::bootstraps seems to do one in terms of the other. Take a look at the output of that function. We have, in essence, one bootstrapped dataset per row.\n\nlibrary(tidyverse)\nlibrary(rsample )\n\nrsample::bootstraps(cars)\n\n# Bootstrap sampling \n# A tibble: 25 × 2\n   splits          id         \n   <list>          <chr>      \n 1 <split [50/18]> Bootstrap01\n 2 <split [50/16]> Bootstrap02\n 3 <split [50/16]> Bootstrap03\n 4 <split [50/18]> Bootstrap04\n 5 <split [50/21]> Bootstrap05\n 6 <split [50/19]> Bootstrap06\n 7 <split [50/20]> Bootstrap07\n 8 <split [50/18]> Bootstrap08\n 9 <split [50/20]> Bootstrap09\n10 <split [50/19]> Bootstrap10\n# … with 15 more rows\n\n\nIn theory, I could unnest this and have one observation from each bootstrap per row, with id serving as an indicator to tell me to which resample the observation belongs to. Which means…I could theoretically bootstrap in SQL.\nSo, let’s do that. I’m going to use duckdb because its SQL-like and has some stats functions (whereas SQLite does not).\nLet’s sample some pairs \\((x_i, y_i)\\) from the relationship \\(y_i = 2x_i + 1 + \\varepsilon_i\\), where the \\(\\varepsilon\\) are iid draws from a standard Gaussian Let’s stick that in a dataframe along with a row number column into our database. The data are shown in Table 1.\n\n\n\n\n\n\n\nTable 1:  My Data \n \n  \n    original_rownum \n    x \n    y \n  \n \n\n  \n    1 \n    2.09 \n    5.73 \n  \n  \n    2 \n    0.97 \n    3.58 \n  \n  \n    3 \n    0.70 \n    1.37 \n  \n  \n    4 \n    1.44 \n    4.84 \n  \n  \n    5 \n    2.07 \n    4.14 \n  \n  \n    6 \n    1.67 \n    3.09 \n  \n\n\n\n\n\n\nTo bootstrap in SQL, we need to emulate what the unnested results of rsample::bootstraps would look like. We need rows of (strap_id, original_data_rownum, and bootstrap_rownum). Let’s discuss the interpretation and purpose of each column.\n\nstrap_id plays the part of id in rsample::bootstraps. We’re just going to group by this column and aggregate the resampled data later.\noriginal_data_rownum doesn’t really serve a purpose. It contains integers 1 through \\(N\\) (where \\(N\\) is our original sample size). We can do a cross join to get pairs (strap_id, original_data_rownum). This means there will be \\(N\\) copies of strap_id, meaning we can get \\(N\\) resamples of our data for each strap_id.\nbootstrap_rownum is a random integer between 1 and \\(N\\). This column DOES serve a purpose, its basically the sampling with replacement bit for the bootstrap. Now, duckdb doesn’t have a function to sample random integers. To do this, I basically sample random numbers on the unit interval do some arithmetic to turn those into integers.\n\nLet’s set that up now. The hardest part really is creating a sequence of numbers, but duckdb makes that pretty easy.\n\nQuery To Make strap_id\n\n-- Set up strap_ids in a table\nCREATE OR REPLACE TABLE strap_ids(strap_id INTEGER);\n-- Do 1000 bootstraps\nINSERT INTO strap_ids(strap_id) select * from range(1, 1001, 1);\n\n\n\n\n\n\n\n\nTable 2:  Contents of strap_ids. These play the role of id in the rsample output. \n \n  \n    strap_id \n  \n \n\n  \n    1 \n  \n  \n    2 \n  \n  \n    3 \n  \n  \n    4 \n  \n  \n    5 \n  \n\n\n\n\n\n\n\n\nQuery To Make original_data_rownum\n\n-- Set up original_data_rownum in a table\nCREATE OR REPLACE TABLE original_data_rownum(original_rownum INTEGER);\n-- I have 2500 observations in my data\nINSERT INTO original_data_rownum(original_rownum) select * from range(1, 2500+1, 1);\n\n\n\n\n\n\n\n\nTable 3:  Contents of original_data_rownum. These play the role of id in the rsample output. \n \n  \n    original_rownum \n  \n \n\n  \n    1 \n  \n  \n    2 \n  \n  \n    3 \n  \n  \n    4 \n  \n  \n    5 \n  \n\n\n\n\n\n\nOk, now we have the two tables strap_ids and original_data_rownum. All we need to do now is cross join then, and do the random number magic. That’s shown below in table Table 4.\n\n\nQuery To Make bootstrap_rownum\n\ncreate or replace table resample_template as \nselect\n  strap_ids.strap_id,\n  original_data_rownum.original_rownum,\n  -- I have 2500 observations in my data\n  round( -0.5 + 2501*random()) as bootstrap_rownum,\nfrom\n  strap_ids\ncross join \n  original_data_rownum;\n\n\n\n\n\n\n\n\nTable 4:  A sample from the table resampel_template. \n \n  \n    strap_id \n    original_rownum \n    bootstrap_rownum \n  \n \n\n  \n    573 \n    2216 \n    4 \n  \n  \n    312 \n    2227 \n    1792 \n  \n  \n    2 \n    554 \n    1577 \n  \n  \n    440 \n    381 \n    688 \n  \n  \n    969 \n    1352 \n    1840"
  },
  {
    "objectID": "posts/2022-11-16-bootstrapping-in-sql/index.html#actually-doing-the-bootstrapping-its-just-a-left-join",
    "href": "posts/2022-11-16-bootstrapping-in-sql/index.html#actually-doing-the-bootstrapping-its-just-a-left-join",
    "title": "Bootstrapping in SQL",
    "section": "Actually Doing The Bootstrapping: Its Just A Left Join!",
    "text": "Actually Doing The Bootstrapping: Its Just A Left Join!\nNow all we have to do is join the original data onto resample_template. The join is going to happen on original_data.original_rownum = resample_template.bootstrap_rownum.\n\ncreate or replace table resampled_data as\nselect\n  resample_template.strap_id,\n  resample_template.bootstrap_rownum,\n  original_data.x,\n  original_data.y\nfrom \n  resample_template\nleft join \n  original_data on original_data.original_rownum = resample_template.bootstrap_rownum;\n\nAnd congratulations, you have what is in essence an unnested rsample::bootstraps output. This happens shockingly fast in duckdb (actually, a bit faster than rsample does it, but that is anecdote I didn’t actually time them). The hard part now is the aggregation function. Obviously, you can’t do very complex statsitical aggregations in duckdb (or any other SQL dialect), but there are a few you can do. For example, let’s bootstrap the mean of \\(x\\) and \\(y\\), as well as the estimated regression coefficient.\n\nselect\n  'Bootstrap' || lpad(strap_id,4,0) as id,\n  'SQL' as method,\n  avg(x) as mean_x,\n  avg(y) as mean_y,\n  corr(y, x) * stddev(y) / stddev(x) as beta\nfrom resampled_data\ngroup by 1\norder by 1;\n\nWe can easily compare the distributions obtained via the SQL bootstrap with distributions obtained from rsample::bootstrap"
  },
  {
    "objectID": "posts/2022-11-16-bootstrapping-in-sql/index.html#but-does-it-work",
    "href": "posts/2022-11-16-bootstrapping-in-sql/index.html#but-does-it-work",
    "title": "Bootstrapping in SQL",
    "section": "But Does It Work",
    "text": "But Does It Work\nYes…I think. The averages for \\(x\\) and \\(y\\) look really good, but the SQL bootstrap tails for the regression coefficient are a little thin."
  },
  {
    "objectID": "posts/2022-11-16-bootstrapping-in-sql/index.html#conclusion",
    "href": "posts/2022-11-16-bootstrapping-in-sql/index.html#conclusion",
    "title": "Bootstrapping in SQL",
    "section": "Conclusion",
    "text": "Conclusion\nThis is pretty silly, and probably inefficient. I’m no data engineer, I’m just a guy with a Ph.D in stats and a lot of time on the weekend. I should get a hobby or something."
  },
  {
    "objectID": "posts/2022-08-16-pca/index.html",
    "href": "posts/2022-08-16-pca/index.html",
    "title": "PCA on The Tags for Cross Validated",
    "section": "",
    "text": "Julia Silge gave a really good talk in 2018 about PCA ond tags on stack overflow. She was able to interpret some of the components to infer some subgroups of users of stack overflow (front-end vs back-end, are they a Microsoft tech developer or not, are you an android dev or not, etc). These principal components were able to shed some light on what drove the variation in questions asked.\nI love this talk, and I crib it all the time. As of late, I’ve not been doing much SQL, so I figured I would recreate Julia’s analysis using data from cross validated. But this time, with a twist!\nWhat if instead of understanding the drivers of variance in questions asked, we analyze the kinds of questions users answer. This could give us insight into the type of analysts we have on cross validated. The site is intended to be for statistical analysis, but it has a mix of prediction questions, machine learning questions, econometrics questions, and much more. Hang around there long enough and you will see some familiar faces (mine included) and you get a pretty good sense of who answers what kinds of questions.\nI’m going to use data available from the stack exchange data explorer available here. I’ve included a code box in this post with the query I’ve used. I’ve sliced out the top 250 users as ranked by reputation and the top 100 tags as calculated by prevalence. We can use {tidymodels} to do a lot of the heay lifting. Let’s get to it."
  },
  {
    "objectID": "posts/2022-08-16-pca/index.html#data-modelling",
    "href": "posts/2022-08-16-pca/index.html#data-modelling",
    "title": "PCA on The Tags for Cross Validated",
    "section": "Data & Modelling",
    "text": "Data & Modelling\nLet’s Take a peek at the data, using me as an example. Below are my top 10 tags as a percent of my total answers. Looks like I like to answer questions about regression, hypothesis testing, and R most frequently. Each of the top 250 users has data similar to this in my dataset. I need to pivot it so that tags become features. Then, I can normalize the data and perform PCA.\n\n\n\n\n \n  \n    Name \n    Tag \n    Percent \n  \n \n\n  \n    Demetri Pananos \n    Regression \n    11.40% \n  \n  \n    Demetri Pananos \n    Hypothesis-Testing \n    6.01% \n  \n  \n    Demetri Pananos \n    R \n    5.90% \n  \n  \n    Demetri Pananos \n    Machine-Learning \n    5.45% \n  \n  \n    Demetri Pananos \n    Logistic \n    4.85% \n  \n  \n    Demetri Pananos \n    Bayesian \n    3.58% \n  \n  \n    Demetri Pananos \n    Statistical-Significance \n    3.42% \n  \n  \n    Demetri Pananos \n    Confidence-Interval \n    2.75% \n  \n  \n    Demetri Pananos \n    T-Test \n    2.64% \n  \n  \n    Demetri Pananos \n    Probability \n    2.31% \n  \n  \n    Demetri Pananos \n    Generalized-Linear-Model \n    2.31% \n  \n\n\n\n\n\nCheckout how dummy easy the analysis is with tidymodels. I think I spent more time cleaning the data than I did modelling it.\n\nrec <- recipe(rnk + UserId + DisplayName ~ ., data = d) %>% \n       step_normalize(all_numeric_predictors()) %>% \n       step_pca(all_numeric_predictors(), num_comp = 3) %>% \n       prep()\n\nprin_comps <- bake(rec, new_data = d) \nweights <- rec %>% \n           tidy(number = 2, type = \"coef\")\n\nThe last two lines extract both the principal components and the weights for each tag on each component. Now. we’re ready to make some plots."
  },
  {
    "objectID": "posts/2022-08-16-pca/index.html#principal-components",
    "href": "posts/2022-08-16-pca/index.html#principal-components",
    "title": "PCA on The Tags for Cross Validated",
    "section": "Principal Components",
    "text": "Principal Components\nThe results for the first 3 principal componensts are shown below. I’ve shown the 20 most extreme components for clarity.\nThe first principal compnent has tags like “Anova”, “T-Test” and “SPSS” as heavily weighted positive, while “Machine Learning”, “Mathematical Statistics” and “Probability” are all weighted heavily negative (the direction of the weights doesn’t matter, it isn’t like one direction is better or worse). To me, I read this as “Beginner” vs “Advanced” answers. Questions with the former tags are usually from users who are maybe taking a stats class for the first time and are learning about the t test or Anova. The dead give away for this is the “SPSS” tag being weighted so heavily1. Looking at more weights verifies this, with the negative weights being associated with topics like “Neural Nets”, and “Maximum Likelihod” while the positive weights have tags like “Statistical Significance” and “Interpretation”.\nNow remember, these components do not explain variance in questions. They explain variance in the question answers! So the first component is really about people who choose to answer simple versus complex topics. The second principal component has a fairly straightforward interpretation. This component explains variation between users who answer classical statistical questions versus those who opt to answer machine learning type questions. Lastly, the third principal component seems to be distinguishing users who answer forecasting type questions (see tags like “Arima”, “Time Series”, “Forecasting”, and “Econometrics”) versus non forecasting type questions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI think what is more interesting is that we can plot some of the more popular users on the site using the principal components. In the first plot, I’ve scattered PC1 vs PC2. Left to right means advanced vs simple questions. It is no surpriuse to see whuber farther left and BruceET farther right. Whuber can answer most anything, and I feel like he often accepts the challenge of a complex answer, opting to comment on simpler questions. Bruce, on the other hand, will always answer a simple question very robustly. Top to bottom means machine learning vs classical stats. I’m not surprised to see Frank Harrell closer to the top, as he has appeared in many questions if not only to scold people for using accuracy as a metric. No surprise Topepo is on the top of this PC. Interestingly, I’m kind of near the origin, if not a bit right of it. Seems like I strike a good balance between ML and stats, but often opt to answer simpler questions.\nPlotting PC1 vs PC3 shows a very predictable pattern. Users near the bottom are more forcasting types, so its no surprise that Rob Hyndman, Dimitris Rizopoulus (who does a lot of longitudinal work), and Stephan Kolassa are near the bottom. I’m near the top, I have no clue about any of that stuff to be honest."
  },
  {
    "objectID": "posts/2022-08-16-pca/index.html#conclusion",
    "href": "posts/2022-08-16-pca/index.html#conclusion",
    "title": "PCA on The Tags for Cross Validated",
    "section": "Conclusion",
    "text": "Conclusion\nThere appear to be at least three dimensions about which analysts on cross validated can be placed. Analysts can either opt to answer easy or difficult questions, which lean classical statistics or machine learning, with additional focus on forecasting on non-forecasting problems. That’s a fairly useful interpretation of the first three principal components!\nIt could be fun to think about how best to use this information. Now that we know these three kinds of subgroups, could we use it to recommend users questions to answer? Its been the case that there is a large gap between questions and answers, so maybe this could be useful but also maybe not."
  },
  {
    "objectID": "posts/2022-11-25-taylor-series/index.html",
    "href": "posts/2022-11-25-taylor-series/index.html",
    "title": "Way Too Many Taylor Series",
    "section": "",
    "text": "Let \\(Y(a)\\) be a potential outcome for a continuous measure under treatment status \\(A=a\\), which for the purpose of this blog post can be considered a binary treatment. When is\n\\[  \\delta = \\dfrac{E[Y(1)]}{E[Y(0)]} \\] well approximated by \\(\\exp(\\delta^\\star)\\) where\n\\[ \\delta^\\star = E \\left[ \\ln \\left( \\dfrac{Y(1)}{Y(0)} \\right) \\right] = E[\\ln(Y(1)]  - E[\\ln(Y(0)] \\>.\\]\nIt seems to be a fairly well accepted proposition that the difference in means on the log scale can be interpreted as a relative change in means on the natural scale, but upon closer inspection they aren’t equivalent. Firstly, Jensen’s inequality prevents interchanging the expectation operator and the logarithm and second \\(E[X/Y] \\neq E[X]/E[Y]\\) since expectation is a linear operation. I think a more nuanced discussion is needed as to if and when we can interpret \\(\\exp(\\delta^\\star)\\) as \\(\\delta\\).\nTo be clear, I’m sure this holds fairly readily. I don’t want to overstate the importance of this blog post, but I don’t want to understate it either. This question came up when considering how AB tests should measure changes in continuous metrics, like revenue. Log-transforming revenue to reign in tails is common advice – and I think that’s fine, especially when it makes the sampling distribution of the sample mean more normal looking. Additionally, talking about changes in a relative sense (i.e. “lift” in the metric) is something that comes natural to a lot of companies. But if we’re going to use \\(\\delta^\\star\\) as the metric for our experiment, then I personally would like to understand under what conditions this is a good approximation. What assumptions am I implicitly making? I don’t think curiosity in that sense is a bad thing, or a waste of time."
  },
  {
    "objectID": "posts/2022-11-25-taylor-series/index.html#taylor-series-for-random-variables",
    "href": "posts/2022-11-25-taylor-series/index.html#taylor-series-for-random-variables",
    "title": "Way Too Many Taylor Series",
    "section": "Taylor Series for Random Variables",
    "text": "Taylor Series for Random Variables\nBefore getting into the meat of the blog post, it might be worthwhile to revisit Taylor series for random variables (which we will make heavy use of in this post). Recall that a Taylor series for a continuously differentiable function \\(f\\) is\n\\[ f(x) = \\sum_{k=0}^{\\infty} \\dfrac{f^{(k)}(x_0)}{k!} (x - x_0)^k \\>,  \\quad \\mid x - x_0 \\mid \\lt d\\]\nand that the error made in approximating \\(f\\) with first \\(n+1\\) terms of this sum, \\(R_n(x)\\), can be bounded by\n\\[ \\mid R_n(x) \\mid \\leq \\dfrac{M}{(n+1)!}(x - x_0)^{n+1} \\>, \\quad \\mid x - x_0 \\mid \\lt d \\>.\\] We can also expand a function of a random variable, \\(X\\), in a Taylor series by considering the variation of \\(X-\\mu\\) about \\(\\mu\\)\n\\[\\begin{align}\nf(X) &= f((X-\\mu) + \\mu) \\>, \\\\\n     &= f(\\mu) + f^{\\prime}(\\mu)(X-\\mu) + \\dfrac{f^{\\prime\\prime}(\\mu)}{2}(X-\\mu)^2 + \\mathcal{O}\\left( (X-\\mu)^3 \\right) \\>.\n\\end{align}\\]\nFrom here, we can take expectations and leverage the linearity of \\(E\\) to get a nice second order approximation of \\(E(f(X))\\)\n\\[\\begin{align}\nE[f(X)] &\\approx  E[f(\\mu)] + f^{\\prime}(\\mu)E[(X-\\mu)] + \\dfrac{f^{\\prime\\prime}(\\mu)}{2}E[(X-\\mu)^2] \\>, \\\\\n        &\\approx  f(\\mu) + \\dfrac{f^{\\prime\\prime}(\\mu)}{2}\\sigma^2 \\>.\n\\end{align}\\]"
  },
  {
    "objectID": "posts/2022-11-25-taylor-series/index.html#applying-taylor-series-to-our-problem",
    "href": "posts/2022-11-25-taylor-series/index.html#applying-taylor-series-to-our-problem",
    "title": "Way Too Many Taylor Series",
    "section": "Applying Taylor Series To Our Problem",
    "text": "Applying Taylor Series To Our Problem\nThe quantity \\(\\exp(\\delta^\\star)\\) could be approximately be \\(\\delta\\) under the right circumstances. Let’s expand \\(\\ln(Y(1)/Y(0))\\) in a Taylor series centered around \\(Y(1)/Y(0) = 1\\). We’re going to be doing quite a few Taylor expansions, so I’m going to color code some of them in order to keep track of which terms belong to which expansion.\n\\[\\begin{align}\nE\\left[\\ln \\left( \\dfrac{Y(1)}{Y(0)} \\right)\\right] &\\approx E \\left [ \\textcolor{#1f77b4}{\\left( \\dfrac{Y(1)}{Y(0)} -1 \\right) + \\dfrac{1}{2} \\left(  \\dfrac{Y(1)}{Y(0)} -1 \\right)^2} \\right] \\>,\\\\\n&\\approx \\textcolor{#ff7f0e}{E \\left[ \\dfrac{Y(1)}{Y(0)} \\right]}  \\textcolor{#1f77b4}{- 1 + \\dfrac{1}{2} E \\left[\\left(  \\dfrac{Y(1)}{Y(0)} -1 \\right)^2  \\right]} \\>.\n\\end{align}\\]\nThis approximation is only valid when \\(0 \\lt Y(1)/Y(0) \\leq 2\\), and the error in this approximation is bounded by \\(E\\left[\\left(\\frac{Y(1)}{Y(0)} -1\\right)^3\\right]\\). Note that we almost have \\(\\delta\\) in our Taylor series expansion, but not quite. We can apply yet another Taylor series expansion on the part in orange to yield\n\\[\\begin{align}\n\\textcolor{#ff7f0e}{E\\left[ \\dfrac{Y(1)}{Y(0)} \\right] \\approx \\dfrac{E[Y(1)]}{E[Y(0)]} -\\frac{\\operatorname{cov}[Y(1), Y(0)]}{\\mathrm{E}[Y(0)]^2}+\\frac{\\mathrm{E}[Y(1)]}{\\mathrm{E}[Y(0)]^3} \\operatorname{var}[Y(0)]} \\>.\n\\end{align}\\]\nLet’s assemble this all together now. Our approximation is now\n\\[\\begin{align}\nE\\left[\\ln \\left( \\dfrac{Y(1)}{Y(0)} \\right)\\right]  &\\approx \\textcolor{#ff7f0e}{ \\dfrac{E[Y(1)]}{E[Y(0)]} -\\frac{\\operatorname{cov}[Y(1), Y(0)]}{\\mathrm{E}[Y(0)]^2}+\\frac{\\mathrm{E}[Y(1)]}{\\mathrm{E}[Y(0)]^3} \\operatorname{var}[Y(0)]}  \\textcolor{#1f77b4}{- 1 + \\dfrac{1}{2} E \\left[\\left(  \\dfrac{Y(1)}{Y(0)} -1 \\right)^2  \\right]} \\>, \\\\\n&\\approx \\dfrac{E[Y(1)]}{E[Y(0)]} - 1 \\textcolor{#ff7f0e}{-\\frac{\\operatorname{cov}[Y(1), Y(0)]}{\\mathrm{E}[Y(0)]^2}+\\frac{\\mathrm{E}[Y(1)]}{\\mathrm{E}[Y(0)]^3} \\operatorname{var}[Y(0)]} \\textcolor{#1f77b4}{ + \\dfrac{1}{2} E \\left[\\left(  \\dfrac{Y(1)}{Y(0)} -1 \\right)^2  \\right]} \\>.\n\\end{align}\\]\nFinally \\(\\delta\\) appears in our long and tedious approximation of \\(\\delta^\\star\\). Let’s ignore the terms colored in blue and orange for a moment and come back to them later.\nOur approximation is now\n\\[E\\left[\\ln \\left( \\dfrac{Y(1)}{Y(0)} \\right)\\right] \\approx \\dfrac{E[Y(1)]}{E[Y(0)]} - 1\\]\n\\[ \\delta^\\star \\approx \\delta -1  \\]\nExponentiating both sides\n\\[\\begin{align}\n\\exp(\\delta^\\star) &\\approx \\exp(\\delta-1) \\\\\n& \\approx 1 + \\delta -1 +  \\mathcal{O}((\\delta-1)^2)\\\\\n& \\approx \\delta\n\\end{align}\\]"
  },
  {
    "objectID": "posts/2022-11-25-taylor-series/index.html#we-did-itnow-what",
    "href": "posts/2022-11-25-taylor-series/index.html#we-did-itnow-what",
    "title": "Way Too Many Taylor Series",
    "section": "We Did It…Now What?",
    "text": "We Did It…Now What?\nNow that we’ve written down all the necessary approximations and assumptions, let’s go back and determine under what circumstances this is a valid approximation. Can we break this approximation? Can we break it really badly?\nLet’s leave that for the next blog post. I’m tired from doing all this algebra."
  },
  {
    "objectID": "posts/2020-10-06-links/index.html",
    "href": "posts/2020-10-06-links/index.html",
    "title": "Log Link vs. Log(y)",
    "section": "",
    "text": "You wanna see a little gotcha in statistics? Take the following data\n\nset.seed(0)\nN = 1000\ny = rlnorm(N, 0.5, 0.5)\n\nand explain why glm(y ~ 1, family = gaussian(link=log) and lm(log(y)~1) produce different estimates of the coefficients. In case you don’t have an R terminal, here are the outputs\n\nlog_lm = lm(log(y) ~ 1)\nsummary(log_lm)\n\n\nCall:\nlm(formula = log(y) ~ 1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.61028 -0.34631 -0.02152  0.35173  1.64112 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.49209    0.01578   31.18   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.499 on 999 degrees of freedom\n\n\n\nglm_mod = glm(y ~ 1 , family = gaussian(link=log))\nsummary(glm_mod)\n\n\nCall:\nglm(formula = y ~ 1, family = gaussian(link = log))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.5282  -0.6981  -0.2541   0.4702   6.5869  \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.61791    0.01698    36.4   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.9918425)\n\n    Null deviance: 990.85  on 999  degrees of freedom\nResidual deviance: 990.85  on 999  degrees of freedom\nAIC: 2832.7\n\nNumber of Fisher Scoring iterations: 5\n\n\nAnswer is the same as the difference between \\(E(g(X))\\) and \\(g(E(X))\\) which are not always the same. Let me explain.\nFirst, let’s start with the lognormal random variable. \\(y \\sim \\operatorname{Lognormal}(\\mu, \\sigma)\\) means \\(\\log(y) \\sim \\operatorname{Normal}(\\mu, \\sigma)\\). So \\(\\mu, \\sigma\\) are the parameters of the underlying normal distribution. When we do lm(log(y) ~ 1), we are modelling \\(E(\\log(y)) = \\beta_0\\). So \\(\\beta_0\\) is an estimate of \\(\\mu\\) and \\(\\exp(\\mu)\\) is an estimate of the median of the lognormal. That is an easy check\n\nmedian(y)\n\n[1] 1.600898\n\n\n\n#Meh, close enough\nexp(coef(log_lm))\n\n(Intercept) \n   1.635723 \n\n\nIf I wanted an estimate of the mean of the lognormal, I would need to add \\(\\sigma^2/2\\) to my estimate of \\(\\mu\\).\n\nmean(y)\n\n[1] 1.855038\n\n\n\n#Meh, close enough\nsigma = var(log_lm$residuals)\nexp(coef(log_lm) + sigma/2)\n\n(Intercept) \n   1.852594 \n\n\nOk, onto the glm now. When we use the glm, we model \\(\\log(E(y)) = \\beta_0\\), so we model the mean of the lognormal directly. Case in point\n\nmean(y)\n\n[1] 1.855038\n\n\n\nexp(coef(glm_mod))\n\n(Intercept) \n   1.855038 \n\n\nand if I wanted the median, I would need to consider the extra factor of \\(\\exp(\\sigma^2/2)\\)\n\nmedian(y)\n\n[1] 1.600898\n\n\n\nexp(coef(glm_mod) - sigma/2)\n\n(Intercept) \n   1.637881 \n\n\nLog link vs. log outcome can be tricky. Just be sure to know what you’re modelling when you use either."
  },
  {
    "objectID": "posts/2022-07-31-gsd-pt-2/index.html",
    "href": "posts/2022-07-31-gsd-pt-2/index.html",
    "title": "Interim Analysis & Group Sequential Designs Pt 2",
    "section": "",
    "text": "This is part 2 of an ongoing series on group sequential designs for AB testing. Previous parts are shown below\nLast time, we noted that we want our AB tests to be shorter so we could learn quicker. Peeking – testing the data before the end of the experiment – inflates the probability we make a false positive unless we choose the critical values of the tests a little more carefully. The reason this happens is because requiring that any of the cumulative test statistics be larger than 1.96 in magnitude defines a region in the space of cumulative means which has a probability density larger than 5%. One way to fix that is just to redefine the space to be smaller by requiring the cumulative test statistics to be larger in magnitude than some other value. I noted this puts the unnecessary requirement on us that the thresholds all be the same. In this blog post, we will discuss other approaches to that problem and their pros and cons.\nIn order to have that discussion, we need to understand what “alpha” (\\(\\alpha\\)) is and how it can be “spent”. That will allow us to talk about “alpha spending functions”."
  },
  {
    "objectID": "posts/2022-07-31-gsd-pt-2/index.html#preliminaries",
    "href": "posts/2022-07-31-gsd-pt-2/index.html#preliminaries",
    "title": "Interim Analysis & Group Sequential Designs Pt 2",
    "section": "Preliminaries",
    "text": "Preliminaries\nIn the last post, we looked at a \\(K=2\\) GSD with equal sized groups. Of course, we don’t need to have equal sized groups and we can have more than two stages. Let \\(n_k\\) be the sample size of the \\(k^{th}\\) group. Then \\(N = \\sum_k n_k\\) is the total sample size. It is sometimes more convenient to refer to the information rates \\(t_k = n_k/N\\). We will do the same for consistency with other sources on GSDs."
  },
  {
    "objectID": "posts/2022-07-31-gsd-pt-2/index.html#what-is-alpha-and-how-do-we-spend-it",
    "href": "posts/2022-07-31-gsd-pt-2/index.html#what-is-alpha-and-how-do-we-spend-it",
    "title": "Interim Analysis & Group Sequential Designs Pt 2",
    "section": "What is \\(\\alpha\\), and How Do We Spend It?",
    "text": "What is \\(\\alpha\\), and How Do We Spend It?\nThe probability we reject the null, \\(H_0\\), when it is true is called \\(\\alpha\\). In a classical test, we would reject \\(H_0\\) when \\(\\vert Z \\vert > z_{1-\\alpha/2}\\), and so \\(P(\\vert Z \\vert > z_{1-\\alpha/2} \\vert H_0) = \\alpha\\). Now consider a \\(K=4\\) GSD so we can work with a concrete example. Let \\(Z^{(k)}\\) be the test statistic after seeing the \\(k^{th}\\) group, and let \\(u_k\\) be the threshold so that if \\(\\vert Z^{(k)} \\vert > u_k\\) then we would reject the null. Then a type one error could happen when 1 …\n\\[\n\\begin{align}\n&\\left( u_1 \\lt \\vert Z^{(1)} \\vert \\right)  \\quad \\mbox{or} \\\\\n& \\left(u_1 \\geq \\vert Z^{(1)} \\vert \\mbox{ and } u_2 \\lt \\vert Z^{(2)} \\vert \\right) \\quad \\mbox{or} \\\\\n& \\left(u_1 \\geq \\vert Z^{(1)} \\vert \\mbox{ and } u_2 \\geq \\vert Z^{(2)} \\vert \\mbox{ and } u_3 \\lt \\vert Z^{(3)} \\vert \\right) \\quad \\mbox{or} \\\\\n& \\left(u_1 \\geq \\vert Z^{(1)} \\vert \\mbox{ and } u_2 \\geq \\vert Z^{(2)} \\vert \\mbox{ and } u_3 \\geq \\vert Z^{(3)} \\vert \\mbox{ and } u_4 \\lt \\vert Z^{(4)} \\vert \\right)\n\\end{align}\n\\tag{1}\\]\nSo a type one error can occur in multiple ways, but we still want the probability we make a type one error to be \\(\\alpha\\), which means we’re going to need to evaluate the probability of the expression above. Note that each line in (Equation 1) are mutually exclusive, so the probability of the expression above is just the sum of the probabilities of each expression. This gives us\n\\[\n\\begin{align}\n\\alpha = &P\\left( u_1 \\lt \\vert Z^{(1)} \\vert \\Big\\vert H_0\\right)  +  \\\\\n& P\\left(u_1 \\geq \\vert Z^{(1)} \\vert \\mbox{ and } u_2 \\lt \\vert Z^{(2)} \\vert \\Big\\vert H_0\\right) + \\\\\n& P\\left(u_1 \\geq \\vert Z^{(1)} \\vert \\mbox{ and } u_2 \\geq \\vert Z^{(2)} \\vert \\mbox{ and } u_3 \\lt \\vert Z^{(3)} \\vert \\Big\\vert H_0\\right) + \\\\\n& P\\left(u_1 \\geq \\vert Z^{(1)} \\vert \\mbox{ and } u_2 \\geq \\vert Z^{(2)} \\vert \\mbox{ and } u_3 \\geq \\vert Z^{(3)} \\vert \\mbox{ and } u_4 \\lt \\vert Z^{(4)} \\vert \\Big\\vert H_0\\right)\n\\end{align}\n\\]\nThe test at each stage contributes towards the probability we make a type one error \\(\\alpha\\). You can think of \\(\\alpha\\) as a “budget”, and at each stage we have to “spend” (see where I’m going?) some of that alpha, with the added condition that we can never buy it back (meaning our \\(\\alpha\\) spending must be increasing). How much we decide to spend dictates what the \\(u_k\\) are going to be.\nBut how do we decide how to spend our \\(\\alpha\\)? If \\(\\alpha\\) is our budget for type one error, we need some sort of spending plan. Or perhaps a spending…function."
  },
  {
    "objectID": "posts/2022-07-31-gsd-pt-2/index.html#alpha-spending-functions",
    "href": "posts/2022-07-31-gsd-pt-2/index.html#alpha-spending-functions",
    "title": "Interim Analysis & Group Sequential Designs Pt 2",
    "section": "\\(\\alpha\\)-Spending Functions",
    "text": "\\(\\alpha\\)-Spending Functions\nAn \\(\\alpha\\)-spending function \\(\\alpha^\\star(t_k)\\) can be any non-decreasing function of the information rate \\(t_k\\) such that \\(\\alpha^\\star(0)=0\\) and \\(\\alpha^\\star(1) = \\alpha\\). Using this approach, we don’t need to specify the number of looks (though we may plan for \\(K\\) of them), nor the number of observations at those looks. Only the maximum sample size needed, \\(N\\).\nEach time we make an analysis, we spend some of our budgeted \\(\\alpha\\). In our first analysis (at \\(t_1 = n_1/N\\)), we spend\n\\[ P\\left( u_1 \\lt \\vert Z^{(1)} \\vert \\Big\\vert H_0\\right) = \\alpha^\\star(t_1) \\>. \\]\nAt the second analysis, we spend\n\\[P\\left(u_1 \\geq \\vert Z^{(1)} \\vert \\mbox{ and } u_2 \\lt \\vert Z^{(2)} \\vert \\Big\\vert H_0\\right) = \\alpha^\\star(t_2) - \\alpha^\\star(t_1) \\>,\\]\nand so on, with the \\(k^{th}\\) analysis being the difference in the alpha spending functions at the successive information rates. The spend is defined in this way so that the sum of the spend totals \\(\\alpha\\) since \\(\\alpha = \\sum_{k=2}^K \\alpha^\\star(t_k) - \\alpha^\\star(t_{k-1})\\). The quantities \\(\\alpha^\\star(t_k) - \\alpha^\\star(t_{k-1})\\) determine what the \\(u_k\\) should be through something called the recursive integration formula, which I will not be covering because wow is it every mathy and I need some time.\nTwo popular \\(\\alpha\\)-spending functions are the Pockock Spending function and the O’Brien Flemming spending function, shown in Figure 1. The equations don’t matter, what matters is the qualitative behavior.\n\n\n\n\nFigure 1: Pocock and O’Brien \\(\\alpha\\)-spending functions\n\n\n\n\n\nIn gray is the line \\(y = 0.05x\\), which would correspond to an alpha spending function in which our spend is proportional to the difference in information rates. Note how the Pocock function is kind of close to the diagonal (but not exactly on it), while the O’Brien Flemmming is constant up until \\(t_k \\approx 0.3\\) and then starts to increase. The result of this qualitative behavioiur is evident when we plot our rejection regions (the \\(u_k\\), which remember depend on the spending function)."
  },
  {
    "objectID": "posts/2022-07-31-gsd-pt-2/index.html#plotting-the-rejection-regions",
    "href": "posts/2022-07-31-gsd-pt-2/index.html#plotting-the-rejection-regions",
    "title": "Interim Analysis & Group Sequential Designs Pt 2",
    "section": "Plotting the Rejection Regions",
    "text": "Plotting the Rejection Regions\nIn my last post, the rejection region was plotted on the joint distribution of the \\(Z^{(1)}\\) and \\(Z^{(2)}\\). That is easy for two dimensions, doable for 3, and impossible for us to comprehend beyond that. Luckily, there is a simpler way of visualizing these rejection regions. We can simply plot the rejection regions \\(u_k\\) as a function of the information rate \\(t_k\\). Let’s plot the rejection regions for the Pocock and O’Brien Flemming spending functions now. But, I’m not going to label them just yet. I want you to think about which one might be which and why (knowing what we know about spending functions and the qualitative behaviour we saw above).\n\n\nCode\nextract_lims <- function(sfu){\n  \n  gs = gsDesign(\n  k=11, \n  timing = seq(0.1, 1, length.out = 11),\n  test.type=2,\n  alpha=0.025, \n  beta = 0.2,\n  sfu=sfu\n)\n  \ntibble(tk = gs$timing,\n       lower = gs$lower$bound,\n       upper = gs$upper$bound,\n       spending = if_else(sfu=='OF', \"O'Brien Flemming\", \"Pocock\")\n       )\n\n}\n\n\nsfus<- c('Pocock', 'OF')\n\nlims <- map_dfr(sfus, extract_lims) %>% \n      pivot_longer(cols = lower:upper, names_to = 'which', values_to = 'uk' )\n\nlims %>% \n  ggplot(aes(tk, uk, linetype = interaction(which, spending))) + \n  geom_line(color=my_blue, size=1) + \n  scale_linetype_manual(values = c(1, 1, 2, 2)) +\n  guides(linetype=F) + \n  theme(aspect.ratio = 1, \n        panel.grid.major = element_line()\n        )+\n  labs(x=expression(t[k]), \n       y=expression(u[k]))\n\n\n\n\n\nCode\nggsave('spending.png', dpi = 240)\n\n\n\n\nClick to see which is which\n\n\n\n\n\n\n\n\n\nOne of the spending functions results in the same \\(u_k\\), regardless of information rate, while the other seems to put a relatively low chance of rejecting the null (low alpha spend) in the beginning but then allows for a larger chance to reject the null later (larger alpha spend). Now, knowing what we know about the spending function qualitative behaviour, which function corresponds to which spending function?\nThe solid line is very clearly the O’Brien Flemming spending function. When \\(t_k\\) is small, then the O’Brien Flemming spending function has a small amount of spend (because \\(\\alpha^\\star(t_k) - \\alpha^\\star(t_{k-1})\\) is very small when \\(t_k\\) is small). But, when \\(t_k\\) is large, then \\(\\alpha^\\star(t_k) - \\alpha^\\star(t_{k-1})\\) is large, leading to more spend and hence smaller \\(u_k\\). The Pocock function is the dashed line, but I have no good rationale why constant lines should come from a spending function which is not on the diagonal. I’d love an explanation if you have one.\nSo what is the difference? Why might you choose one spending function over another? Note that the O’Brien Flemming function results in critical values which are really big in the beginning and really small at the end. This means we have the best chance to reject the null near the end of the experiment. I mean…that’s nice and all, but for AB testing we want to save as much time as possible, and the large critical values in the beginning work against us in that regard. On the other hand Pocock spending has constant critical values, meaning we are more likely to save time and detect an effect early. However, we run the risk of not detecting an effect over the experimental period, so there is risk of commiting a type II error. If it were me, I would choose Pocock boundaries. Move faster, try more things, and you can likely make up for type II erros by doing so.\nWhat would be kind of cool is to specify our own alpha spending function, perhaps one which makes it realy easy to reject the null early but later punishes us for spending so much early on. Maybe that would be a good idea for another post."
  },
  {
    "objectID": "posts/2022-07-31-gsd-pt-2/index.html#conclusion",
    "href": "posts/2022-07-31-gsd-pt-2/index.html#conclusion",
    "title": "Interim Analysis & Group Sequential Designs Pt 2",
    "section": "Conclusion",
    "text": "Conclusion\nI feel like we are more or less ready to start doing GSD in AB testing. We know why peeking is bad, and more or less how to fix it now. We also have a better understanding of some of the tradeoffs between the two most prevalent spending functions. There is still a lot to talk about, including confidence intervals for these sorts of tests (which are NOT intuitive in the least) as well as stopping for futility as well as binding/non-binding boundaries. Honestly, I don’t have a good idea for what the next part will be about, but I promise it will make sense."
  },
  {
    "objectID": "posts/2022-07-31-gsd-pt-2/index.html#visualising-alpha-spending-in-action",
    "href": "posts/2022-07-31-gsd-pt-2/index.html#visualising-alpha-spending-in-action",
    "title": "Interim Analysis & Group Sequential Designs Pt 2",
    "section": "Visualising Alpha Spending in Action",
    "text": "Visualising Alpha Spending in Action\nIts one thing to talk about alpha spending (Equation 1 and the probability statements that follow it), but it is another thing completely to see it in action.\nI’m going to use {rpact} to obtain the \\(u_k\\) for a \\(K=4\\) stage GSD using the O’Brien Flemming spending function. Then, I’m going to simulate some data from a GSD and compute the spend to show you how it works. I really hope you take the time to do the same, it can really clear up how the spending works.\nFirst, we need data, and a lot of it. Some of the spend can be on the order of 1e-5, so I’m going to cheat a little. The book I’m working from writes down the joint distribution of the \\(Z\\) under some assumptions (namely that the standard deviation is known and the data are normal). Let’s use that joint distirbution to simulate 10, 000, 000 samples. This should give me about 3 decimal places of accuracy.\n\nset.seed(0)\nn = 250\nK = 4\n\n# Construct the Cholesky Factor, row-wise\nA = matrix(rep(0, K*K), nrow = K)\nA[1, 1] = 1\nfor(i in 2:K){\n  A[i, 1:i] = sqrt(n)/sqrt(i*n)\n}\n\n# Construct the covariance \nS = A %*% t(A)\n# Draw from a multivariate normal\n# Lots of draws because the alpha spend will be small\nX = MASS::mvrnorm(10e6, rep(0, K), S)\n\nNow, let’s use {rpart} to get those critical values as well as how much alpha should be spent at each stage.\n\nr = rpact::getDesignGroupSequential(\n  kMax = K, \n  sided=2, \n  alpha=0.05, \n  beta=0.2,\n  informationRates = seq(0.25, 1, length.out=K),\n  typeOfDesign = 'OF'\n  )\n\nz_vals = r$criticalValues\naspend = r$alphaSpent\n\nNow, its just a matter of taking means. The ith column of X represents a mean I might see in a group sequential design at the ith stage. We know what the critical value is for each stage, so we just have to estimate the proportion of observations in each column which are beyond the associated critical value.\n\nX1 = abs(X[, 1])>z_vals[1]\nX2 = abs(X[, 2])>z_vals[2]\nX3 = abs(X[, 3])>z_vals[3]\nX4 = abs(X[, 4])>z_vals[4]\n\nTo compute the alpha spend, we just compute the probability statement above\n\nmy_spend = c(\n  mean(X1),\n  mean((!X1)&X2),\n  mean((!X1)&(!X2)&X3),\n  mean((!X1)&(!X2)&(!X3)&X4)\n)\n\nNow, we just take the cumulative sum of my_spend to determine how much alpha we spend up to the ith stage\n\n\nCode\nplot(cumsum(my_spend), aspend, xlab = 'Simulated Spend', ylab='Spend From rpact')\nabline(0, 1)\n\n\n\n\n\nOh MAN does that feel good! We spend very nearly the projected alpha at each stage. THAT is alpha spending in action!"
  },
  {
    "objectID": "posts/2021-04-03-confidence-intervals/index.html",
    "href": "posts/2021-04-03-confidence-intervals/index.html",
    "title": "On Interpretations of Confidence Intervals",
    "section": "",
    "text": "The 95% in 95% confidence interval refers not to the probability that any one interval contains the estimand, but rather to the long term relative frequency of the estimator containing the estimand in an infinite sequence of replicated experiments under ideal conditions.\nNow, if this were twitter I would get ratioed so hard I might have to take a break and walk it off. Luckily, this is my blog and not yours so I can say whatever I want with impunity. But, rather than shout my opinions and demand people listen, I thought it might be a good exercise to explain to you why I think this and perhaps why people might disagree. Let’s for a moment ignore the fact that the interpretation I use above is the de jure definition of a confidence interval and instead start where a good proportion of statistical learning starts; with a deck of shuffled cards.\nI present to you a shuffled deck. Its a regular deck of cards, no funny business with the cards or the shuffling. What is the probability the top card of this deck an ace? I’d wager a good portion of people would say 4/52. If you, dear reader, said 4/52 then I believe you have made a benign mistake, but a mistake all the same. And I suspect the reason you’ve made this mistake is because you’ve swapped a hard question (the question about this deck) for an easier question (a question about the long term relative frequencies of coming to shuffled decks with no funny business and finding aces).\nSwapping hard questions for easy questions is not a new observation. Daniel Khaneman writes about it in Thinking Fast and Slow and provides numerous examples. I’ll repeat some examples from the book here. We might swap the question:\nThe book Thinking Fast and Slow explains why we do this, or better yet why we have no control over this. I won’t explain it here. But it is important to know that this is something we do, mostly unconsciously.\nSo back to the deck of cards. Questions about the deck in front of you are hard. Its either an ace or not, but you can’t tell! The card is face down and there is no other information you could use to make the decision. So, you answer an easier one using information that you do know, namely the number of aces in the deck, the number of cards in the deck, the information that each card is equally likely to be on top given the fact there is no funny business with the cards or the shuffling, and the basic rules of probability you might have learned in high school if not elsewhere. But the answer you give is for a fundamentally different question, namely “If I were to observe a long sequence of well shuffled decks with no funny business, what fraction of them have an ace on top?”. Your answer is about that long sequence of shuffled decks. It isn’t about any one particular deck, and certainly not the one in front of you.\nI think the same thing happens with confidence intervals. The estimator has the property that 95% of the time it is constructed (under ideal circumstances) it will contain the estimand. But any one interval does or does not contain the estimand. And unlike the deck of cards which can easily be examined, we can’t ever know for certain if the interval successfully captured the estimand. There is no moment where we get to verify the estimand is in the confidence interval, and so we are sort of left guessing thus prompting us to offer a probability that we are right.\nThe mistake is benign. It hurts no one to think about confidence intervals as having a 95% probability of containing the estimand. Your company will not lose money, your paper will (hopefully) not be rejected, and the world will not end. That being said, it is unfortunately incorrect if not by appealing to the definition, then perhaps for other reasons.\nI’ll start with an appeal to authority. Sander Greenland and coauthors (who include notable epidemiologist Ken Rothman and motherfucking Doug Altman) include interpretation of a confidence interval as having 95% probability of containing the true effect as misconception 19 in this amazing paper. They note ” It is possible to compute an interval that can be interpreted as having 95% probability of containing the true value” but go on to say that this results in us doing a Bayesian analysis and computing a credible interval. If these guys are wrong, I don’t want to be right.\nAdditionally, when I say “The probability of a coin being flipped heads is 0.5” that references a long term frequency. I could, in principle, demonstrate that frequency by flipping a coin a lot and computing the empirical frequency of heads, which assuming the coin is fair and the number of flips large enough, will be within an acceptable range 0.5. To those people who say “This interval contains the estimand with 95% probability” I say “prove it”. Demonstrate to me via simulation or otherwise this long term relative frequency. I can’t imagine how this could be demonstrated because any fixed dataset will yield same answer over and over. Perhaps what supporters of this perspective mean is something closer to the Bayesian interpretation of probability (where probability is akin to strength in a belief). If so, the debate is decidedly answered because probability in frequentism is not about belief strength but about frequencies. Additionally, what is the random component in this probability? The data from the experiment are fixed, to allow these to vary is to appeal to my interpretation of the interval. If the estimand is random, then we are in another realm all together as frequentism assumes fixed parameters and random data. Maybe they mean something else which I just can’t think of. If there is something else, please let me know.\nI’ve gotten flack about confidence intervals on twitter."
  },
  {
    "objectID": "posts/2021-04-03-confidence-intervals/index.html#flack-1-framing-it-as-a-bet",
    "href": "posts/2021-04-03-confidence-intervals/index.html#flack-1-framing-it-as-a-bet",
    "title": "On Interpretations of Confidence Intervals",
    "section": "Flack 1: Framing It As A Bet",
    "text": "Flack 1: Framing It As A Bet\nYou present to me a shuffled deck with no funny business and offer me a bet in which I win X0,000 dollars if the card is an ace and lose X0 dollars if the card is not. “Aha Demetri! If you think the probability of the card on top being an ace is 0 or 1 you are either a fool for not taking the bet or are a fool for being so over confident! Your position is indefensible!” one person on twitter said to me (ok, they didn’t say it verbatim like this, but that was the intent).\nWell, not so fast. Nothing about my interpretation precludes me from using the answer to a simpler question to make decisions (I would argue statistics is the practice of doing jus that, but I digress). The top card is still an ace or not, but I can still think about an infinite sequence of shuffled decks anyway. In most of those scenarios, the card on top is an ace. Thus, I take the bet and hope the top card is an ace (much like I hope the confidence interval captures the true estimand, even though I know it either does or does not)."
  },
  {
    "objectID": "posts/2021-04-03-confidence-intervals/index.html#flack-2-my-next-interval-has-95-probability",
    "href": "posts/2021-04-03-confidence-intervals/index.html#flack-2-my-next-interval-has-95-probability",
    "title": "On Interpretations of Confidence Intervals",
    "section": "Flack 2: My Next Interval Has 95% Probability",
    "text": "Flack 2: My Next Interval Has 95% Probability\n“But Demetri, if 95% refers to the frequency of intervals containing the estimand, then surely my next interval has 95% probability of capturing the estimand prior to seeing data. Hence, individual intervals do have 95% probability of containing the estimand”.\nI get this sometimes, but don’t fully understand how it is supposed to be convincing. I see no problem with saying “the next interval has 95% probability” just like I have no problem with saying “If you shuffle those cards, the probability an ace is on top is 4/52” or “My next Roll Up The Rim cup has a 1 in 6 chance of winning”. This is starting to get more philosophical than I care it to, but those all reference non-existent things. Once they are brought into existence, it would be silly to think that they retain these properties. My cup is either winner or loser, even if I don’t roll it."
  },
  {
    "objectID": "posts/2021-04-03-confidence-intervals/index.html#flack-3-but-schrödingers-cat",
    "href": "posts/2021-04-03-confidence-intervals/index.html#flack-3-but-schrödingers-cat",
    "title": "On Interpretations of Confidence Intervals",
    "section": "Flack 3: But Schrödinger’s Cat…",
    "text": "Flack 3: But Schrödinger’s Cat…\nNo. Stop. This is not relevant in the least. I’m talking about cards and coins, not quarks or electrons. The Wikipedia article even says “Schrödinger did not wish to promote the idea of dead-and-live cats as a serious possibility; on the contrary, he intended the example to illustrate the absurdity of the existing view of quantum mechanics”. Cards can’t be and not-be aces until flipped. Get out of here."
  },
  {
    "objectID": "posts/2021-04-03-confidence-intervals/index.html#wrapping-up-dont-me",
    "href": "posts/2021-04-03-confidence-intervals/index.html#wrapping-up-dont-me",
    "title": "On Interpretations of Confidence Intervals",
    "section": "Wrapping Up, Don’t @ Me",
    "text": "Wrapping Up, Don’t @ Me\nTo be completely fair, I think the question about the cards I’ve presented to you is unfair. The question asks for a probability, and while 0 and 1 are valid probabilities, the question is phrased in a way so that you are prompted for a number between 0 and 1. Likewise, the name “95% confidence interval” begs for the wrong interpretation. That is the problem we face when we use language, which is naturally imprecise and full of shortcuts and ambiguity, to talk about things as precise as mathematics. It is a seminal case study in what I like to call the precision-usefulness trade off; precise statements are not useful. It is by, interpreting them and communicating them in common language that they become useful and that usefulness comes at the cost of precision (note, this explanation of the trade off is itself susceptible to the trade off). The important part is that we use confidence intervals to convey uncertainty in the estimate for which they are derived from. It isn’t important what you or I think about it, as the confidence interval is merely a means to an end.\nAS I noted, the mistake is benign, and these arguments are mostly a mental exercise than a fight against a method which may induce harm. Were it not for COVID19, I would encourage us all to go out for a beer and have these conversations rather than do it over twitter. Anyway, if you promise not to @ me anymore about this and I promise not to tweet about it anymore."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Numbers, Letters, Sometimes Both",
    "section": "",
    "text": "Choosing the Optimal MDE for Experimentation\n\n\n\n\n\n\n\nAB Tests\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nMar 31, 2023\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCausal Risk Ratios in AB Tests with One Sided Non-Compliance\n\n\n\n\n\n\n\nAB Tests\n\n\nStatistics\n\n\nCausal Inference\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJourneyman Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 31, 2022\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWay Too Many Taylor Series\n\n\n\n\n\n\n\nStatistics\n\n\nAB Testing\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2022\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBootstrapping in SQL\n\n\n\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2022\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPCA on The Tags for Cross Validated\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nDemetri Pananos\n\n\n\n\n\n\n  \n\n\n\n\nInterim Analysis & Group Sequential Designs Pt 2\n\n\n\n\n\n\n\nStatistics\n\n\nAB Testing\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2022\n\n\nDemetri Pananos\n\n\n\n\n\n\n  \n\n\n\n\nForecasting Experimental Lift Using Hierarchical Bayesian Modelling\n\n\n\n\n\n\n\nStatistics\n\n\nAB Testing\n\n\nBayes\n\n\n\n\n\n\n\n\n\n\n\nJul 20, 2022\n\n\nDemetri Pananos\n\n\n\n\n\n\n  \n\n\n\n\nInterim Analysis & Group Sequential Designs Pt 1\n\n\n\n\n\n\n\nStatistics\n\n\nAB Testing\n\n\n\n\n\n\n\n\n\n\n\nJul 6, 2022\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis Is A Quarto Blog\n\n\n\n\n\n\n\nNews\n\n\n\n\n\n\n\n\n\n\n\nJun 22, 2022\n\n\nDemetri Pananos\n\n\n\n\n\n\n  \n\n\n\n\nFlippin’ Fun!\n\n\n\n\n\n\n\nBayes\n\n\nStan\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2022\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHacking Sklearn To Do The Optimism Corrected Bootstrap\n\n\n\n\n\n\n\nStatistics\n\n\nMachine Learning\n\n\nPython\n\n\nScikit-Learn\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2021\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn Interpretations of Confidence Intervals\n\n\n\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2021\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog Link vs. Log(y)\n\n\n\n\n\n\n\nR\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2020\n\n\nDemetri Pananos\n\n\n\n\n\n\n  \n\n\n\n\nGradient Descent with ODEs\n\n\n\n\n\n\n\nPython\n\n\nMachine Learning\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nMay 21, 2019\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeat Litle Combinatorics Problem\n\n\n\n\n\n\n\nPython\n\n\nProbability\n\n\n\n\n\n\n\n\n\n\n\nAug 31, 2018\n\n\nDemetri Pananos\n\n\n\n\n\n\n  \n\n\n\n\nCoins and Factors\n\n\n\n\n\n\n\nPython\n\n\nRiddler\n\n\n\n\n\n\n\n\n\n\n\nDec 29, 2017\n\n\nDemetri Pananos\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I’m a senior data scientist at Zapier. I hold a Ph.D in Biostatistics from Western University. I’m usually playing chess, having a coffee, and playing fetch with my dog.\nI’m more interested in experimentation and inference than I am in machine learning. Questions like “What is the estimated impact of the new approach” and “What do we stand to lose if we are wrong” are the most prevelant types of questions I try to answer."
  },
  {
    "objectID": "consulting/index.html",
    "href": "consulting/index.html",
    "title": "Demetri Pananos Ph.D",
    "section": "",
    "text": "I am available for private statistical consulting. I’ve had the good fortune to work as a statistical analyst for The Department of Medicine at Western University. In that role I provided statistical services to researching physicians, including but not limited to:\n\nProviding statistical expertise in grant writing\nHelping design analysis plans for funded studies\nHelping clean data to prepare for modelling\nStatistical modelling\nReporting and interpreting results of modelling\nCommunication of results to non-expert audiences (including writing manuscripts for publication), and\nResponding to reviewer requests.\n\nIf you need help with statistics of any kind, I’m always open to lend a hand. Please feel free to reach out at dpananos [at] gmail [dot] com."
  },
  {
    "objectID": "consulting/index.html#pricing",
    "href": "consulting/index.html#pricing",
    "title": "Demetri Pananos Ph.D",
    "section": "Pricing",
    "text": "Pricing\nI have variable pricing depending on requirements and timeline to complete work. I offer a 30 minute consult free of charge to scope out project needs and will send you an estimated cost for the work."
  }
]