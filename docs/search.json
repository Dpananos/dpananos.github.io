[
  {
    "objectID": "consulting/index.html",
    "href": "consulting/index.html",
    "title": "Demetri Pananos Ph.D",
    "section": "",
    "text": "I am no longer accepting clients for private consulting. Please check back for updates."
  },
  {
    "objectID": "consulting/index.html#statistical-consulting",
    "href": "consulting/index.html#statistical-consulting",
    "title": "Demetri Pananos Ph.D",
    "section": "",
    "text": "I am no longer accepting clients for private consulting. Please check back for updates."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I’m a Senior Software Engineer DataDog. Previously, I was a Statistical Engineer at Eppo (which was acquired by Datadog), and a Staff Data Scientist at Zapier. While at Zapier, I lead experimentation and consulted on or analyzed 50+ experiments.\nI’m generally interested in building products for causal inference and experimentation.\nWhen I’m not doing statistics, I’m playing fetch with my dog Cauchy."
  },
  {
    "objectID": "posts/2023-09-24-l1/index.html",
    "href": "posts/2023-09-24-l1/index.html",
    "title": "Regularization of Noisy Multinomial Counts",
    "section": "",
    "text": "I’m reading Statistical Learning with Sparsity. Its like Elements of Statistical Learning, but just for the LASSO. In chapter 3, there is this interesting example of using the LASSO to estimate rates for noisy multinomial processes. Here is the setup from the book\n\nSuppose we have a sample of \\(N\\) counts \\(\\{ y_k \\}_{k-=1}^N\\) from an \\(N\\)-cell multinomial distribution, and let \\(r_k = y_k / \\sum_{\\ell=1}^N y_{\\ell}\\) be the corresponding vector of proportions. For example, in large-scale web applications, these counts might represent the number of people in each country in teh YSA that visited a particular website during a given week. This vector could be sparse depending on the specifics, so there is desire to regularize toward a broader, more stable distribution \\(\\mathbf{u} = \\{ u_k \\}_{k=1}^N\\) (for example, the same demographic, except measured over years)\n\nIn short, you’ve got some vector of probabilities from a multinomial process, \\(\\mathbf{r}\\). These probabilities are noisy, and instead of regularizing them to be uniform, you want to regularize them towards some other, more stable, distribution \\(\\mathbf{u}\\). The result is other distribution, \\(\\mathbf{q}\\), and you constrain yourself so that the max approximation error between \\(\\mathbf{r}\\) and \\(\\mathbf{q}\\) is less than some tolerance, \\(\\Vert \\mathbf{r} - \\mathbf{q} \\Vert _\\infty &lt; \\delta\\). The authors then go on to show that minimizing the KL-divergence for discrete distributions, subject to this constraint, is equivalent to the LASSO with a poisson likelihood. Only added trick is that you use \\(\\log(\\mathbf{u})\\) as an offset in the model call. For more detail, see chapter 3 of the book.\nThe authors kind of give a half hearted attempt at an application of such a procedure, so I tried to come up with one myself. In a previous blog post, we used looked at what kind of experts exist on statsexchange. We can use the data explorer I used for that post to look at two frequencies:\n\nFrequencies of total posts over a typical week in the year 2022, and\nFrequencies of posts tagged “regularization” posts over a typical week in the year 2022.\n\nThere are considerably more non-regularization posts than there are regularization posts, so while we should expect the frequencies to be similar there is probably some noise. Let’s use the query (below) to get our data and make a plot of those frequencies.\n\n\nClick to see SQL Query\n\n\nselect\ncast(A.CreationDate as date) as creation_date,\ncount(distinct A.id) as num_posts,\ncount(distinct case when lower(TargetTagName) = 'regularization' then A.id end) as reg_posts\nfrom Posts A\nleft join PostTags as B on A.Id = B.PostId\nleft join TagSynonyms as C on B.TagId = C.Id\nwhere A.PostTypeId = 1 and A.CreationDate between '2022-01-01' and '2022-12-31'\ngroup by cast(A.CreationDate as date)\norder by cast(A.CreationDate as date)\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(kableExtra)\n\ntheme_set(\n  theme_minimal(base_size = 12) %+replace%\n    theme(\n      aspect.ratio = 1/1.61,\n      legend.position = 'top'\n    )\n  )\n\n\nd &lt;- read_csv('QueryResults.csv') %&gt;% \n     mutate_at(\n       .vars = vars(contains('post')),\n       .funs = list(p = \\(x) x/sum(x))\n     )\n\nd$oday &lt;- factor(d$Day, ordered=T, levels = c('Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday'))\n\nd %&gt;% \n  arrange(oday) %&gt;% \n  select(oday, num_posts, reg_posts) %&gt;% \n  kbl(\n    col.names = c('Day of Week', 'Total Post Count', 'Regularization Post Count')\n  ) %&gt;% \n  kable_styling(bootstrap_options = 'striped')\n\n\n\n\n\nDay of Week\nTotal Post Count\nRegularization Post Count\n\n\n\n\nSunday\n1699\n13\n\n\nMonday\n2494\n37\n\n\nTuesday\n2759\n24\n\n\nWednesday\n2805\n26\n\n\nThursday\n2689\n32\n\n\nFriday\n2500\n27\n\n\nSaturday\n1703\n10\n\n\n\n\n\n\n\n\n\nCode\nbase_plot &lt;- d %&gt;% \n            ggplot() + \n            geom_point(aes(oday, num_posts_p, color='All Posts')) + \n            geom_point(aes(oday, reg_posts_p, color='Regularization Posts')) + \n            scale_y_continuous(labels = scales::percent) + \n            labs(x='Day', y='Post Frequency', color='') \n            \nbase_plot\n\n\n\n\n\n\n\n\n\nObviously, the smaller number of posts about regularization leads to larger noise in the multinomial estimates. You can very easily see that in the plot; Monday is not special, its very likely noise.\nSo here is where we can use LASSO to reign in some of that noise. We just need to specify how discordant we want our predicted frequencies to be from the observed frequencies, and fit a LASSO model with a penalty which achieves this desired tolerance.\nSo let’s say my predictions from the LASSO are the vector \\(\\mathbf{p}\\) and the observed frequencies for regularization posts are \\(\\mathbf{r}\\). Let’s say I want the largest error between the two to be no larger than 0.05. Shown below is some code for how to do that:\n\n# Set up the regression problem\nu &lt;- d$num_posts_p\nr &lt;- d$reg_posts_p\nX &lt;- model.matrix(~Day-1, data=d)\nmax_error &lt;- 0.03\n\n# Grid of penalties to search over\nlambda.grid &lt;- 2^seq(-8, 0, 0.005)\n\n# Fit the model and compute the difference between the largest error and our tolerance\nfit_lasso &lt;- function(x){\n  fit &lt;- glmnet(X, r, family='poisson', offset=log(u), lambda=x)\n  p &lt;- predict(fit, newx=X, newoffset=log(u), type='response')\n  abs(max(r-p) - max_error)\n}\n\nerrors&lt;-map_dbl(lambda.grid, fit_lasso)\n# Select the lambda which produces error closest to 0.03\nlambda &lt;- lambda.grid[which.min(errors)]\n# Make predictions under this model\nfit &lt;- glmnet(X, r, family='poisson', offset=log(u), lambda=lambda)\nd$predicted &lt;- predict(fit, newx=X, newoffset=log(u), type='response')[, 's0']\n\nnew_plot &lt;- d %&gt;% \n            ggplot() + \n            geom_point(aes(oday, num_posts_p, color='All Posts')) + \n            geom_point(aes(oday, reg_posts_p, color='Regularization Posts')) + \n            geom_point(aes(oday, predicted, color='Predicted Posts')) + \n            scale_y_continuous(labels = scales::percent) + \n            labs(x='Day', y='Post Frequency', color='')\n\nnew_plot\n\n\n\n\n\n\n\n\nThis might be easier to see if we follow Tufte and show small multiples. You can see as the penalty increases, the green dots (predicted) go from the blue dots (observed frequencies of regularization related posts) to the pink dots (the frequencies of all plots).\n\n\nCode\nlam &lt;- c(0.001, 0.01, 0.02, 0.04)\n\nall_d &lt;- map_dfr(lam, ~{\n  fit &lt;- glmnet(X, r, family='poisson', offset=log(u), lambda=.x)\n  d$predicted &lt;- predict(fit, newx=X, newoffset=log(u), type='response')[, 's0']\n  d$lambda = .x\n  \n  d\n})\n\nall_d %&gt;% \n      ggplot() + \n      geom_point(aes(oday, num_posts_p, color='All Posts')) + \n      geom_point(aes(oday, reg_posts_p, color='Regularization Posts')) + \n      geom_point(aes(oday, predicted, color='Predicted Posts')) + \n      scale_y_continuous(labels = scales::percent) + \n      labs(x='Day', y='Post Frequency', color='') + \n      facet_wrap(~lambda, labeller = label_both) + \n      scale_x_discrete(\n        labels = c('Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat')\n      )\n\n\n\n\n\n\n\n\n\nCool! Using \\(\\mathbf{u}\\) (here, the frequency of total posts on each day) as an offset acts as a sort of target towards which to regularize."
  },
  {
    "objectID": "posts/2021-04-03-confidence-intervals/index.html",
    "href": "posts/2021-04-03-confidence-intervals/index.html",
    "title": "On Interpretations of Confidence Intervals",
    "section": "",
    "text": "The 95% in 95% confidence interval refers not to the probability that any one interval contains the estimand, but rather to the long term relative frequency of the estimator containing the estimand in an infinite sequence of replicated experiments under ideal conditions.\nNow, if this were twitter I would get ratioed so hard I might have to take a break and walk it off. Luckily, this is my blog and not yours so I can say whatever I want with impunity. But, rather than shout my opinions and demand people listen, I thought it might be a good exercise to explain to you why I think this and perhaps why people might disagree. Let’s for a moment ignore the fact that the interpretation I use above is the de jure definition of a confidence interval and instead start where a good proportion of statistical learning starts; with a deck of shuffled cards.\nI present to you a shuffled deck. Its a regular deck of cards, no funny business with the cards or the shuffling. What is the probability the top card of this deck an ace? I’d wager a good portion of people would say 4/52. If you, dear reader, said 4/52 then I believe you have made a benign mistake, but a mistake all the same. And I suspect the reason you’ve made this mistake is because you’ve swapped a hard question (the question about this deck) for an easier question (a question about the long term relative frequencies of coming to shuffled decks with no funny business and finding aces).\nSwapping hard questions for easy questions is not a new observation. Daniel Khaneman writes about it in Thinking Fast and Slow and provides numerous examples. I’ll repeat some examples from the book here. We might swap the question:\nThe book Thinking Fast and Slow explains why we do this, or better yet why we have no control over this. I won’t explain it here. But it is important to know that this is something we do, mostly unconsciously.\nSo back to the deck of cards. Questions about the deck in front of you are hard. Its either an ace or not, but you can’t tell! The card is face down and there is no other information you could use to make the decision. So, you answer an easier one using information that you do know, namely the number of aces in the deck, the number of cards in the deck, the information that each card is equally likely to be on top given the fact there is no funny business with the cards or the shuffling, and the basic rules of probability you might have learned in high school if not elsewhere. But the answer you give is for a fundamentally different question, namely “If I were to observe a long sequence of well shuffled decks with no funny business, what fraction of them have an ace on top?”. Your answer is about that long sequence of shuffled decks. It isn’t about any one particular deck, and certainly not the one in front of you.\nI think the same thing happens with confidence intervals. The estimator has the property that 95% of the time it is constructed (under ideal circumstances) it will contain the estimand. But any one interval does or does not contain the estimand. And unlike the deck of cards which can easily be examined, we can’t ever know for certain if the interval successfully captured the estimand. There is no moment where we get to verify the estimand is in the confidence interval, and so we are sort of left guessing thus prompting us to offer a probability that we are right.\nThe mistake is benign. It hurts no one to think about confidence intervals as having a 95% probability of containing the estimand. Your company will not lose money, your paper will (hopefully) not be rejected, and the world will not end. That being said, it is unfortunately incorrect if not by appealing to the definition, then perhaps for other reasons.\nI’ll start with an appeal to authority. Sander Greenland and coauthors (who include notable epidemiologist Ken Rothman and motherfucking Doug Altman) include interpretation of a confidence interval as having 95% probability of containing the true effect as misconception 19 in this amazing paper. They note ” It is possible to compute an interval that can be interpreted as having 95% probability of containing the true value” but go on to say that this results in us doing a Bayesian analysis and computing a credible interval. If these guys are wrong, I don’t want to be right.\nAdditionally, when I say “The probability of a coin being flipped heads is 0.5” that references a long term frequency. I could, in principle, demonstrate that frequency by flipping a coin a lot and computing the empirical frequency of heads, which assuming the coin is fair and the number of flips large enough, will be within an acceptable range 0.5. To those people who say “This interval contains the estimand with 95% probability” I say “prove it”. Demonstrate to me via simulation or otherwise this long term relative frequency. I can’t imagine how this could be demonstrated because any fixed dataset will yield same answer over and over. Perhaps what supporters of this perspective mean is something closer to the Bayesian interpretation of probability (where probability is akin to strength in a belief). If so, the debate is decidedly answered because probability in frequentism is not about belief strength but about frequencies. Additionally, what is the random component in this probability? The data from the experiment are fixed, to allow these to vary is to appeal to my interpretation of the interval. If the estimand is random, then we are in another realm all together as frequentism assumes fixed parameters and random data. Maybe they mean something else which I just can’t think of. If there is something else, please let me know.\nI’ve gotten flack about confidence intervals on twitter."
  },
  {
    "objectID": "posts/2021-04-03-confidence-intervals/index.html#flack-1-framing-it-as-a-bet",
    "href": "posts/2021-04-03-confidence-intervals/index.html#flack-1-framing-it-as-a-bet",
    "title": "On Interpretations of Confidence Intervals",
    "section": "Flack 1: Framing It As A Bet",
    "text": "Flack 1: Framing It As A Bet\nYou present to me a shuffled deck with no funny business and offer me a bet in which I win X0,000 dollars if the card is an ace and lose X0 dollars if the card is not. “Aha Demetri! If you think the probability of the card on top being an ace is 0 or 1 you are either a fool for not taking the bet or are a fool for being so over confident! Your position is indefensible!” one person on twitter said to me (ok, they didn’t say it verbatim like this, but that was the intent).\nWell, not so fast. Nothing about my interpretation precludes me from using the answer to a simpler question to make decisions (I would argue statistics is the practice of doing jus that, but I digress). The top card is still an ace or not, but I can still think about an infinite sequence of shuffled decks anyway. In most of those scenarios, the card on top is an ace. Thus, I take the bet and hope the top card is an ace (much like I hope the confidence interval captures the true estimand, even though I know it either does or does not)."
  },
  {
    "objectID": "posts/2021-04-03-confidence-intervals/index.html#flack-2-my-next-interval-has-95-probability",
    "href": "posts/2021-04-03-confidence-intervals/index.html#flack-2-my-next-interval-has-95-probability",
    "title": "On Interpretations of Confidence Intervals",
    "section": "Flack 2: My Next Interval Has 95% Probability",
    "text": "Flack 2: My Next Interval Has 95% Probability\n“But Demetri, if 95% refers to the frequency of intervals containing the estimand, then surely my next interval has 95% probability of capturing the estimand prior to seeing data. Hence, individual intervals do have 95% probability of containing the estimand”.\nI get this sometimes, but don’t fully understand how it is supposed to be convincing. I see no problem with saying “the next interval has 95% probability” just like I have no problem with saying “If you shuffle those cards, the probability an ace is on top is 4/52” or “My next Roll Up The Rim cup has a 1 in 6 chance of winning”. This is starting to get more philosophical than I care it to, but those all reference non-existent things. Once they are brought into existence, it would be silly to think that they retain these properties. My cup is either winner or loser, even if I don’t roll it."
  },
  {
    "objectID": "posts/2021-04-03-confidence-intervals/index.html#flack-3-but-schrödingers-cat",
    "href": "posts/2021-04-03-confidence-intervals/index.html#flack-3-but-schrödingers-cat",
    "title": "On Interpretations of Confidence Intervals",
    "section": "Flack 3: But Schrödinger’s Cat…",
    "text": "Flack 3: But Schrödinger’s Cat…\nNo. Stop. This is not relevant in the least. I’m talking about cards and coins, not quarks or electrons. The Wikipedia article even says “Schrödinger did not wish to promote the idea of dead-and-live cats as a serious possibility; on the contrary, he intended the example to illustrate the absurdity of the existing view of quantum mechanics”. Cards can’t be and not-be aces until flipped. Get out of here."
  },
  {
    "objectID": "posts/2021-04-03-confidence-intervals/index.html#wrapping-up-dont-me",
    "href": "posts/2021-04-03-confidence-intervals/index.html#wrapping-up-dont-me",
    "title": "On Interpretations of Confidence Intervals",
    "section": "Wrapping Up, Don’t @ Me",
    "text": "Wrapping Up, Don’t @ Me\nTo be completely fair, I think the question about the cards I’ve presented to you is unfair. The question asks for a probability, and while 0 and 1 are valid probabilities, the question is phrased in a way so that you are prompted for a number between 0 and 1. Likewise, the name “95% confidence interval” begs for the wrong interpretation. That is the problem we face when we use language, which is naturally imprecise and full of shortcuts and ambiguity, to talk about things as precise as mathematics. It is a seminal case study in what I like to call the precision-usefulness trade off; precise statements are not useful. It is by, interpreting them and communicating them in common language that they become useful and that usefulness comes at the cost of precision (note, this explanation of the trade off is itself susceptible to the trade off). The important part is that we use confidence intervals to convey uncertainty in the estimate for which they are derived from. It isn’t important what you or I think about it, as the confidence interval is merely a means to an end.\nAS I noted, the mistake is benign, and these arguments are mostly a mental exercise than a fight against a method which may induce harm. Were it not for COVID19, I would encourage us all to go out for a beer and have these conversations rather than do it over twitter. Anyway, if you promise not to @ me anymore about this and I promise not to tweet about it anymore."
  },
  {
    "objectID": "posts/2022-07-31-gsd-pt-2/index.html",
    "href": "posts/2022-07-31-gsd-pt-2/index.html",
    "title": "Interim Analysis & Group Sequential Designs Pt 2",
    "section": "",
    "text": "This is part 2 of an ongoing series on group sequential designs for AB testing. Previous parts are shown below\nLast time, we noted that we want our AB tests to be shorter so we could learn quicker. Peeking – testing the data before the end of the experiment – inflates the probability we make a false positive unless we choose the critical values of the tests a little more carefully. The reason this happens is because requiring that any of the cumulative test statistics be larger than 1.96 in magnitude defines a region in the space of cumulative means which has a probability density larger than 5%. One way to fix that is just to redefine the space to be smaller by requiring the cumulative test statistics to be larger in magnitude than some other value. I noted this puts the unnecessary requirement on us that the thresholds all be the same. In this blog post, we will discuss other approaches to that problem and their pros and cons.\nIn order to have that discussion, we need to understand what “alpha” (\\(\\alpha\\)) is and how it can be “spent”. That will allow us to talk about “alpha spending functions”."
  },
  {
    "objectID": "posts/2022-07-31-gsd-pt-2/index.html#preliminaries",
    "href": "posts/2022-07-31-gsd-pt-2/index.html#preliminaries",
    "title": "Interim Analysis & Group Sequential Designs Pt 2",
    "section": "Preliminaries",
    "text": "Preliminaries\nIn the last post, we looked at a \\(K=2\\) GSD with equal sized groups. Of course, we don’t need to have equal sized groups and we can have more than two stages. Let \\(n_k\\) be the sample size of the \\(k^{th}\\) group. Then \\(N = \\sum_k n_k\\) is the total sample size. It is sometimes more convenient to refer to the information rates \\(t_k = n_k/N\\). We will do the same for consistency with other sources on GSDs."
  },
  {
    "objectID": "posts/2022-07-31-gsd-pt-2/index.html#what-is-alpha-and-how-do-we-spend-it",
    "href": "posts/2022-07-31-gsd-pt-2/index.html#what-is-alpha-and-how-do-we-spend-it",
    "title": "Interim Analysis & Group Sequential Designs Pt 2",
    "section": "What is \\(\\alpha\\), and How Do We Spend It?",
    "text": "What is \\(\\alpha\\), and How Do We Spend It?\nThe probability we reject the null, \\(H_0\\), when it is true is called \\(\\alpha\\). In a classical test, we would reject \\(H_0\\) when \\(\\vert Z \\vert &gt; z_{1-\\alpha/2}\\), and so \\(P(\\vert Z \\vert &gt; z_{1-\\alpha/2} \\vert H_0) = \\alpha\\). Now consider a \\(K=4\\) GSD so we can work with a concrete example. Let \\(Z^{(k)}\\) be the test statistic after seeing the \\(k^{th}\\) group, and let \\(u_k\\) be the threshold so that if \\(\\vert Z^{(k)} \\vert &gt; u_k\\) then we would reject the null. Then a type one error could happen when 1 …\n\\[\n\\begin{align}\n&\\left( u_1 \\lt \\vert Z^{(1)} \\vert \\right)  \\quad \\mbox{or} \\\\\n& \\left(u_1 \\geq \\vert Z^{(1)} \\vert \\mbox{ and } u_2 \\lt \\vert Z^{(2)} \\vert \\right) \\quad \\mbox{or} \\\\\n& \\left(u_1 \\geq \\vert Z^{(1)} \\vert \\mbox{ and } u_2 \\geq \\vert Z^{(2)} \\vert \\mbox{ and } u_3 \\lt \\vert Z^{(3)} \\vert \\right) \\quad \\mbox{or} \\\\\n& \\left(u_1 \\geq \\vert Z^{(1)} \\vert \\mbox{ and } u_2 \\geq \\vert Z^{(2)} \\vert \\mbox{ and } u_3 \\geq \\vert Z^{(3)} \\vert \\mbox{ and } u_4 \\lt \\vert Z^{(4)} \\vert \\right)\n\\end{align}\n\\tag{1}\\]\nSo a type one error can occur in multiple ways, but we still want the probability we make a type one error to be \\(\\alpha\\), which means we’re going to need to evaluate the probability of the expression above. Note that each line in (Equation 1) are mutually exclusive, so the probability of the expression above is just the sum of the probabilities of each expression. This gives us\n\\[\n\\begin{align}\n\\alpha = &P\\left( u_1 \\lt \\vert Z^{(1)} \\vert \\Big\\vert H_0\\right)  +  \\\\\n& P\\left(u_1 \\geq \\vert Z^{(1)} \\vert \\mbox{ and } u_2 \\lt \\vert Z^{(2)} \\vert \\Big\\vert H_0\\right) + \\\\\n& P\\left(u_1 \\geq \\vert Z^{(1)} \\vert \\mbox{ and } u_2 \\geq \\vert Z^{(2)} \\vert \\mbox{ and } u_3 \\lt \\vert Z^{(3)} \\vert \\Big\\vert H_0\\right) + \\\\\n& P\\left(u_1 \\geq \\vert Z^{(1)} \\vert \\mbox{ and } u_2 \\geq \\vert Z^{(2)} \\vert \\mbox{ and } u_3 \\geq \\vert Z^{(3)} \\vert \\mbox{ and } u_4 \\lt \\vert Z^{(4)} \\vert \\Big\\vert H_0\\right)\n\\end{align}\n\\]\nThe test at each stage contributes towards the probability we make a type one error \\(\\alpha\\). You can think of \\(\\alpha\\) as a “budget”, and at each stage we have to “spend” (see where I’m going?) some of that alpha, with the added condition that we can never buy it back (meaning our \\(\\alpha\\) spending must be increasing). How much we decide to spend dictates what the \\(u_k\\) are going to be.\nBut how do we decide how to spend our \\(\\alpha\\)? If \\(\\alpha\\) is our budget for type one error, we need some sort of spending plan. Or perhaps a spending…function."
  },
  {
    "objectID": "posts/2022-07-31-gsd-pt-2/index.html#alpha-spending-functions",
    "href": "posts/2022-07-31-gsd-pt-2/index.html#alpha-spending-functions",
    "title": "Interim Analysis & Group Sequential Designs Pt 2",
    "section": "\\(\\alpha\\)-Spending Functions",
    "text": "\\(\\alpha\\)-Spending Functions\nAn \\(\\alpha\\)-spending function \\(\\alpha^\\star(t_k)\\) can be any non-decreasing function of the information rate \\(t_k\\) such that \\(\\alpha^\\star(0)=0\\) and \\(\\alpha^\\star(1) = \\alpha\\). Using this approach, we don’t need to specify the number of looks (though we may plan for \\(K\\) of them), nor the number of observations at those looks. Only the maximum sample size needed, \\(N\\).\nEach time we make an analysis, we spend some of our budgeted \\(\\alpha\\). In our first analysis (at \\(t_1 = n_1/N\\)), we spend\n\\[ P\\left( u_1 \\lt \\vert Z^{(1)} \\vert \\Big\\vert H_0\\right) = \\alpha^\\star(t_1) \\&gt;. \\]\nAt the second analysis, we spend\n\\[P\\left(u_1 \\geq \\vert Z^{(1)} \\vert \\mbox{ and } u_2 \\lt \\vert Z^{(2)} \\vert \\Big\\vert H_0\\right) = \\alpha^\\star(t_2) - \\alpha^\\star(t_1) \\&gt;,\\]\nand so on, with the \\(k^{th}\\) analysis being the difference in the alpha spending functions at the successive information rates. The spend is defined in this way so that the sum of the spend totals \\(\\alpha\\) since \\(\\alpha = \\sum_{k=2}^K \\alpha^\\star(t_k) - \\alpha^\\star(t_{k-1})\\). The quantities \\(\\alpha^\\star(t_k) - \\alpha^\\star(t_{k-1})\\) determine what the \\(u_k\\) should be through something called the recursive integration formula, which I will not be covering because wow is it every mathy and I need some time.\nTwo popular \\(\\alpha\\)-spending functions are the Pockock Spending function and the O’Brien Flemming spending function, shown in Figure 1. The equations don’t matter, what matters is the qualitative behavior.\n\n\n\n\n\nFigure 1: Pocock and O’Brien \\(\\alpha\\)-spending functions\n\n\n\n\n\n\n\n\nIn gray is the line \\(y = 0.05x\\), which would correspond to an alpha spending function in which our spend is proportional to the difference in information rates. Note how the Pocock function is kind of close to the diagonal (but not exactly on it), while the O’Brien Flemmming is constant up until \\(t_k \\approx 0.3\\) and then starts to increase. The result of this qualitative behavioiur is evident when we plot our rejection regions (the \\(u_k\\), which remember depend on the spending function)."
  },
  {
    "objectID": "posts/2022-07-31-gsd-pt-2/index.html#plotting-the-rejection-regions",
    "href": "posts/2022-07-31-gsd-pt-2/index.html#plotting-the-rejection-regions",
    "title": "Interim Analysis & Group Sequential Designs Pt 2",
    "section": "Plotting the Rejection Regions",
    "text": "Plotting the Rejection Regions\nIn my last post, the rejection region was plotted on the joint distribution of the \\(Z^{(1)}\\) and \\(Z^{(2)}\\). That is easy for two dimensions, doable for 3, and impossible for us to comprehend beyond that. Luckily, there is a simpler way of visualizing these rejection regions. We can simply plot the rejection regions \\(u_k\\) as a function of the information rate \\(t_k\\). Let’s plot the rejection regions for the Pocock and O’Brien Flemming spending functions now. But, I’m not going to label them just yet. I want you to think about which one might be which and why (knowing what we know about spending functions and the qualitative behaviour we saw above).\n\n\nCode\nextract_lims &lt;- function(sfu){\n  \n  gs = gsDesign(\n  k=11, \n  timing = seq(0.1, 1, length.out = 11),\n  test.type=2,\n  alpha=0.025, \n  beta = 0.2,\n  sfu=sfu\n)\n  \ntibble(tk = gs$timing,\n       lower = gs$lower$bound,\n       upper = gs$upper$bound,\n       spending = if_else(sfu=='OF', \"O'Brien Flemming\", \"Pocock\")\n       )\n\n}\n\n\nsfus&lt;- c('Pocock', 'OF')\n\nlims &lt;- map_dfr(sfus, extract_lims) %&gt;% \n      pivot_longer(cols = lower:upper, names_to = 'which', values_to = 'uk' )\n\nlims %&gt;% \n  ggplot(aes(tk, uk, linetype = interaction(which, spending))) + \n  geom_line(color=my_blue, size=1) + \n  scale_linetype_manual(values = c(1, 1, 2, 2)) +\n  guides(linetype=F) + \n  theme(aspect.ratio = 1, \n        panel.grid.major = element_line()\n        )+\n  labs(x=expression(t[k]), \n       y=expression(u[k]))\n\n\n\n\n\n\n\n\n\nCode\nggsave('spending.png', dpi = 240)\n\n\n\n\nClick to see which is which\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne of the spending functions results in the same \\(u_k\\), regardless of information rate, while the other seems to put a relatively low chance of rejecting the null (low alpha spend) in the beginning but then allows for a larger chance to reject the null later (larger alpha spend). Now, knowing what we know about the spending function qualitative behaviour, which function corresponds to which spending function?\nThe solid line is very clearly the O’Brien Flemming spending function. When \\(t_k\\) is small, then the O’Brien Flemming spending function has a small amount of spend (because \\(\\alpha^\\star(t_k) - \\alpha^\\star(t_{k-1})\\) is very small when \\(t_k\\) is small). But, when \\(t_k\\) is large, then \\(\\alpha^\\star(t_k) - \\alpha^\\star(t_{k-1})\\) is large, leading to more spend and hence smaller \\(u_k\\). The Pocock function is the dashed line, but I have no good rationale why constant lines should come from a spending function which is not on the diagonal. I’d love an explanation if you have one.\nSo what is the difference? Why might you choose one spending function over another? Note that the O’Brien Flemming function results in critical values which are really big in the beginning and really small at the end. This means we have the best chance to reject the null near the end of the experiment. I mean…that’s nice and all, but for AB testing we want to save as much time as possible, and the large critical values in the beginning work against us in that regard. On the other hand Pocock spending has constant critical values, meaning we are more likely to save time and detect an effect early. However, we run the risk of not detecting an effect over the experimental period, so there is risk of commiting a type II error. If it were me, I would choose Pocock boundaries. Move faster, try more things, and you can likely make up for type II erros by doing so.\nWhat would be kind of cool is to specify our own alpha spending function, perhaps one which makes it realy easy to reject the null early but later punishes us for spending so much early on. Maybe that would be a good idea for another post."
  },
  {
    "objectID": "posts/2022-07-31-gsd-pt-2/index.html#conclusion",
    "href": "posts/2022-07-31-gsd-pt-2/index.html#conclusion",
    "title": "Interim Analysis & Group Sequential Designs Pt 2",
    "section": "Conclusion",
    "text": "Conclusion\nI feel like we are more or less ready to start doing GSD in AB testing. We know why peeking is bad, and more or less how to fix it now. We also have a better understanding of some of the tradeoffs between the two most prevalent spending functions. There is still a lot to talk about, including confidence intervals for these sorts of tests (which are NOT intuitive in the least) as well as stopping for futility as well as binding/non-binding boundaries. Honestly, I don’t have a good idea for what the next part will be about, but I promise it will make sense."
  },
  {
    "objectID": "posts/2022-07-31-gsd-pt-2/index.html#visualising-alpha-spending-in-action",
    "href": "posts/2022-07-31-gsd-pt-2/index.html#visualising-alpha-spending-in-action",
    "title": "Interim Analysis & Group Sequential Designs Pt 2",
    "section": "Visualising Alpha Spending in Action",
    "text": "Visualising Alpha Spending in Action\nIts one thing to talk about alpha spending (Equation 1 and the probability statements that follow it), but it is another thing completely to see it in action.\nI’m going to use {rpact} to obtain the \\(u_k\\) for a \\(K=4\\) stage GSD using the O’Brien Flemming spending function. Then, I’m going to simulate some data from a GSD and compute the spend to show you how it works. I really hope you take the time to do the same, it can really clear up how the spending works.\nFirst, we need data, and a lot of it. Some of the spend can be on the order of 1e-5, so I’m going to cheat a little. The book I’m working from writes down the joint distribution of the \\(Z\\) under some assumptions (namely that the standard deviation is known and the data are normal). Let’s use that joint distirbution to simulate 10, 000, 000 samples. This should give me about 3 decimal places of accuracy.\n\nset.seed(0)\nn = 250\nK = 4\n\n# Construct the Cholesky Factor, row-wise\nA = matrix(rep(0, K*K), nrow = K)\nA[1, 1] = 1\nfor(i in 2:K){\n  A[i, 1:i] = sqrt(n)/sqrt(i*n)\n}\n\n# Construct the covariance \nS = A %*% t(A)\n# Draw from a multivariate normal\n# Lots of draws because the alpha spend will be small\nX = MASS::mvrnorm(10e6, rep(0, K), S)\n\nNow, let’s use {rpart} to get those critical values as well as how much alpha should be spent at each stage.\n\nr = rpact::getDesignGroupSequential(\n  kMax = K, \n  sided=2, \n  alpha=0.05, \n  beta=0.2,\n  informationRates = seq(0.25, 1, length.out=K),\n  typeOfDesign = 'OF'\n  )\n\nz_vals = r$criticalValues\naspend = r$alphaSpent\n\nNow, its just a matter of taking means. The ith column of X represents a mean I might see in a group sequential design at the ith stage. We know what the critical value is for each stage, so we just have to estimate the proportion of observations in each column which are beyond the associated critical value.\n\nX1 = abs(X[, 1])&gt;z_vals[1]\nX2 = abs(X[, 2])&gt;z_vals[2]\nX3 = abs(X[, 3])&gt;z_vals[3]\nX4 = abs(X[, 4])&gt;z_vals[4]\n\nTo compute the alpha spend, we just compute the probability statement above\n\nmy_spend = c(\n  mean(X1),\n  mean((!X1)&X2),\n  mean((!X1)&(!X2)&X3),\n  mean((!X1)&(!X2)&(!X3)&X4)\n)\n\nNow, we just take the cumulative sum of my_spend to determine how much alpha we spend up to the ith stage\n\n\nCode\nplot(cumsum(my_spend), aspend, xlab = 'Simulated Spend', ylab='Spend From rpact')\nabline(0, 1)\n\n\n\n\n\n\n\n\n\nOh MAN does that feel good! We spend very nearly the projected alpha at each stage. THAT is alpha spending in action!"
  },
  {
    "objectID": "posts/2022-07-31-gsd-pt-2/index.html#footnotes",
    "href": "posts/2022-07-31-gsd-pt-2/index.html#footnotes",
    "title": "Interim Analysis & Group Sequential Designs Pt 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOr if you like set theory, we could write each line as \\(\\left( \\bigcap_{k=1}^{j-1} \\vert{Z^{(k)}} \\vert \\lt u_k \\right) \\cap \\left( \\vert{Z^{(j)}} \\vert \\geq u_j \\right)\\).↩︎"
  },
  {
    "objectID": "posts/2025-03-03-cuped/index.html",
    "href": "posts/2025-03-03-cuped/index.html",
    "title": "A Brief Tour of CUPED and Related Methods (Pt. 1)",
    "section": "",
    "text": "Randomized experiments have a rich history, almost none of which is the concern of this post. Rather, I want to focus on a tiny slice of randomized experiment literature: CUPED (Controlled-experiment Using Pre-Experiment Data). The technique has been impactful in the online randomized experiment (a.k.a. A/B Testing) community, with many vendors – Eppo included – offering the technique. Despite the popularity and impact, customers and data scientists still seem to misunderstand why CUPED works, why different implementations of the procedure (a la regression adjustment) still provide the same benefit, and why different implementations are equivalent.\nThis sequence of posts seeks to answer popular questions regarding the CUPED, and in particular frames the technique as a regression procedure first as opposed to a “new” or different technique. I discuss papers that pre-date the CUPED as well as discuss how the literature has evolved since the publication of CUPED. This is not a systematic review, nor a scoping review; this is a story of regression erasure (I kid).\nThis post in particular will demonstrate the equivlence between CUPED and regression."
  },
  {
    "objectID": "posts/2025-03-03-cuped/index.html#footnotes",
    "href": "posts/2025-03-03-cuped/index.html#footnotes",
    "title": "A Brief Tour of CUPED and Related Methods (Pt. 1)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe algebraic details are as follows. Let \\(Y_i^{(t)}\\) be an outcome in the treatment group and let \\(Y_i^{(c)}\\) be an outcome in the control group. Then \\[\\begin{align} \\Delta_{c v}&=\\widehat{Y}_{c v}^{(t)}-\\widehat{Y}_{c v}^{(c)}\\\\\n&= \\bar{Y}^{(t)}-\\theta \\bar{X}^{(t)}+\\theta E[X^{(t)}] - (\\bar{Y}^{(c)}-\\theta \\bar{X}^{(c)}+\\theta E[X^{(c)}] )\n\\end{align}\\] Note that prior to the treatment \\(E[X^{(t)}] = E[X^{(c)}]\\). Deng et. al also recommend using the same \\(\\theta\\) for both treatment and control groups, which we will see in a future post can be improved upon.↩︎\nHat tip Evan Miller, I saw this section’s title in an internal doc and had a good laugh.↩︎"
  },
  {
    "objectID": "posts/2023-11-02-shifted-beta-geometric/index.html",
    "href": "posts/2023-11-02-shifted-beta-geometric/index.html",
    "title": "Estimating the Shifted Beta Geometric Model in Stan",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(cmdstanr)\nThe shifted beta geometric model (sBG) is a model that is used to forecast retention/survival of users in contractual settings (think netflix, disney plus, tinder gold, etc). The model is quite simple and posits:\nExcel isn’t a great tool for fitting models, so Let’s write this in Stan."
  },
  {
    "objectID": "posts/2023-11-02-shifted-beta-geometric/index.html#sbg-in-stan",
    "href": "posts/2023-11-02-shifted-beta-geometric/index.html#sbg-in-stan",
    "title": "Estimating the Shifted Beta Geometric Model in Stan",
    "section": "sBG in Stan",
    "text": "sBG in Stan\nThe two things we need are the probability density function and the survival function. Fader and Hardie provide these in their paper. Mathematically, the probability density and survival function are\n\\[ P(T=t \\mid \\alpha, \\beta) = \\dfrac{B(\\alpha+1, \\beta+t-1)}{B(\\alpha, \\beta)} \\&gt;,\\]\n\\[ S(T=t \\mid \\alpha, \\beta) = \\dfrac{B(\\alpha, \\beta+t)}{B(\\alpha, \\beta)} \\&gt;. \\] Here, \\(B(\\alpha, \\beta)\\) is the beta function and not the beta distribution (I made that mistake early). Stan operates on the log scale, and so we’ll have to take the log of these. Stan has a log beta function called lbeta so we’ll use that in our functions for the density and survival function.\n\n\nfunctions{\n  real sbg_lpdf(real time, real alpha, real beta){\n    \n    return lbeta(alpha+1, beta+time-1) - lbeta(alpha, beta);\n  }\n  \n  real sbg_lccdf(real time, real alpha, real beta){\n    \n    return lbeta(alpha, beta + time) - lbeta(alpha, beta);\n  }\n  \n}\n\n\nThe data we need to fit the model include:\n\nHow many customers were lost at each time period,\nThe times at which customers were lost, and\nThe total number of customers under observation\n\nWe’ll also include an array of times at which to estimate the survival curve\n\n\ndata{\n  // Fitting the model\n  int N;\n  int n_total;\n  array[N] int lost;\n  array[N] real time;\n  \n  // Making Predictions\n  int N_pred;\n  array[N_pred] real pred_times;\n}\n\n\nLater, we’ll need the last time point we observed customers who haven’t churned. This is the truncation time\n\n\ntransformed data{\n  real truncation_time = max(time);\n}\n\n\nAll that is left to do is specify parameters, write the model block, and generate predictions for our survival curve. The likelihood computations are shown in the paper I referenced, so I’ll let you read that if you’re interested\n\n\nparameters{\n  real&lt;lower=0&gt; alpha;\n  real&lt;lower=0&gt; beta;\n}\nmodel{\n  alpha ~ cauchy(0, 1);\n  beta ~ cauchy(0, 1);\n  \n  for(i in 1:N){\n    target += lost[i] * sbg_lpdf(time[i]| alpha, beta);\n  }\n  target += (n_total - sum(lost)) * sbg_lccdf(truncation_time| alpha, beta);\n}\ngenerated quantities{\n  \n  array[N_pred] real expected_surviving;\n  \n  for(i in 1:N_pred){\n    expected_surviving[i] = exp(sbg_lccdf(pred_times[i]| alpha, beta));\n  }\n  \n}"
  },
  {
    "objectID": "posts/2022-11-25-taylor-series/index.html",
    "href": "posts/2022-11-25-taylor-series/index.html",
    "title": "Way Too Many Taylor Series",
    "section": "",
    "text": "Let \\(Y(a)\\) be a potential outcome for a continuous measure under treatment status \\(A=a\\), which for the purpose of this blog post can be considered a binary treatment. When is\n\\[  \\delta = \\dfrac{E[Y(1)]}{E[Y(0)]} \\] well approximated by \\(\\exp(\\delta^\\star)\\) where\n\\[ \\delta^\\star = E \\left[ \\ln \\left( \\dfrac{Y(1)}{Y(0)} \\right) \\right] = E[\\ln(Y(1)]  - E[\\ln(Y(0)] \\&gt;.\\]\nIt seems to be a fairly well accepted proposition that the difference in means on the log scale can be interpreted as a relative change in means on the natural scale, but upon closer inspection they aren’t equivalent. Firstly, Jensen’s inequality prevents interchanging the expectation operator and the logarithm and second \\(E[X/Y] \\neq E[X]/E[Y]\\) since expectation is a linear operation. I think a more nuanced discussion is needed as to if and when we can interpret \\(\\exp(\\delta^\\star)\\) as \\(\\delta\\).\nTo be clear, I’m sure this holds fairly readily. I don’t want to overstate the importance of this blog post, but I don’t want to understate it either. This question came up when considering how AB tests should measure changes in continuous metrics, like revenue. Log-transforming revenue to reign in tails is common advice – and I think that’s fine, especially when it makes the sampling distribution of the sample mean more normal looking. Additionally, talking about changes in a relative sense (i.e. “lift” in the metric) is something that comes natural to a lot of companies. But if we’re going to use \\(\\delta^\\star\\) as the metric for our experiment, then I personally would like to understand under what conditions this is a good approximation. What assumptions am I implicitly making? I don’t think curiosity in that sense is a bad thing, or a waste of time."
  },
  {
    "objectID": "posts/2022-11-25-taylor-series/index.html#introduction",
    "href": "posts/2022-11-25-taylor-series/index.html#introduction",
    "title": "Way Too Many Taylor Series",
    "section": "",
    "text": "Let \\(Y(a)\\) be a potential outcome for a continuous measure under treatment status \\(A=a\\), which for the purpose of this blog post can be considered a binary treatment. When is\n\\[  \\delta = \\dfrac{E[Y(1)]}{E[Y(0)]} \\] well approximated by \\(\\exp(\\delta^\\star)\\) where\n\\[ \\delta^\\star = E \\left[ \\ln \\left( \\dfrac{Y(1)}{Y(0)} \\right) \\right] = E[\\ln(Y(1)]  - E[\\ln(Y(0)] \\&gt;.\\]\nIt seems to be a fairly well accepted proposition that the difference in means on the log scale can be interpreted as a relative change in means on the natural scale, but upon closer inspection they aren’t equivalent. Firstly, Jensen’s inequality prevents interchanging the expectation operator and the logarithm and second \\(E[X/Y] \\neq E[X]/E[Y]\\) since expectation is a linear operation. I think a more nuanced discussion is needed as to if and when we can interpret \\(\\exp(\\delta^\\star)\\) as \\(\\delta\\).\nTo be clear, I’m sure this holds fairly readily. I don’t want to overstate the importance of this blog post, but I don’t want to understate it either. This question came up when considering how AB tests should measure changes in continuous metrics, like revenue. Log-transforming revenue to reign in tails is common advice – and I think that’s fine, especially when it makes the sampling distribution of the sample mean more normal looking. Additionally, talking about changes in a relative sense (i.e. “lift” in the metric) is something that comes natural to a lot of companies. But if we’re going to use \\(\\delta^\\star\\) as the metric for our experiment, then I personally would like to understand under what conditions this is a good approximation. What assumptions am I implicitly making? I don’t think curiosity in that sense is a bad thing, or a waste of time."
  },
  {
    "objectID": "posts/2022-11-25-taylor-series/index.html#taylor-series-for-random-variables",
    "href": "posts/2022-11-25-taylor-series/index.html#taylor-series-for-random-variables",
    "title": "Way Too Many Taylor Series",
    "section": "Taylor Series for Random Variables",
    "text": "Taylor Series for Random Variables\nBefore getting into the meat of the blog post, it might be worthwhile to revisit Taylor series for random variables (which we will make heavy use of in this post). Recall that a Taylor series for a continuously differentiable function \\(f\\) is\n\\[ f(x) = \\sum_{k=0}^{\\infty} \\dfrac{f^{(k)}(x_0)}{k!} (x - x_0)^k \\&gt;,  \\quad \\mid x - x_0 \\mid \\lt d\\]\nand that the error made in approximating \\(f\\) with first \\(n+1\\) terms of this sum, \\(R_n(x)\\), can be bounded by\n\\[ \\mid R_n(x) \\mid \\leq \\dfrac{M}{(n+1)!}(x - x_0)^{n+1} \\&gt;, \\quad \\mid x - x_0 \\mid \\lt d \\&gt;.\\] We can also expand a function of a random variable, \\(X\\), in a Taylor series by considering the variation of \\(X-\\mu\\) about \\(\\mu\\)\n\\[\\begin{align}\nf(X) &= f((X-\\mu) + \\mu) \\&gt;, \\\\\n     &= f(\\mu) + f^{\\prime}(\\mu)(X-\\mu) + \\dfrac{f^{\\prime\\prime}(\\mu)}{2}(X-\\mu)^2 + \\mathcal{O}\\left( (X-\\mu)^3 \\right) \\&gt;.\n\\end{align}\\]\nFrom here, we can take expectations and leverage the linearity of \\(E\\) to get a nice second order approximation of \\(E(f(X))\\)\n\\[\\begin{align}\nE[f(X)] &\\approx  E[f(\\mu)] + f^{\\prime}(\\mu)E[(X-\\mu)] + \\dfrac{f^{\\prime\\prime}(\\mu)}{2}E[(X-\\mu)^2] \\&gt;, \\\\\n        &\\approx  f(\\mu) + \\dfrac{f^{\\prime\\prime}(\\mu)}{2}\\sigma^2 \\&gt;.\n\\end{align}\\]"
  },
  {
    "objectID": "posts/2022-11-25-taylor-series/index.html#applying-taylor-series-to-our-problem",
    "href": "posts/2022-11-25-taylor-series/index.html#applying-taylor-series-to-our-problem",
    "title": "Way Too Many Taylor Series",
    "section": "Applying Taylor Series To Our Problem",
    "text": "Applying Taylor Series To Our Problem\nThe quantity \\(\\exp(\\delta^\\star)\\) could be approximately be \\(\\delta\\) under the right circumstances. Let’s expand \\(\\ln(Y(1)/Y(0))\\) in a Taylor series centered around \\(Y(1)/Y(0) = 1\\). We’re going to be doing quite a few Taylor expansions, so I’m going to color code some of them in order to keep track of which terms belong to which expansion.\n\\[\\begin{align}\nE\\left[\\ln \\left( \\dfrac{Y(1)}{Y(0)} \\right)\\right] &\\approx E \\left [ \\textcolor{#1f77b4}{\\left( \\dfrac{Y(1)}{Y(0)} -1 \\right) + \\dfrac{1}{2} \\left(  \\dfrac{Y(1)}{Y(0)} -1 \\right)^2} \\right] \\&gt;,\\\\\n&\\approx \\textcolor{#ff7f0e}{E \\left[ \\dfrac{Y(1)}{Y(0)} \\right]}  \\textcolor{#1f77b4}{- 1 + \\dfrac{1}{2} E \\left[\\left(  \\dfrac{Y(1)}{Y(0)} -1 \\right)^2  \\right]} \\&gt;.\n\\end{align}\\]\nThis approximation is only valid when \\(0 \\lt Y(1)/Y(0) \\leq 2\\), and the error in this approximation is bounded by \\(E\\left[\\left(\\frac{Y(1)}{Y(0)} -1\\right)^3\\right]\\). Note that we almost have \\(\\delta\\) in our Taylor series expansion, but not quite. We can apply yet another Taylor series expansion on the part in orange to yield\n\\[\\begin{align}\n\\textcolor{#ff7f0e}{E\\left[ \\dfrac{Y(1)}{Y(0)} \\right] \\approx \\dfrac{E[Y(1)]}{E[Y(0)]} -\\frac{\\operatorname{cov}[Y(1), Y(0)]}{\\mathrm{E}[Y(0)]^2}+\\frac{\\mathrm{E}[Y(1)]}{\\mathrm{E}[Y(0)]^3} \\operatorname{var}[Y(0)]} \\&gt;.\n\\end{align}\\]\nLet’s assemble this all together now. Our approximation is now\n\\[\\begin{align}\nE\\left[\\ln \\left( \\dfrac{Y(1)}{Y(0)} \\right)\\right]  &\\approx \\textcolor{#ff7f0e}{ \\dfrac{E[Y(1)]}{E[Y(0)]} -\\frac{\\operatorname{cov}[Y(1), Y(0)]}{\\mathrm{E}[Y(0)]^2}+\\frac{\\mathrm{E}[Y(1)]}{\\mathrm{E}[Y(0)]^3} \\operatorname{var}[Y(0)]}  \\textcolor{#1f77b4}{- 1 + \\dfrac{1}{2} E \\left[\\left(  \\dfrac{Y(1)}{Y(0)} -1 \\right)^2  \\right]} \\&gt;, \\\\\n&\\approx \\dfrac{E[Y(1)]}{E[Y(0)]} - 1 \\textcolor{#ff7f0e}{-\\frac{\\operatorname{cov}[Y(1), Y(0)]}{\\mathrm{E}[Y(0)]^2}+\\frac{\\mathrm{E}[Y(1)]}{\\mathrm{E}[Y(0)]^3} \\operatorname{var}[Y(0)]} \\textcolor{#1f77b4}{ + \\dfrac{1}{2} E \\left[\\left(  \\dfrac{Y(1)}{Y(0)} -1 \\right)^2  \\right]} \\&gt;.\n\\end{align}\\]\nFinally \\(\\delta\\) appears in our long and tedious approximation of \\(\\delta^\\star\\). Let’s ignore the terms colored in blue and orange for a moment and come back to them later.\nOur approximation is now\n\\[E\\left[\\ln \\left( \\dfrac{Y(1)}{Y(0)} \\right)\\right] \\approx \\dfrac{E[Y(1)]}{E[Y(0)]} - 1\\]\n\\[ \\delta^\\star \\approx \\delta -1  \\]\nExponentiating both sides\n\\[\\begin{align}\n\\exp(\\delta^\\star) &\\approx \\exp(\\delta-1) \\\\\n& \\approx 1 + \\delta -1 +  \\mathcal{O}((\\delta-1)^2)\\\\\n& \\approx \\delta\n\\end{align}\\]"
  },
  {
    "objectID": "posts/2022-11-25-taylor-series/index.html#we-did-itnow-what",
    "href": "posts/2022-11-25-taylor-series/index.html#we-did-itnow-what",
    "title": "Way Too Many Taylor Series",
    "section": "We Did It…Now What?",
    "text": "We Did It…Now What?\nNow that we’ve written down all the necessary approximations and assumptions, let’s go back and determine under what circumstances this is a valid approximation. Can we break this approximation? Can we break it really badly?\nLet’s leave that for the next blog post. I’m tired from doing all this algebra."
  },
  {
    "objectID": "posts/2022-08-16-pca/index.html",
    "href": "posts/2022-08-16-pca/index.html",
    "title": "PCA on The Tags for Cross Validated",
    "section": "",
    "text": "Julia Silge gave a really good talk in 2018 about PCA ond tags on stack overflow. She was able to interpret some of the components to infer some subgroups of users of stack overflow (front-end vs back-end, are they a Microsoft tech developer or not, are you an android dev or not, etc). These principal components were able to shed some light on what drove the variation in questions asked.\nI love this talk, and I crib it all the time. As of late, I’ve not been doing much SQL, so I figured I would recreate Julia’s analysis using data from cross validated. But this time, with a twist!\nWhat if instead of understanding the drivers of variance in questions asked, we analyze the kinds of questions users answer. This could give us insight into the type of analysts we have on cross validated. The site is intended to be for statistical analysis, but it has a mix of prediction questions, machine learning questions, econometrics questions, and much more. Hang around there long enough and you will see some familiar faces (mine included) and you get a pretty good sense of who answers what kinds of questions.\nI’m going to use data available from the stack exchange data explorer available here. I’ve included a code box in this post with the query I’ve used. I’ve sliced out the top 250 users as ranked by reputation and the top 100 tags as calculated by prevalence. We can use {tidymodels} to do a lot of the heay lifting. Let’s get to it."
  },
  {
    "objectID": "posts/2022-08-16-pca/index.html#data-modelling",
    "href": "posts/2022-08-16-pca/index.html#data-modelling",
    "title": "PCA on The Tags for Cross Validated",
    "section": "Data & Modelling",
    "text": "Data & Modelling\nLet’s Take a peek at the data, using me as an example. Below are my top 10 tags as a percent of my total answers. Looks like I like to answer questions about regression, hypothesis testing, and R most frequently. Each of the top 250 users has data similar to this in my dataset. I need to pivot it so that tags become features. Then, I can normalize the data and perform PCA.\n\n\n\n\n\nName\nTag\nPercent\n\n\n\n\nDemetri Pananos\nRegression\n11.40%\n\n\nDemetri Pananos\nHypothesis-Testing\n6.01%\n\n\nDemetri Pananos\nR\n5.90%\n\n\nDemetri Pananos\nMachine-Learning\n5.45%\n\n\nDemetri Pananos\nLogistic\n4.85%\n\n\nDemetri Pananos\nBayesian\n3.58%\n\n\nDemetri Pananos\nStatistical-Significance\n3.42%\n\n\nDemetri Pananos\nConfidence-Interval\n2.75%\n\n\nDemetri Pananos\nT-Test\n2.64%\n\n\nDemetri Pananos\nProbability\n2.31%\n\n\nDemetri Pananos\nGeneralized-Linear-Model\n2.31%\n\n\n\n\n\n\n\nCheckout how dummy easy the analysis is with tidymodels. I think I spent more time cleaning the data than I did modelling it.\n\nrec &lt;- recipe(rnk + UserId + DisplayName ~ ., data = d) %&gt;% \n       step_normalize(all_numeric_predictors()) %&gt;% \n       step_pca(all_numeric_predictors(), num_comp = 3) %&gt;% \n       prep()\n\nprin_comps &lt;- bake(rec, new_data = d) \nweights &lt;- rec %&gt;% \n           tidy(number = 2, type = \"coef\")\n\nThe last two lines extract both the principal components and the weights for each tag on each component. Now. we’re ready to make some plots."
  },
  {
    "objectID": "posts/2022-08-16-pca/index.html#principal-components",
    "href": "posts/2022-08-16-pca/index.html#principal-components",
    "title": "PCA on The Tags for Cross Validated",
    "section": "Principal Components",
    "text": "Principal Components\nThe results for the first 3 principal componensts are shown below. I’ve shown the 20 most extreme components for clarity.\nThe first principal compnent has tags like “Anova”, “T-Test” and “SPSS” as heavily weighted positive, while “Machine Learning”, “Mathematical Statistics” and “Probability” are all weighted heavily negative (the direction of the weights doesn’t matter, it isn’t like one direction is better or worse). To me, I read this as “Beginner” vs “Advanced” answers. Questions with the former tags are usually from users who are maybe taking a stats class for the first time and are learning about the t test or Anova. The dead give away for this is the “SPSS” tag being weighted so heavily1. Looking at more weights verifies this, with the negative weights being associated with topics like “Neural Nets”, and “Maximum Likelihod” while the positive weights have tags like “Statistical Significance” and “Interpretation”.\nNow remember, these components do not explain variance in questions. They explain variance in the question answers! So the first component is really about people who choose to answer simple versus complex topics. The second principal component has a fairly straightforward interpretation. This component explains variation between users who answer classical statistical questions versus those who opt to answer machine learning type questions. Lastly, the third principal component seems to be distinguishing users who answer forecasting type questions (see tags like “Arima”, “Time Series”, “Forecasting”, and “Econometrics”) versus non forecasting type questions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI think what is more interesting is that we can plot some of the more popular users on the site using the principal components. In the first plot, I’ve scattered PC1 vs PC2. Left to right means advanced vs simple questions. It is no surpriuse to see whuber farther left and BruceET farther right. Whuber can answer most anything, and I feel like he often accepts the challenge of a complex answer, opting to comment on simpler questions. Bruce, on the other hand, will always answer a simple question very robustly. Top to bottom means machine learning vs classical stats. I’m not surprised to see Frank Harrell closer to the top, as he has appeared in many questions if not only to scold people for using accuracy as a metric. No surprise Topepo is on the top of this PC. Interestingly, I’m kind of near the origin, if not a bit right of it. Seems like I strike a good balance between ML and stats, but often opt to answer simpler questions.\nPlotting PC1 vs PC3 shows a very predictable pattern. Users near the bottom are more forcasting types, so its no surprise that Rob Hyndman, Dimitris Rizopoulus (who does a lot of longitudinal work), and Stephan Kolassa are near the bottom. I’m near the top, I have no clue about any of that stuff to be honest."
  },
  {
    "objectID": "posts/2022-08-16-pca/index.html#conclusion",
    "href": "posts/2022-08-16-pca/index.html#conclusion",
    "title": "PCA on The Tags for Cross Validated",
    "section": "Conclusion",
    "text": "Conclusion\nThere appear to be at least three dimensions about which analysts on cross validated can be placed. Analysts can either opt to answer easy or difficult questions, which lean classical statistics or machine learning, with additional focus on forecasting on non-forecasting problems. That’s a fairly useful interpretation of the first three principal components!\nIt could be fun to think about how best to use this information. Now that we know these three kinds of subgroups, could we use it to recommend users questions to answer? Its been the case that there is a large gap between questions and answers, so maybe this could be useful but also maybe not."
  },
  {
    "objectID": "posts/2022-08-16-pca/index.html#footnotes",
    "href": "posts/2022-08-16-pca/index.html#footnotes",
    "title": "PCA on The Tags for Cross Validated",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBecause once you’ve taken a few stats courses, you know bette than to use SPSS.↩︎"
  },
  {
    "objectID": "posts/2023-12-01-ron/index.html",
    "href": "posts/2023-12-01-ron/index.html",
    "title": "Brain Teaser About Frequentist Statistics",
    "section": "",
    "text": "Ron Kohavi is somewhat of a prominent figure in the A/B testing community. Armed with having coauthored “Trustworthy Online Controlled Experiments”, experience at places like AirBNB and Microsoft, and what seems to be a $20/month subscription to ChatGPT, he will often post these kinds of questions on LinkedIn.\nThe post below really caught my attention (probably because of the AI generated accompanying picture, a fad everyone seems to be doing for some reason).\n\n\n\n\n\nThis should be fairly simple to reason through. Below is a plot depicting the concept of statistical power. The black curve is the sampling distribution under the null, the blue curve is the sampling distribution under the alternative implied by the minimal detectable effect (MDE). The area under the blue curve is the statsitical power (its the probability we observe a test statistic greater than the critical value).\n\n\nCode\nlibrary(tidyverse)\n\nx &lt;- seq(-5, 10, 0.1)\nxs &lt;- seq(1.96, 10, 0.1)\nplot(x, dnorm(x), type='l', labels=F, ylab='')\nlines(x, dnorm(x, 3, 1), type='l', col='blue')\npolygon(\n  c(xs, rev(xs)),\n  c(dnorm(xs, 3, 1), rep(0, length(xs))),\n  col=alpha('blue', 0.3),\n  border = F\n)\nlegend('topleft', col=c('black','blue'), legend = c(expression(H[0]), expression(H[A])), lty=c(1, 1))\n\n\n\n\n\n\n\n\n\nAccording to Ron, we run our experiment and observe the our (MDE) exactly. In our picture, that means we observe the mean of the sampling distribution under the alternative, so we just need to find how far out the MDE is with respect to the null distribution.\n\n\nCode\nlibrary(tidyverse)\n\nx &lt;- seq(-5, 10, 0.1)\nxs &lt;- seq(1.96, 10, 0.1)\nplot(x, dnorm(x), type='l', labels=F, ylab='')\nlines(x, dnorm(x, 3, 1), type='l', col='blue')\npolygon(\n  c(xs, rev(xs)),\n  c(dnorm(xs, 3, 1), rep(0, length(xs))),\n  col=alpha('blue', 0.3),\n  border = F\n)\nlegend('topleft', col=c('black','blue'), legend = c(expression(H[0]), expression(H[A])), lty=c(1, 1))\nabline(v=3)\n\n\n\n\n\n\n\n\n\nLet’s do a little bit of math. Assuming the sampling distribution under the null is standard normal, the critical value would be \\(\\mu_0 + \\sigma_0z_{1-\\alpha}\\). The critical value under the alternative would then be \\(\\mu_1 + z_{1-\\beta}\\sigma_1\\). Plugging in some numbers, this means the mean under the alternative should have a z-zcore of \\(\\mu_0 + \\sigma_0z_{1-\\alpha} - \\sigma_1z_{1-\\beta}\\). Now, we just need to evaluate \\(1 - \\mathbf{\\Phi}(\\mu_0 + \\sigma_0z_{1-\\alpha} - \\sigma_1z_{1-\\beta})\\).\nAssume that these distributions are standardized so that \\(\\mu_0 = 0\\) and \\(\\sigma_0 = \\sigma_1=1\\). This results in a p-value of\n\npnorm(2.8, lower.tail = F)\n\n[1] 0.00255513\n\n\nand I bet because I’ve only focused on one tail I should multiply by 2, so a p value of ~ 0.005, which seems to be correct."
  },
  {
    "objectID": "posts/2022-07-06-gsd/index.html",
    "href": "posts/2022-07-06-gsd/index.html",
    "title": "Interim Analysis & Group Sequential Designs Pt 1",
    "section": "",
    "text": "Special thanks to Jacob Fiksel for writing a great blog post which inspired me to write my own.\nAt Zapier, AB testing kind of has a bad rap. AB testing is perceived as slow – sometimes taking up to a month to complete a single test– with the chance that we don’t get a definitive result (i.e. we fail to reject the null). One of our priorities (and hence my priority) is to find a way to speed up AB testing so we can learn faster.\nPeeking is one way to do that. Peeking involves testing the experimental data before the end of the experiment (“peeking” at the results to see if they indicate a change). As you may know from other popular posts on the matter, or from sophomore stats, this can inflate the type one error. That’s a real shame, because peeking is a really attractive way to end an experiment early and save some time. Additionally, people are curious! They want to know how things are going. Fortunately, there are ways to satisfy the urge to peek while preserving the type one error rate.\nOne way to peek while preserving the type one error rate is through Group Sequential Designs (GSDs). This series of blog posts is intended to delve into some of the theory of GSDs. To me, theoretical understanding – knowing why something works, or at least being able to understand how in principle I could do this myself – is the key to learning. I’m happy to just do this in isolation, but I bet someone else may benefit too.\nI’m working mainly from this book, but I don’t anticipate I will discuss the entirety of the book. I really want to know a few key things:"
  },
  {
    "objectID": "posts/2022-07-06-gsd/index.html#goal-for-this-post",
    "href": "posts/2022-07-06-gsd/index.html#goal-for-this-post",
    "title": "Interim Analysis & Group Sequential Designs Pt 1",
    "section": "Goal For This Post",
    "text": "Goal For This Post\nWe know that under “peeking conditions” – just testing the data as they roll in – inflates the type one error rate. In this post, I want to understand why that happens. Like…where is the problem exactly? Where will be our theoretical basis for attacking the problem of controlling the type one error rate?\nBut first, a little background on GSDs."
  },
  {
    "objectID": "posts/2022-07-06-gsd/index.html#background",
    "href": "posts/2022-07-06-gsd/index.html#background",
    "title": "Interim Analysis & Group Sequential Designs Pt 1",
    "section": "Background",
    "text": "Background\nThe “G” in GSD means that the hypothesis test is performed on groups of observations. Given a maximum number of groups \\(K\\), the sample size of \\(k^{th}\\) each group is \\(n_k\\).\nThe “S” in GSD means the test is performed sequentially. If after observing the \\(k^{th}\\) group the test statistic (computed using all the data observed up to that point) is beyond some threshold, then the null is rejected and the experiment is finished. If not, the next group of observations is made and added to the existing data, wherein the process continues until the final group has been observed. If after observing the final group the test statistic does not exceed the threshold, then we fail to reject the null. The process for \\(K=2\\) is illustrated in Figure 1.\n\n\nCode\nflowchart TD\n  A[Observe Group k=1] --&gt; B[Perform Test]\n  B --&gt; C{Data From k=1 \\n Significant?}\n  C -- Yes --&gt; D[Reject Null]\n  C -- No --&gt; E[Observe Group k=2]\n  E --&gt; G{Data From k=1 and \\n k=2 Significant?}\n  G -- Yes --&gt; D\n  G -- No --&gt; H[Fail To Reject Null]\n\n\n\n\n\nFigure 1: A GSD for \\(K=2\\)\n\n\n\nflowchart TD\n  A[Observe Group k=1] --&gt; B[Perform Test]\n  B --&gt; C{Data From k=1 \\n Significant?}\n  C -- Yes --&gt; D[Reject Null]\n  C -- No --&gt; E[Observe Group k=2]\n  E --&gt; G{Data From k=1 and \\n k=2 Significant?}\n  G -- Yes --&gt; D\n  G -- No --&gt; H[Fail To Reject Null]"
  },
  {
    "objectID": "posts/2022-07-06-gsd/index.html#some-math-on-means",
    "href": "posts/2022-07-06-gsd/index.html#some-math-on-means",
    "title": "Interim Analysis & Group Sequential Designs Pt 1",
    "section": "Some Math on Means",
    "text": "Some Math on Means\nMeans are a fairly standard place to start for a statsitical test, so we will start there too. Let \\(X_{k, i}\\) be the \\(i^{th}\\) observation in the \\(k^{th}\\) group. Then the mean of group \\(k\\) is\n\\[ \\bar{X}_k = \\dfrac{1}{n_k} \\sum_{i=1}^{n_{k}} X_{k, i} \\]\nSince we are accumulating data, let’s write the cumulative mean up to and including group \\(k\\) as \\(\\bar{X}^{(k)}\\), and let the cumulative standard deviation up to and including group \\(k\\) be \\(\\sigma^{(k)}\\). We can actually write \\(\\bar{X}^{(k)}\\) in terms of the group means \\(\\bar{X}_{k}\\) using some algebra. Its just a weighted mean of the previous \\(\\bar{X}_{k}\\) weighted by the sample size.\n\\[\n\\bar{X}^{(k)} =  \\dfrac{\\sum_{\\tilde{k} = 1}^{k^\\prime} n_{\\tilde{k}} \\bar{X}_{\\tilde{k}}}{\\sum_{\\tilde{k} = 1}^{k^\\prime} n_{\\tilde{k}}}\n\\tag{1}\\]"
  },
  {
    "objectID": "posts/2022-07-06-gsd/index.html#a-simple-example",
    "href": "posts/2022-07-06-gsd/index.html#a-simple-example",
    "title": "Interim Analysis & Group Sequential Designs Pt 1",
    "section": "A Simple Example",
    "text": "A Simple Example\nRemember that our goal is to understand why the type one error rate increases when we peek as data accumulates, as we might do in an AB test. Answering how much is a little easier, so let’s do that first. Let’s do so by analyzing a \\(K=2\\) GSD where we assume:\n\nThat each group has the same sample size \\(n_1 = n_2 = n\\).\nThat the data we observe are IID bernoulli trials \\(X_{k, i} \\sim \\operatorname{Bernoulli}(p=0.5)\\) for \\(k=1, 2\\) and \\(j=1, \\dots, n\\).\nThat our false postie rate \\(\\alpha = 0.05\\)\n\n\nHow Much Does The Type One Inflate?\nLet’s just simulate data under the assumptions above. At each stage, let’s test the null that \\(H_0: p=0.5\\) against \\(H_A: p \\neq 0.5\\) and see how frequently we reject the null. In our simulation, we will assume “peeking” conditions, meaning we’re just going to do a test of proportions at each stage.\n\nlibrary(tidyverse)\n\nset.seed(0)\n\n# Simulation Parameters\np&lt;- 0.5\nn&lt;- 250\nnsims &lt;- as.integer((1/0.01)^2)\n\n# Run the simulation\nsims&lt;-rerun(nsims, {\n  # K=1\n  x1 &lt;- rbinom(1, n, p)\n  # K=2, accumulating data from each state\n  x2 &lt;- x1 + rbinom(1, n, p)\n  \n  # Compute some various quntities we will need, like the Z score\n  K &lt;- str_c('K=',1:2)\n  X &lt;- c(x1, x2) / ((1:2)*n)\n  mu &lt;- p\n  sds &lt;- sqrt(p*(1-p)/(n*1:2))\n  Z &lt;- (X-p)/sds\n  reject &lt;- abs(Z)&gt;1.96\n  \n  tibble(K, X, mu, sds, Z, reject)\n}) %&gt;% \n  bind_rows(.id='sim')\n\nfpr&lt;-sims %&gt;% \n  group_by(sim) %&gt;% \n  summarise(result=any(reject)) %&gt;% \n  summarise(fpr = mean(result)) %&gt;% \n  pull(fpr)\n\nFrom our simulation, we reject the null around 8.6% of the time. That is certainly higher than the nominal 5%, but if we recall our sophomore stats classes, isn’t there a 9.8% (\\(1-0.95^2\\)) chance we reject the null?\nThe 8.6% isn’t simulation error. We forgot that \\(\\bar{X}^{(1)}\\) and \\(\\bar{X}^{(2)}\\) are correlated. The correlation between \\(\\bar{X}^{(1)}\\) and \\(\\bar{X}^{(2)}\\) makes intuitive sense. If the sample mean for the first group is small, then the accumulated mean is also likely to be small than if we were to just take a new sample. Let’s take a more detailed look at Equation 1. Note that\n\\[ \\bar{X}^{(2)} = \\dfrac{n_1 \\bar{X}_1 + n_2\\bar{X}_2}{n_1 + n_2} \\&gt;.\\]\n\\(\\bar{X}^{(1)}\\) (which is just \\(\\bar{X}_1\\) ) appears in the expression for \\(\\bar{X}^{(2)}\\). In the extreme case where \\(n_2=1\\), the stage 2 mean is going to be \\(n_1 \\bar{X}_1/(n_1+1) + X_{2, 1}/(n_1+1)\\). How much could a single observation change the sample mean? It depends on the observation, but also on how big that sample is. The stuff you learned in sophmore stats about type one error inflating like \\(1 - (1-\\alpha)^k\\) assumes the test statistics are independent. So where does the 8.6% come from? To answer that, we need to understand the joint distribution of the \\(\\bar{X}^{(k)}\\).\n\n\nWhy The Type One Inflates\nThe assumptions we made above allow us to get a little analytic traction. We know that the sampling distribution of \\(\\bar{X}^{(1)}\\) and \\(\\bar{X}^{(2)}\\) are asymptotic normal thanks to the CLT\n\\[ \\bar{X}^{(1)} \\sim \\operatorname{Normal}\\left(p, \\dfrac{p(1-p)}{n}\\right)  \\]\n\\[ \\bar{X}^{(2)}\\sim \\operatorname{Normal}\\left( p, \\dfrac{p(1-p)}{2 n} \\right)  \\]\n\n\nCode\nmy_blue &lt;- rgb(45/250, 62/250, 80/250, 1)\ntheme_set(theme_classic())\n\n\nsims %&gt;% \n  ggplot(aes(X))+\n  geom_histogram(aes(y=..density..), fill = 'light gray', color = 'black')+\n  facet_wrap(~K) + \n  geom_line(aes(y = dnorm(X,\n                          mean = mu,\n                          sd = sds[PANEL])),\n            color = my_blue, \n            size = 1)+\n  theme(\n    panel.grid.major = element_line()\n  )+\n  labs(y='Density',\n       x = expression(bar(X)^(k)))\n\n\n\n\n10,000 simulations of a \\(K=2\\) GSD. Each group has 250 observations. Note that \\(\\bar{X}^{(2)}\\) has smaller standard error due to the fact that 500 (250 + 250) observations are used in the computation. Sampling distributions show in blue.\n\n\n\n\n\nConsider the random vector \\(\\theta = \\left(\\bar{X}^{(1)}, \\bar{X}^{(2)}\\right)\\). Since each components has a normal marginal then the joint must be multivariate normal\n\\[ \\theta \\sim \\mathcal{N}(\\mathbf{p}, \\Sigma) \\]\nwith mean \\(\\mathbf{p} = (p,p)\\) and covariance1\n\\[ \\Sigma= p(1-p)\\begin{bmatrix}\n\\dfrac{1}{n_1} &  \\dfrac{1}{n_1 + n_2} \\\\\n\\dfrac{1}{n_1 + n_2} & \\dfrac{1}{n_1 + n_2}\n\\end{bmatrix}\n\\]\n\n\nCode\nsigma_1 &lt;- sqrt(qchisq(0.95, 2))\nsigma_2 &lt;- sqrt(qchisq(0.99, 2))\nsig&lt;- p*(1-p) * matrix(c(1/n, 1/(2*n), 1/(2*n), 1/(2*n) ), nrow = 2)\ntt &lt;- seq(0, 1, 0.01)\nx &lt;- cos(2*pi*tt)\ny &lt;- sin(2*pi*tt)\nR &lt;- cbind(x,y)\ne = eigen(sig)\nV = sqrt(diag(e$values))\n\nlevel_curve_1 &lt;- sigma_1*R %*% (e$vectors %*% V %*% t(e$vectors)) + p\ncolnames(level_curve_1) &lt;- c(\"X1\", \"X2\")\nlevel_curve_1 &lt;- as_tibble(level_curve_1)\nlevel_curve_2 &lt;- sigma_2*R %*% (e$vectors %*% V %*% t(e$vectors)) + p\ncolnames(level_curve_2) &lt;- c(\"X1\", \"X2\")\nlevel_curve_2 &lt;- as_tibble(level_curve_2)\n\njoint &lt;-sims %&gt;% \n  select(sim, K, X) %&gt;% \n  pivot_wider(names_from='K', values_from='X') %&gt;% \n  rename(X1 = `K=1`, X2=`K=2`) %&gt;% \n  select(-sim) %&gt;% \n  sample_n(1000)\n\njoint %&gt;% \n  ggplot(aes(X1, X2))+\n  geom_point(color = 'dark gray', fill='gray', alpha = 0.5, shape=21)+\n  geom_path(data=level_curve_1, aes(X1, X2), color = my_blue, linetype='dashed')+\n  geom_path(data=level_curve_2, aes(X1, X2), color = my_blue)+\n  lims(x=c(.4, .6), y=c(.4, .6))+\n  theme(\n    panel.grid.major = element_line(),\n    aspect.ratio = 1\n  )+\n  labs(x=expression(bar(X)^(1)),\n       y=expression(bar(X)^(2)))\n\n\n\n\n\nFigure 2: 1000 samples from the density of \\(\\theta\\). Dashed line indicates where region of 95% probability, solid line indicates region of 99% probability.\n\n\n\n\n\n\n\n\nNow that we know the joint sampling distribution for our statistics of interest (namely \\(\\bar{X}^{(1)}\\) and \\(\\bar{X}^{(2)}\\)), let’s examine when we would reject the null under “peeking” conditions. For brevity, let’s call \\(Z^{(k)}\\) the standardized cumulative means. Then we would reject the null under “peeking” conditions if \\(\\Big\\vert Z^{(k)} \\Big\\vert &gt; 1.96\\) for at least one \\(k=1, 2\\). As a probabilistic statement, we want to know\n\\[ Pr\\left( \\Big\\vert Z^{(1)} \\Big\\vert &gt; 1.96 \\cup 1.96 &lt; \\Big\\vert Z^{(2)} \\Big\\vert \\right) \\&gt;. \\]\nBecause the joint is multivariate normal, we can compute this probability directly. However, I’m just going to simulate it.\n\n# Standardize the MVN by converting covariance matrix into a correlation matrix\nD &lt;- solve(diag(sqrt(diag(sig))))\ncormat &lt;- D %*% sig %*% D\nZ&lt;- MASS::mvrnorm((1/0.001)^2, rep(0, 2), cormat)\n\n\nz1 = abs(Z[, 1])&gt;1.96\nz2 = abs(Z[, 2])&gt;1.96\n\nfpr &lt;- mean(z1|z2)\n\nand we get something like 8.3%. But that doesn’t answer why, that just means I did my algebra correctly. As always, a visualization might help. take a look at Figure 3. The shaded regions show the areas where the null would be rejected. These are the areas we would make a false positive. The dots indicate the standardized draws from the density of \\(\\theta\\). Remember, this distribution is the null distribution for our GSD – these are draws from \\(\\theta\\) when \\(H_0\\) is true. And now here is the important part…\n\nThe shaded region depends on critical values we use for each test in the sequence. If we naively use \\(Z_{1-\\alpha/2}\\) as the critical value for each group as in “peeking” conditions, then the shaded region is too big!\n\n\n\nCode\nsims %&gt;% \n  select(sim, K, Z) %&gt;% \n  pivot_wider(names_from='K', values_from='Z') %&gt;% \n  rename(Z1 = `K=1`, Z2=`K=2`) %&gt;% \n  select(-sim) %&gt;% \n  sample_n(1000) %&gt;% \n  ggplot(aes(Z1, Z2))+\n  geom_point(color = 'dark gray', fill='gray', alpha = 0.5, shape=21)+\n  scale_x_continuous(limits = c(-5, 5), expand=c(0,0))+\n  scale_y_continuous(limits = c(-5, 5), expand=c(0,0))+\n  annotate(\"rect\", xmin = -5, xmax = -1.96, ymin = -5, ymax = 5, alpha = .5, fill=my_blue)+\n  annotate(\"rect\", xmin = 1.96, xmax = 5, ymin = -5, ymax = 5, alpha = .5, fill=my_blue)+\n  annotate(\"rect\", xmin = -1.96, xmax = 1.96, ymin = 1.96, ymax = 5, alpha = .5, fill=my_blue)+\n  annotate(\"rect\", xmin = -1.96, xmax = 1.96, ymin = -1.96, ymax = -5, alpha = .5, fill=my_blue)+\n  geom_hline(aes(yintercept=-1.96), linetype='dashed')+\n  geom_hline(aes(yintercept=1.96), linetype='dashed')+\n  geom_vline(aes(xintercept=-1.96), linetype='dashed')+\n  geom_vline(aes(xintercept=1.96), linetype='dashed')+\n  theme(\n    panel.grid.major = element_line(),\n    aspect.ratio = 1\n  )+\n  labs(x=expression(Z^(1)),\n       y=expression(Z^(2)))\n\nfind_region &lt;- function(za){\n  z1 = abs(Z[, 1])&gt;za\n  z2 = abs(Z[, 2])&gt;za\n\n  fpr &lt;- mean(z1|z2)\n  \n  (fpr - 0.05)^2\n}\n\nresult&lt;-optimize(find_region, interval=c(0, 3))\n\n\n\n\n\nFigure 3: Standardized draws from the density of \\(\\theta\\). Shaded regions indicate where the null hypothesis would be rejected under “peeking” conditions. The shaded region has approximately 8.5% probability mass and represents the false positive rate. We need to select a different region so that the shaded region has probability mass closer to 5%.\n\n\n\n\n\n\n\n\nThat is the why! When we naively just run our test each time we peek, we are defining a region in \\(\\theta\\) space which has too much probability. Peeking is fine, you just have to be careful in defining your rejection region in \\(\\theta\\) space. Defining a better rejection region isn’t too hard, and we can do it using a numerical search. When we do so, we find that using a critical value of 2.18 results in a type one error closer to the desired 5%. However, we’re implicitly restricted ourselves to having the threshold be the same for each group. That doesn’t have to be the case as we will see eventually."
  },
  {
    "objectID": "posts/2022-07-06-gsd/index.html#conclusion",
    "href": "posts/2022-07-06-gsd/index.html#conclusion",
    "title": "Interim Analysis & Group Sequential Designs Pt 1",
    "section": "Conclusion",
    "text": "Conclusion\nWe’ve done algebra, and it wasn’t for nothing. It have us insight into exactly what is going on and why the type one error increases under peeking. We also know that there is a way to fix it, we just need to define the shaded region a little more carefully. This will lead us to talk about alpha spending and various alpha spending functions."
  },
  {
    "objectID": "posts/2022-07-06-gsd/index.html#appendix",
    "href": "posts/2022-07-06-gsd/index.html#appendix",
    "title": "Interim Analysis & Group Sequential Designs Pt 1",
    "section": "Appendix",
    "text": "Appendix\n\nCovariance Calculation\nThe diagonals of the covariance matrix \\(\\Sigma\\) are simply the variances of the marginal distributions.\n\\[ \\Sigma_{1, 1} = \\dfrac{p(1-p)}{n_1} \\]\n\\[ \\Sigma_{2, 2} = \\dfrac{p(1-p)}{n_1 + n_2} \\]\nWhat remains is the covariance, which can be obtained with some covariance rules\n\\[ \\begin{align} \\operatorname{Cov}\\left(\\bar{X}^{(1)}, \\bar{X}^{(2)}\\right) &= \\operatorname{Cov}\\left(\\bar{X}_1, \\dfrac{n_1\\bar{X}_1 + n_2\\bar{X}_2}{n_1 + n_2}\\right)\\\\\n&=\\dfrac{n_1}{n_1 + n_2}\\operatorname{Var}(\\bar{X_1}) + \\dfrac{n_2}{n_1+n_2}\\operatorname{Cov}(\\bar{X}_1, \\bar{X}_2)\n\\end{align}\\]\nSince the groups are independent, the sample means are also independent (but the cumlative means are not). Meaning \\(\\operatorname{Cov}(\\bar{X}_1, \\bar{X}_2)=0\\) so\n\\[ \\operatorname{Cov}\\left(\\bar{X}^{(1)}, \\bar{X}^{(2)}\\right) = \\dfrac{p(1-p)}{n_1 + n_2} \\]"
  },
  {
    "objectID": "posts/2022-07-06-gsd/index.html#footnotes",
    "href": "posts/2022-07-06-gsd/index.html#footnotes",
    "title": "Interim Analysis & Group Sequential Designs Pt 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee the Section 6.1 for a calculation↩︎"
  },
  {
    "objectID": "posts/2025-08-10-shrinking/index.html",
    "href": "posts/2025-08-10-shrinking/index.html",
    "title": "Let’s Take About 15% Of The Top There",
    "section": "",
    "text": "I keep coming back to A New Look at P Values for Randomized Clinical Trials – an instant classic paper on RCTs, which could be extended to online experimentation as well. The authors, who include Gelman and Greenland, take an empirical Bayes approach to infer the joint distribution of z statistics and the Signal to Noise Ratio (SNR, the true effect divided by the standard error) from the Cochrane Database of Systematic Reviews, thereby being able to examine the relationship between, as an example, the exaggeration factor (the amount by which the estimate of the treatment effect over estimates the truth) and the p value. It is an incredibly well written paper and I highly recommend you read it.\nQuoting from the paper directly\n\nRecall that the z statistic is the estimated effect divided by the standard error (SE) of the estimate. We also wish to consider the signal-to-noise ratio (SNR), which is the true effect divided by the SE of the effect estimate. The SNR cannot be observed directly, but there is a very simple relation between the SNR and the z statistic. Because the estimated effect is equal to the true effect plus an independent normal error term, the z statistic is equal to the SNR plus an independent, standard normal error term.1 Thus, the distribution of the z statistic is the “convolution” of the distribution of the SNR and the standard normal distribution. The crux of our approach is that we can estimate the distribution of the absolute z statistics across the Cochrane Database and then derive the distribution of the absolute SNRs by “deconvolution,” that is, by removing the standard normal component. This allows us to study a number of important statistical properties of the RCTs in the Cochrane Database.\n\nThe authors give the weights and standard deviations for the elements of the mixture distribution, and from these we can simulate SNRs from the population of all trails that are exchangeable with those in the database (i.e. trials that could be in the Cochrane Database in the future). In R…\n\nlibrary(tidyverse)\nrmix = function(n,p,m,s){\n  d=rmultinom(n,1,p)\n  rnorm(n,m%*%d,s%*%d)\n}\n\n# Standard deviation for the snr\ns &lt;- c(0.61, 1.42, 2.16, 5.64)\np &lt;- c(0.32, 0.3, 0.3, 0.07)\nm &lt;- rep(0, 4)\nn &lt;- 1e6\nsnr &lt;- rmix(n, p, m, s)\nz &lt;- rnorm(n, snr, 1)\n\nNow that we have the simulated SNR and z statistics, it is very straight forward to replicate all results from the paper. As an example, here is how we can re-create figure 2 (the distribution of statistical power across the simulated SNR).\n\npower &lt;- pnorm(-1.96 - snr) + 1 - pnorm(1.96-snr)\nhist(power, xlab = 'Power', breaks=40, col = 'white', probability = T)\n\n\n\n\n\n\n\n\nThe paper goes on to make a number of conclusions, including that most RCTs which report a statistically significant treatment effect exaggerate the effect size, sometimes dramatically so (see Figure 3, top left). This exaggeration made me think “why not just shrink the damn thing then”, and given some of the information in the paper, I think we can come up with a nice little rule of thumb: most RCTs should shrink the treatment effect by about 15%. Let’s examine why.\nThe mixture distribution for the z statistics is comprised of 4 normal distributions with standard deviation 1.17, 1.74, 2.38, and 5.73, with mixture weights 0.32, 0.3, 0.3, and 0.07. This means that the standard deviation of the mixture is 2.32. Let’s make a not-too-wrong assumption that the distribution of z values is approximately normal. Granted, this is demonstrably false – the distribution has fatter tails, but doing so will allow us to treat this distribution as a prior for future RCTs. Now, suppose we run an RCT in the future which would be exchangeable with those in the Cochrane database. Since the distribution of z is assumed normal then we can shrink the estimates by leveraging a conjugate normal prior (granted, prior for the z is not normal, but it is very close).\n\ns_z &lt;- c(1.17, 1.74, 2.38, 5.73)\ns_weighted &lt;- sqrt(weighted.mean(s_z^2, p))\n\nnew_snr &lt;- rmix(n, p, m, s)\nnew_z &lt;- rnorm(n, new_snr, 1)\nshrunken_z &lt;- new_z * 1.0 / (1/s_weighted^2 + 1)\n\nnew_exag &lt;- abs(new_z/new_snr)\nshrunken_exag &lt;- abs(shrunken_z/new_snr)\n\nNote that the z are multiplied by a factor which is approximately 0.843 (round up to 0.85 for a nice number, which is equivalent to a 15% reduction). Now, let’s recreate figure 3 for the shurnken estimates and compare them to the unshrunken estimates\n\n\nCode\np_values &lt;- 2*pnorm(-abs(new_z))\np_strata &lt;- cut(p_values, c(c(0, 0.001, 0.005, 0.01, 0.05, 0.1), seq(.5, 1, 0.1)))\n\n\ndf_sum &lt;- tibble(\n  p_strata = p_strata, \n  Orignal = new_exag,\n  Shrunken = shrunken_exag\n) %&gt;% \n  pivot_longer(-p_strata, names_to = 'estimate', values_to = 'exag') %&gt;% \n  group_by(p_strata, \n           estimate\n           ) %&gt;%\n  summarise(\n    q25 = quantile(exag, 0.25, na.rm = TRUE),\n    q50 = quantile(exag, 0.50, na.rm = TRUE),\n    q75 = quantile(exag, 0.75, na.rm = TRUE),\n    mn = mean(exag)\n  ) %&gt;%\n  mutate(strata_num = as.numeric(p_strata))\n\n# Plot\ndodge_width &lt;- -0.2\n\nggplot(df_sum, aes(x = strata_num, fill = estimate)) +\n  geom_rect(aes(\n    xmin = strata_num - 0.2 + \n      ifelse(estimate == \"Orignal\", -dodge_width, dodge_width),\n    xmax = strata_num + 0.2 + \n      ifelse(estimate == \"Orignal\", -dodge_width, dodge_width),\n    ymin = q25, ymax = q75\n  ), alpha = 0.7, color='black'\n  ) +\n  geom_segment(aes(\n    x = strata_num - 0.2 + \n      ifelse(estimate == \"Orignal\", -dodge_width, dodge_width),\n    xend = strata_num + 0.2 + \n      ifelse(estimate == \"Orignal\", -dodge_width, dodge_width),\n    y = q50, yend = q50\n  ),\n  size = 1\n  ) +\n  geom_hline(yintercept = 1, color = \"grey50\", size = 2, alpha = 0.5) +\n  scale_x_reverse(\n    breaks = unique(df_sum$strata_num),\n    labels = levels(p_strata),\n    guide = guide_axis(n.dodge = 2)\n  ) +\n  labs(\n    x = \"P Value Strata\",\n    y = \"Exaggeration Quartiles\",\n    fill='Estimate'\n  ) +\n  theme_classic(base_size = 14) +\n  theme(legend.position = 'top') + \n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\n\nUnsurprisingly, the estimates are shrunk towards an exaggeration factor of 0.0 and results from statistically significant trials have a smaller exaggeration factor. However, we can do better. van Zwet (also an author on the paper) published a shrinkage estimator based on these data in Statsitics in Medicine in 2021. His approach was to use \\(\\mathbb{E}(\\mbox{SNR} \\mid z)\\) as the shrunken estimate, and provides some code in that paper from which to estimate the conditional distribution of the SNR given z from the mixture of normals (it is actually quite clever and doesn’t require any regression, only some algebra). I’ve plotted van Zwet’s exaggeration factors below, which are even better than the normal normal conjugate model’s! Unfortunately, the shrinkage van Zwet’s shrinkage estimator doesn’t have such a simple rule of thumb which could make for a snappy, Letterkenny referencing, blog post 🤷.\n\n\nCode\nz_hat &lt;- map_dbl(new_z, ~{\n  \n  s2 &lt;- s_z^2\n  q &lt;- p*dnorm(.x,0,sqrt(s2+1))\n  q &lt;- q/sum(q)\n  m &lt;- .x * s2 / (s2+1)\n  sum(q * m)\n})\n\n\nzwet_shrunken_exag &lt;- abs(z_hat / new_snr)\n\n# Assign a numeric index per estimate for dodging\ndf_sum &lt;- tibble(\n  p_strata = p_strata, \n  Orignal = new_exag,\n  Shrunken = shrunken_exag,\n  `van Zwet Shrunken` = zwet_shrunken_exag\n) %&gt;% \n  pivot_longer(-p_strata, names_to = 'estimate', values_to = 'exag') %&gt;% \n  group_by(p_strata, \n           estimate\n           ) %&gt;%\n  summarise(\n    q25 = quantile(exag, 0.25, na.rm = TRUE),\n    q50 = quantile(exag, 0.50, na.rm = TRUE),\n    q75 = quantile(exag, 0.75, na.rm = TRUE),\n    mn = mean(exag)\n  ) %&gt;%\n  mutate(strata_num = as.numeric(p_strata)) %&gt;%\n  group_by(p_strata) %&gt;%\n  mutate(est_index = as.numeric(factor(estimate, \n                                       levels = c(\"van Zwet Shrunken\", \"Shrunken\", \"Orignal\")))) %&gt;%\n  ungroup()\n\nbar_width &lt;- 0.25\n\nggplot(df_sum, aes(x = strata_num, fill = estimate)) +\n  # IQR rectangles\n  geom_rect(aes(\n    xmin = strata_num + (est_index - 2) * bar_width,\n    xmax = strata_num + (est_index - 2) * bar_width + bar_width,\n    ymin = q25, ymax = q75\n  ),\n  colour = \"black\", alpha = 0.7\n  ) +\n  # Median lines\n  geom_segment(aes(\n    x = strata_num + (est_index - 2) * bar_width,\n    xend = strata_num + (est_index - 2) * bar_width + bar_width,\n    y = q50, yend = q50\n  ),\n  size = 1, colour = \"black\"\n  ) +\n  geom_hline(yintercept = 1, color = \"grey50\", size = 1.5, alpha = 0.5) +\n  scale_x_reverse(\n    breaks = unique(df_sum$strata_num),\n    labels = levels(p_strata),\n    guide = guide_axis(n.dodge = 2)\n  ) +\n  labs(\n    x = \"P Value Strata\",\n    y = \"Exaggeration Quartiles\",\n    fill = 'Estimate'\n  ) +\n  theme_classic(base_size = 14) +\n  theme(legend.position = 'top') + \n  scale_fill_brewer(palette = \"Set1\")"
  },
  {
    "objectID": "posts/2024-05-21-interview/index.html",
    "href": "posts/2024-05-21-interview/index.html",
    "title": "Interesting Interview Questions",
    "section": "",
    "text": "I’m about to do some interviews this week which got me reflecting on some of my favorite questions I’ve been asked. Usually, these are little toy problems you’d find in an intro to probability textbook, but they can be pretty fun and rewarding to solve."
  },
  {
    "objectID": "posts/2024-05-21-interview/index.html#question-1",
    "href": "posts/2024-05-21-interview/index.html#question-1",
    "title": "Interesting Interview Questions",
    "section": "Question 1",
    "text": "Question 1\nPerson A has \\(n\\) fair coins, and person B has \\(n+1\\) fair coins. They each can flip all their coins simultaneously. What is the probability that person B gets more heads? Provide your answer as a function of \\(n\\)."
  },
  {
    "objectID": "posts/2024-05-21-interview/index.html#answer",
    "href": "posts/2024-05-21-interview/index.html#answer",
    "title": "Interesting Interview Questions",
    "section": "Answer",
    "text": "Answer\nI actually took the liberty of editing this question (the function of \\(n\\) was not included). Anyway, this is pretty simple. Suppose \\(n\\) is large enough to justify using a normal approximation. Then\n\\[ A \\sim \\operatorname{normal}\\left( \\dfrac{n}{2}, \\dfrac{n}{4} \\right) \\&gt;,\\]\n\\[ B \\sim \\operatorname{normal}\\left( \\dfrac{n}{2}, \\dfrac{n}{4} \\right) \\&gt;.\\]\nWe’re interested in \\(\\Pr(B \\gt A) = \\Pr(B - A \\gt 0)\\). So let \\(D=B-A\\). Then \\(D\\) has the following distribution\n\\[ D \\sim \\operatorname{normal}\\left( \\dfrac{1}{2}, \\dfrac{2n+1}{4} \\right) \\&gt;.\\]\nNow, we just need to make a standardized normal random variable and pass it through the CDF of a gaussian. That turns out to be\n\\[ Z = \\dfrac{0-\\frac{1}{2}}{\\sqrt{\\frac{2n+1}{4}}} = \\dfrac{-1}{\\sqrt{2n+1}} \\]\nand so\n\\[ \\Pr(B \\gt A) = Pr(D \\gt 0) = 1 - \\Phi(Z(n)) \\&gt;.\\]\nI’m not sure what the interviewer expected, but we can see that the probability approaches 1/2 as \\(n \\to \\infty\\).\nNow, this might be a “good ’nuff” answer, but it relies on an approximation that breaks down for small \\(n\\). What is the REAL answer?\nFor that, we need to use convolution. If \\(D=B-A\\) then \\(D\\) can take on integer values between \\(-n\\) (where B flips 0 heads and A flips \\(n\\)) and \\(n+1\\) (where B flips \\(n+1\\) heads and A flips 0).\nThe convolution is then the following sum\n\\[\\Pr(D = k) = \\sum_{i=1}^{n+1} \\Pr(B=i) \\Pr(A = i-k)\\]\nwith the added stipulation that \\(\\Pr(A&gt;n) = 0\\). Ok, not a fun sum to do by hand, let’s cook up a numpy function\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom, norm\nimport matplotlib.patches as patches\nimport pandas as pd\nimport itertools\n\ndef f(k:int, n:int):\n    i = np.arange(1, n+2).reshape(-1, 1)\n    density = binom(n=n+1, p=0.5).pmf(i)*binom(n=n, p=0.5).pmf(i-k)\n    return density.sum()\n\ndef approx_f(n:int):\n    z = -1 / np.sqrt(2*n+1) \n    return 1 - norm.cdf(z)\n\nmax_coins = 1000\ncoins = np.arange(1, max_coins+1, 5)\nprobs = np.zeros(coins.size)\n\nfor i, n in enumerate(coins):\n\n    outcomes = np.arange(1, n+2)\n    probs[i] = f(outcomes, n)\n\n\n\n\n\n\n\n\n\n\nIn fact, it looks like the probability is always 0.5, which is interesting. Let’s see why.\nCredit to Misha Lavrov for showing me this very simple and elegant solution. Again, let \\(D = B-A\\), but now instead focus on \\(D+n = B + (n-A)\\) which is the number of heads from \\(B\\) plus the number of tails from \\(A\\). Since the coins are fair, this is the sum of two binomials with \\(2n+1\\) trials. Hence, the probability mass function is\n\\[ \\Pr(D+n = n+k) = {2n+1 \\choose n+k} 2^{-2n-1} \\&gt;, k \\in \\left[ -n \\&gt;, \\cdots \\&gt;, n+1\\right] \\]\nThis means \\(\\Pr(D+n)\\) is symmetric about \\(n+0.5\\) implying that \\(\\Pr(D\\leq 0) = \\Pr(D\\gt 0) = 0.5\\) as desired."
  },
  {
    "objectID": "posts/2024-09-01-ab-length/index.html",
    "href": "posts/2024-09-01-ab-length/index.html",
    "title": "How Long to Run a Bayesian AB Test",
    "section": "",
    "text": "Bayesian decision making with loss functions was a very popular with stakeholders and data scientists when I was working in AB testing. In short, eschew p values instead ask “If I made this decision, and I was wrong, what do I stand to lose?” and then make the decision which minimizes your expected loss. In my opinion, and from what I’ve heard from others, this is an attractive method because it places the thing we care most about (usually revenue, or improvement to the metric) at the heart of the decision.\nBut while Bayes is nice for decision making, I would always get the same question from stakeholders: “How long do we run the experiment”? As a frequentist, you can answer this by appealing to statistical power, but Bayes has at least more options. Kruschke 1 argues for using posterior precision as a stopping criterion, others (e.g. David Robinson2) have mentioned setting some threshold for expected loss. These seem to be the most popular from what I’ve read, and I think both of these make a lot of sense – especially the stopping criterion based on precision. But that doesn’t really answer my question. I want to answer “how long do we have to run the experiment”, which should be answered in a unit of time and not “until your precision is \\(x\\)”. Answering in this way just results in another question, such as “how much precision do I need”. Additionally, these approaches have no time element – the experiment could go on for much longer than we might be willing to accept, and we may still not reach our stopping criterion. What do we do in those cases, and how do we understand the trade offs? How do I prevent stakeholders from using the stopping without the stopping criterion as precedence for running under powered experiments? I don’t have answers to those questions.\nWhen I asked about this on twitter, a few people mentioned The Expected Value of Sample Information (EVSI) which I think is closer to what I want. In short, the EVSI helps us understand the change in expected loss the decision maker could obtain from getting access to an additional sample before making a decision. This is closer to what I want, because now I could say to a stakeholder “Don’t/Stop now, there is a \\(p\\%\\) chance the results would change in a week”. It may not be a direct answer for how long, but its closer. Additionally, Carl Vogel (then of Babylist) seemed to be using this approach, as explained in his posit::conf 2023 talk3, so it seems like SOMEONE is doing this already and I’m not just crazy.\nI think the EVSI approach is good, but I have lingering questions, mostly about calibration. So in this post, I want to run a few simulations to see how well calibrated this procedure can be in the case where we run an experiment with two variants and a binomial outcome. I’ll set up some math as a refresher, then we’ll dive into the simulations."
  },
  {
    "objectID": "posts/2024-09-01-ab-length/index.html#introduction",
    "href": "posts/2024-09-01-ab-length/index.html#introduction",
    "title": "How Long to Run a Bayesian AB Test",
    "section": "",
    "text": "Bayesian decision making with loss functions was a very popular with stakeholders and data scientists when I was working in AB testing. In short, eschew p values instead ask “If I made this decision, and I was wrong, what do I stand to lose?” and then make the decision which minimizes your expected loss. In my opinion, and from what I’ve heard from others, this is an attractive method because it places the thing we care most about (usually revenue, or improvement to the metric) at the heart of the decision.\nBut while Bayes is nice for decision making, I would always get the same question from stakeholders: “How long do we run the experiment”? As a frequentist, you can answer this by appealing to statistical power, but Bayes has at least more options. Kruschke 1 argues for using posterior precision as a stopping criterion, others (e.g. David Robinson2) have mentioned setting some threshold for expected loss. These seem to be the most popular from what I’ve read, and I think both of these make a lot of sense – especially the stopping criterion based on precision. But that doesn’t really answer my question. I want to answer “how long do we have to run the experiment”, which should be answered in a unit of time and not “until your precision is \\(x\\)”. Answering in this way just results in another question, such as “how much precision do I need”. Additionally, these approaches have no time element – the experiment could go on for much longer than we might be willing to accept, and we may still not reach our stopping criterion. What do we do in those cases, and how do we understand the trade offs? How do I prevent stakeholders from using the stopping without the stopping criterion as precedence for running under powered experiments? I don’t have answers to those questions.\nWhen I asked about this on twitter, a few people mentioned The Expected Value of Sample Information (EVSI) which I think is closer to what I want. In short, the EVSI helps us understand the change in expected loss the decision maker could obtain from getting access to an additional sample before making a decision. This is closer to what I want, because now I could say to a stakeholder “Don’t/Stop now, there is a \\(p\\%\\) chance the results would change in a week”. It may not be a direct answer for how long, but its closer. Additionally, Carl Vogel (then of Babylist) seemed to be using this approach, as explained in his posit::conf 2023 talk3, so it seems like SOMEONE is doing this already and I’m not just crazy.\nI think the EVSI approach is good, but I have lingering questions, mostly about calibration. So in this post, I want to run a few simulations to see how well calibrated this procedure can be in the case where we run an experiment with two variants and a binomial outcome. I’ll set up some math as a refresher, then we’ll dive into the simulations."
  },
  {
    "objectID": "posts/2024-09-01-ab-length/index.html#refresher-on-decision-making",
    "href": "posts/2024-09-01-ab-length/index.html#refresher-on-decision-making",
    "title": "How Long to Run a Bayesian AB Test",
    "section": "Refresher on Decision Making",
    "text": "Refresher on Decision Making\nSuppose I launch a test with two variants. I’m interested in estimating each variant’s mean outcome, \\(\\theta\\) (which for all intents and purposes could a conversion rate for some process). I collect some data, \\(y\\), and I get posterior distributions for each \\(\\theta\\), \\(p(\\theta \\mid y)\\). I need to make a decision about which variant to launch, and I want to do that based on their mean outcomes, but I don’t know the mean outcomes perfectly. One way to make the decision is through minimizing expected loss. Let \\(\\mathcal{L}(\\theta_A, \\theta_B, x)\\) be the loss I would incur were I to launch variant \\(x\\) (e.g. A/B) when in truth \\(\\neg x\\) (e.g. B/A) was the superior variant. The expected loss for shipping \\(x\\) is provided by the following integral\n\\[ E[\\mathcal{L}](x) = \\int_{\\theta_A} \\int_{\\theta_{B}} \\mathcal{L}(\\theta_A, \\theta_B, x) p(\\theta_A, \\theta_{B} \\mid y_B, y_B) \\, d\\theta_A \\, d\\theta_B \\&gt;, \\]\nwhere \\(p\\) is the joint posterior distribution of \\(\\theta_A, \\theta_B\\). The decision, \\(D\\), we should make is to launch the variant which results in smallest expected loss\n\\[ D =  \\underset{{x \\in \\{A, B\\}}}{\\operatorname{arg min}} \\left\\{ E[\\mathcal{L}](x)  \\right\\} \\&gt;. \\]\nFor more on this sort of evaluation for AB testing, see this report by Chris Stucchio. In what follows, I’m going to use the following loss function\n\\[ \\mathcal{L}(\\theta_A, \\theta_b, x) = \\begin{cases} & \\max(\\theta_B-\\theta_A, 0) \\&gt;, & x=A \\\\\\\\ & \\max(\\theta_A-\\theta_B, 0) \\&gt;, & x=B\\end{cases} \\&gt; \\]\nI interpret this loss function to be the improvement to the mean outcome we would has missed out on, had we shipped the wrong variant. As an example, if we ship A but B is truly better, then we lose out on a total of \\(\\theta_B - \\theta_A\\). If we ship A and A is truly the superior variant, then we lose out on 0 improvement to the metric (because we chose right). Note that this works for revenue too! This decision can be written succinctly in code using draws from the posterior as follows\n\ndecision &lt;- function(draws_a, draws_b){\n  diff &lt;- draws_b - draws_a\n  loss_a &lt;- mean(pmax(diff, 0))\n  loss_b &lt;- mean(pmax(-diff, 0))\n  \n  if(loss_a &lt; loss_b){\n    winner &lt;- \"A\"\n  } else {\n    winner &lt;- \"B\"\n  }\n  \n  list(winner=winner, loss_a=loss_a, loss_b=loss_b)\n}"
  },
  {
    "objectID": "posts/2024-09-01-ab-length/index.html#quick-primer-on-evsi",
    "href": "posts/2024-09-01-ab-length/index.html#quick-primer-on-evsi",
    "title": "How Long to Run a Bayesian AB Test",
    "section": "Quick Primer on EVSI",
    "text": "Quick Primer on EVSI\nThe expected loss from the optimal decision given the data \\(y_A, y_B\\) would be\n\\[\\begin{align*} E[\\mathcal{L}](D) &=   \\underset{{x \\in \\{A, B\\}}}{\\operatorname{min}} \\left\\{ E[\\mathcal{L}](x)  \\right\\} \\\\ &= \\underset{{x \\in \\{A, B\\}}}{\\operatorname{min}}  \\int_{\\theta_A} \\int_{\\theta_{B}} \\mathcal{L}(\\theta_A, \\theta_B, x) p(\\theta_A, \\theta_{B} \\mid y_A, y_B) \\, d\\theta_B \\, d\\theta_A  \\end{align*} \\]\nSuppose I had the opportunity to gain additional samples. Were I to observe \\(\\tilde{y}_A, \\tilde{y}_B\\), the expected loss would be\n\\[ E[\\mathcal{L}](\\tilde{D})= \\underset{{x \\in \\{A, B\\}}}{\\operatorname{min}}  \\int_{\\theta_A} \\int_{\\theta_{B}} \\mathcal{L}(\\theta_A, \\theta_B, x) p(\\theta_A, \\theta_{B} \\mid y_A + \\tilde{y}_A, y_B + \\tilde{y}_B) \\, d\\theta_B \\, d\\theta_A  \\] where \\(\\tilde{D}\\) is the optimal decision from the new data. Of course, we can’t say what this optimal decision would be since we don’t know the new data. But, having a model for these data, we could sample from that model and integrate over the uncertainty for \\(\\tilde{y}_A, \\tilde{y}_B\\). That means we would compute\n\\[ E[\\mathcal{L}](\\tilde{D})= \\int_{\\theta_a} \\int_{\\theta_b}\\underset{{x \\in \\{A, B\\}}}{\\operatorname{min}}  \\int_{\\tilde{\\theta}_A} \\int_{\\tilde{\\theta}_{B}} \\mathcal{L}(\\tilde{\\theta}_A, \\tilde{\\theta}_B, x) p(\\tilde{\\theta}_A, \\tilde{\\theta}_{B} \\mid y_A + \\tilde{y}_A, y_B + \\tilde{y}_B) p(\\tilde{y}_A, \\tilde{y}_B \\mid \\theta_A, \\theta_B ) p(\\theta_A, \\theta_B \\mid y_A, y_B) \\, d\\tilde{\\theta}_B \\, d\\tilde{\\theta}_A  \\, d\\theta_B \\, d\\theta_A \\]\nNow, this looks like a right harry integral. However, its straightforward to understand. Consider the following process:\n\nUse the posterior predictive (\\(p(\\tilde{y}_A, \\tilde{y}_B \\mid \\theta_A, \\theta_B ) p(\\theta_A, \\theta_B \\mid y_A, y_B)\\)) to sample data you would have seen were you to continue the experiment.\nAct as if that data was the true data and condition your model on it to obtain your future updated posterior (\\(p(\\tilde{\\theta}_A, \\tilde{\\theta}_{B} \\mid y_A + \\tilde{y}_A, y_B + \\tilde{y}_B)\\))\nCompute the optimal loss for each decision having seen that data\nDetermine what decision you would make had that been the data you would have seen.\nRepeat for as many draws from the posterior predictive as you like, and average over samples.\n\nThe EVSI should then be\n\\[EVSI = E[\\mathcal{L}](D) -  E[\\mathcal{L}](\\tilde{D}) \\]\nwhich can be understood as the expected reduction in expected loss were you to continue sampling."
  },
  {
    "objectID": "posts/2024-09-01-ab-length/index.html#evsi-changing-ones-mind-and-calibration",
    "href": "posts/2024-09-01-ab-length/index.html#evsi-changing-ones-mind-and-calibration",
    "title": "How Long to Run a Bayesian AB Test",
    "section": "EVSI, Changing One’s Mind, and Calibration",
    "text": "EVSI, Changing One’s Mind, and Calibration\nWhile EVSI can be used to determine if one should continue to sample based on costs and expected benefits, I am interested in a slightly different question. Ostensibly, EVSI can be used to justify continued sampling in the event one variant looks like a winner because, as Carl Vogel says, “[each future] posterior may not change my mind at all based on what I was going to do under the prior. They may change my mind a lot based on what I was going to do under the prior”. This leads me to ask about calibration for changing my mind. If this procedure tells me there is a \\(p\\%\\) I change my mind about which variant to ship, do I actually change my mind \\(p\\%\\) of the time?\nThe answer is “Yea, kind of”! To see this, let’s do some simulation. I’ll simulate some data from a two variant AB test with a binary outcome. Then, I will compute the EVSI and determine if more data might change my mind as to what variant I might ship. Then, I’ll simulate additional data from the truth to see if my mind would have changed.\nFrom there we can determine the calibration by looking at the proportion of simulations in which my mind actually changed, versus the proportion of simulations in which the EVSI computation forecasted I would have changed my mind. The code is in the cell below if you want to look at how I do this.\nHere is a high level overview of the simulation:\n\nThe true probability in group A is 10%.\nThe true probability in group B is 10.3% (a 3% lift).\nWe get 20, 000 subjects in the experiment per week, which is 10, 000 in each group per week.\nWe run the experiment for a single week and ask “If I ran for one more week, might I change my mind about what I ship”?\nWe use a beta binomial model for the analysis, and we have an effective prior sample size of 1000 (our priors are \\(\\alpha = 100\\) and \\(\\beta=900\\)).\n\nI realize there are a lot of hyperparameters we could tune here: More/fewer samples, higher/lower lift, day of week effects, etc etc. I’m going to keep it very simple for now and let you experiment as you see fit. This also means I can’t make sweeping statements about how well this performs or not.\n\n\nCode\nlibrary(tidyverse)\n\ntrue_probability_a &lt;- 0.1\ntrue_lift &lt;- 1.03\n\ntrue_probability_b &lt;- true_probability_a * true_lift\nN &lt;- 10000\nnsims &lt;- 1000\n\nALPHA &lt;- 100\nBETA &lt;- 900\n\nfuture::plan(future::multisession, workers=10)\nresults &lt;- furrr::future_map_dfr(1:nsims, ~{\n  \n  # Run the experiment and observe outcomes\n  ya &lt;- rbinom(1, N, true_probability_a)\n  yb &lt;- rbinom(1, N, true_probability_b)\n  \n  # Compute the posterior\n  posterior_a &lt;- rbeta(1000, ALPHA + ya, BETA + N - ya)\n  posterior_b &lt;- rbeta(1000, ALPHA + yb, BETA + N - yb)\n  \n  # What decision would we make RIGHT NOW if we had to?\n  out_now &lt;- decision(posterior_a, posterior_b)\n  current_decision &lt;- out_now$winner\n  current_loss_a &lt;- out_now$loss_a\n  current_loss_b &lt;- out_now$loss_b\n  \n  # What might we see in the future, conditioned on what we know now?\n  forecasted_out &lt;- map(1:nsims, ~{\n    \n\n    theta_a &lt;- sample(posterior_a, size=1)\n    theta_b &lt;- sample(posterior_b, size=1)\n    \n    \n    future_y_a &lt;- rbinom(1, N, theta_a)\n    future_y_b &lt;- rbinom(1, N, theta_b)\n    \n    future_posterior_a &lt;- rbeta(1000, ALPHA + ya + future_y_a, BETA + 2*N - ya - future_y_a)\n    future_posterior_b &lt;- rbeta(1000, ALPHA + yb + future_y_b, BETA + 2*N - yb - future_y_b)\n    \n    decision(future_posterior_a, future_posterior_b)\n    \n  })\n  \n  # Laplace smoothing to make sure there is always a non-zero chance of something happening.\n  forecasted_winners&lt;- c(\"A\", \"A\", \"B\", \"B\", map_chr(forecasted_out, 'winner'))\n  forecasted_loss_a &lt;- map_dbl(forecasted_out, 'loss_a')\n  forecasted_loss_b &lt;- map_dbl(forecasted_out, 'loss_b')\n  \n  prob_mind_change &lt;- mean(forecasted_winners != current_decision)\n  \n  # Now, run the experiment into the future to see if we changed our minds\n  \n  ya &lt;- ya + rbinom(1, N, true_probability_a)\n  yb &lt;- yb + rbinom(1, N, true_probability_b)\n  \n  # Compute the posterior\n  posterior_a &lt;- rbeta(1000, ALPHA + ya, BETA + 2*N - ya)\n  posterior_b &lt;- rbeta(1000, ALPHA + yb, BETA + 2*N - yb)\n  \n  # What decision would we make RIGHT NOW if we had to?\n  future_decision &lt;- decision(posterior_a, posterior_b)\n  \n  mind_changed &lt;- as.numeric(future_decision$winner != current_decision)\n  \n  tibble(current_winner = current_decision, \n         future_winner = future_decision$winner,\n         mind_changed, \n         prob_mind_change, \n         forecasted_evsi_a = current_loss_a-mean(forecasted_loss_a), \n         forecasted_evsi_b = current_loss_b-mean(forecasted_loss_b), \n         true_evsi_a = current_loss_a - future_decision$loss_a,\n         true_evsi_b = current_loss_b - future_decision$loss_b)\n  \n  \n}, \n.progress = T, \n.options = furrr::furrr_options(seed =T))\n\n\nCalibration results are shown below. Its…not bad, right? I suppose if this calibration really irked you then you could run this simulation assuming the true lift was \\(z\\%\\), and recalibrate the estimates using whatever procedure you like!\n\n\nCode\nout &lt;- with(results, CalibrationCurves::valProbggplot(prob_mind_change, mind_changed))\n\nout$ggPlot"
  },
  {
    "objectID": "posts/2024-09-01-ab-length/index.html#conclusions",
    "href": "posts/2024-09-01-ab-length/index.html#conclusions",
    "title": "How Long to Run a Bayesian AB Test",
    "section": "Conclusions",
    "text": "Conclusions\nI really like EVSI, and generally this whole procedure for thinking about if we should continue to collect data or not. The results from this tiny little simulation are not intended to be generalizable, and they very likely depend on things like the prior, how much additional data you would collect, and the priors for the model. However, I’ve chosen these parameters to roughly reflect real world scenarios I came across working in AB testing, so this is really encouraging to me!\nSo how would I answer a stakeholder who wanted to know how long to run the test if they wanted to use a Bayesian method? Generally I might advise a minimum run length of 1 week (so we can integrate over day of week effects). Then, its all sequential from there! Use this procedure to determine the probability you change your mind, and how much the additional sample might change your mind in terms of revenue/improvement/whatever your criterion is. There may come a time at which no more data would change your mind (at least according to the data you’ve seen so far) and at that point the inference problem is kind of irrelevant.\nGenerally, there are no “right” procedures when it comes to decision making – only trade offs. I particularly like this approach because its good at triaging risk, which is what I think data science should be."
  },
  {
    "objectID": "posts/2024-09-01-ab-length/index.html#footnotes",
    "href": "posts/2024-09-01-ab-length/index.html#footnotes",
    "title": "How Long to Run a Bayesian AB Test",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDoing Bayesian data analysis↩︎\nhttp://varianceexplained.org/r/bayesian-ab-testing/↩︎\nhttps://www.youtube.com/watch?v=GN5PJXajxKw↩︎"
  },
  {
    "objectID": "posts/2023-12-02-gen-gamma/index.html",
    "href": "posts/2023-12-02-gen-gamma/index.html",
    "title": "The Generalized Gamma Distribution For Parametric Survival",
    "section": "",
    "text": "A couple weeks back, I posted a little something something on the Shifted Beta Geometric distribution. That distribution is used in estimating churn in contractual settings (think Netflix, or any other service whereby you renew your service monthly). Its a nice model, but I want something more flexible.\nI’ve been aware of the generalized gamma distribution through Jordan Nafa (who likely uses it for some very interesting decision theory applications). Briefly, if \\(T\\) is the event time, then let \\(Y=\\log(T)\\), and \\(Z = (Y-\\mu)/\\sigma\\). Then \\(Z\\) has the following density\n\\[ f(z ; k)=\\frac{k^{k-1 / 2}}{\\Gamma(k)} \\exp \\left(k^{1 / 2} z-k e^{k^{-1 / 2} z}\\right)\\]\nand \\(T = \\exp(Y)\\) is distributed according to the generalized gamma distribution . Here, \\(-\\infty \\lt z \\lt \\infty\\), \\(-\\infty \\lt \\mu \\lt \\infty\\), and \\(\\sigma, k&gt;0\\). For more on the generalized gamma, especially for use in survival analysis, see Statistical Models and Methods for Lifetime Data by Jerry Lawless (1982).\nThe nice thing about the generalized gamma is that the exponential, gamma, and weibull distributions – all common parmetric survival likelihoods – are special cases (and the log normal is a limiting distribution) 1.\nThat is especially nice for me. I’m working on some life time value modelling and it would be great if I didn’t have to try several models. Instead I can just use the generalized gamma and hope it fits well enough if the data are best approximated via one of the aforementioned survival functions.\nIn this post, I want to implement some survival analyses using the generalized gamma. Let’s get started."
  },
  {
    "objectID": "posts/2023-12-02-gen-gamma/index.html#data",
    "href": "posts/2023-12-02-gen-gamma/index.html#data",
    "title": "The Generalized Gamma Distribution For Parametric Survival",
    "section": "Data",
    "text": "Data\nWe’ll need some data. Rather than simulate it myself, I’ll use the veteran data from {survival}. The survival function is roughly exponential, which is good because we know the generalized gamma can fit it in principle. There is a trt indicator in these data, so we’ll fit one survival curve to each strata. Shown below are the Kaplan-Meir non-parametric estimates for these data. Rather than plot the survival curve \\(S(t)\\), I choose to plot \\(1-S(t)\\) because my brain groks the plot easier as “the proportion of individuals in a cohort who would have experienced the outcome by time \\(t\\)”. The log of this quantity is the cumulative hazard, but I don’t know if \\(1-S(t)\\) has a proper name. I mean … it is technically an estimate of the CDF of the event time distribution.\n\n\n\n\nNon-parametric estimate of \\(1 - S(t|trt)\\) for each trt strata in the veteran dataset. The curves look roughly exponential meaning the generalized gamma should provide a good fit.\n\n\n\n\n\nStan does not have an implementation for the generalized gamma, so we’ll have to write that ourselves in the functions block."
  },
  {
    "objectID": "posts/2023-12-02-gen-gamma/index.html#lpdf-_lcdf-and-_lccdf-implementations-in-stan",
    "href": "posts/2023-12-02-gen-gamma/index.html#lpdf-_lcdf-and-_lccdf-implementations-in-stan",
    "title": "The Generalized Gamma Distribution For Parametric Survival",
    "section": "_lpdf, _lcdf, and _lccdf Implementations in Stan",
    "text": "_lpdf, _lcdf, and _lccdf Implementations in Stan\nTo do parametric survival analysis in Stan, we need three functions:\n\nThe log probability density as generalized_gamma_lpdf so we can increment the log posterior density when we observe an outcome,\nThe log complementary CDF as generalized_gamma_lccdf so we can increment the log posterior density when we observe a censoring event, and\nThe log CDF as generalized_gamma_lcdf so we can implement the _lccdf.\n\nThe first and third functions are implemented already by Krzysztof Sakrejda in this repo. The _lpdf and _lcdf are given, so now we just need the complementary cdf function _lccdf. Since Stan works on the log probability scale, we need to return the the log of the complementary cdf. Since we have the log cdf we could just do\n\n\n\n\nreal generalized_gamma_lccdf(real x, real k, real mu, real sigma) {\n \n return log(1 - exp(generalized_gamma_lcdf(x | k, mu sigma)));\n \n}\n\n\nStan has a nicer function to do this called log_diff_exp which can be used to take differences in exponential space and then take the log of the result.\nAdditionally, we can create a random number generator for the generalized gamma distribution by noting that flexsurv’s implementation of the generalized gamma is equivalent to ours if we let \\(Q=1/\\sqrt{k}\\). Our functions block then looks like\n\n\nfunctions {\n  \n  real generalized_gamma_lpdf(real x, real k, real mu, real sigma) {\n    real w;\n    real d;\n    w = (log(x) - mu) / sigma;\n    d = (k - .5) * log(k) - log(sigma) - lgamma(k) + (sqrt(k) * w - k * exp(1 / sqrt(k) * w)) - log(x);\n    \n    return d;\n  }\n  \n  real generalized_gamma_cdf(real x, real k, real mu, real sigma) {\n    real w;\n    real d;\n    w = (log(x) - mu) / sigma;\n    d = gamma_p(k, k * exp(w / sqrt(k)));\n    \n    return d;\n  }\n  \n  real generalized_gamma_lcdf(real x, real k, real mu, real sigma) {\n    real w;\n    real d;\n    w = (log(x) - mu) / sigma;\n    d = log(gamma_p(k, k * exp(1 / sqrt(k) * w)));\n    return d;\n  }\n  \n  real generalized_gamma_lccdf(real x, real k, real mu, real sigma) {\n    return log_diff_exp(0, generalized_gamma_lcdf(x | k, mu, sigma));\n  }\n  \n  real generalized_gamma_rng(real k, real mu, real sigma) {\n    real Q = 1.0 / sqrt(k);\n    real gamma = gamma_rng(Q^-2, 1);\n    real w = log(Q^2 * gamma) / Q;\n    return exp(mu + sigma * w);\n  }"
  },
  {
    "objectID": "posts/2023-12-02-gen-gamma/index.html#fitting-the-model",
    "href": "posts/2023-12-02-gen-gamma/index.html#fitting-the-model",
    "title": "The Generalized Gamma Distribution For Parametric Survival",
    "section": "Fitting The Model",
    "text": "Fitting The Model\nThe model fits fairly quickly (4 chains in parallel takes &lt;2 seconds on my M1 Pro macbook pro). We can easily plot the survival curve against the Kaplan-Meir estimate to compare, however a better comparison would be to draw samples from the event time distribution and compute the ecdf of those samples. That plot is shown below, and is equivalent to a posterior predictive check in the case where there is no censoring. You can see that the KM estimates look similar to the ecdfs, which is good enough for me.\n\n\nRunning MCMC with 4 chains, at most 10 in parallel...\n\nChain 1 finished in 1.9 seconds.\nChain 2 finished in 1.9 seconds.\nChain 3 finished in 1.9 seconds.\nChain 4 finished in 1.9 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.9 seconds.\nTotal execution time: 2.0 seconds.\n\n\n\n\n\n\nKaplan-Meir estimates (black) with posterior survival functions.\n\n\n\n\n\n\n\n\n\nKaplan-Meir estimates (black) with estimated ECDFs computed from draws of the posterior distribution. Each colored line corresponds to one sample of event times from the posterior distribution, conditional on \\(\\mu\\), \\(\\sigma\\), and \\(k\\)."
  },
  {
    "objectID": "posts/2023-12-02-gen-gamma/index.html#footnotes",
    "href": "posts/2023-12-02-gen-gamma/index.html#footnotes",
    "title": "The Generalized Gamma Distribution For Parametric Survival",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLeft as an exercise to the reader.↩︎"
  },
  {
    "objectID": "posts/2023-03-31-optimal-mde/index.html",
    "href": "posts/2023-03-31-optimal-mde/index.html",
    "title": "Choosing the Optimal MDE for Experimentation",
    "section": "",
    "text": "Planning experiments means deciding how long to run the experiment (by doing a sample size calculation). A big factor in this decision is agreeing on a minimal detectable effect or MDE. Smaller MDEs mean longer experiments and hence fewer of them in a given window of time. Larger MDEs mean shorter experiments, but they also mean there is a chance we could fail to reject smaller effects 1. Clearly, there is a sweet spot for the MDE; not so large that we are passing over interventions which would improve a given metric, but not so small that we are wasting our time collecting samples.\nThis blog post is intended to demonstrate how data scientists can empirically estimate the “optimal” MDE for experimenting teams. Here, “optimal” refers to the MDE which optimizes the long run cumulative improvement to the metric (i.e. the MDE which is estimated to improve the metric the most in a given window of time) under some mild and some strong assumptions. I begin by assuming teams have a model of likely effect sizes for their interventions vis a vis a Bayesian model like I have done in my past post Forecasting Experimental Lift Using Hierarchical Bayesian Modelling.\nI begin with an illustration of the larger idea, at a level I hope would be appropriate for a product manager. Then, I formally describe the procedure and implement it in R, making some illustrative assumptions about a hypothetical team running experiments. Lastly, I demonstrate how various assumptions can effect the estimate of the optimal MDE and estimate a linear model for the optimal MDE as a function of the assumptions.\nAlthough imperfect, I believe this approach offers a superior approach to determining the MDE for experimenting teams, and also has the benefit of being re-estimable experiment over experiment thanks to the Bayesian modelling driving the approach."
  },
  {
    "objectID": "posts/2023-03-31-optimal-mde/index.html#introduction",
    "href": "posts/2023-03-31-optimal-mde/index.html#introduction",
    "title": "Choosing the Optimal MDE for Experimentation",
    "section": "",
    "text": "Planning experiments means deciding how long to run the experiment (by doing a sample size calculation). A big factor in this decision is agreeing on a minimal detectable effect or MDE. Smaller MDEs mean longer experiments and hence fewer of them in a given window of time. Larger MDEs mean shorter experiments, but they also mean there is a chance we could fail to reject smaller effects 1. Clearly, there is a sweet spot for the MDE; not so large that we are passing over interventions which would improve a given metric, but not so small that we are wasting our time collecting samples.\nThis blog post is intended to demonstrate how data scientists can empirically estimate the “optimal” MDE for experimenting teams. Here, “optimal” refers to the MDE which optimizes the long run cumulative improvement to the metric (i.e. the MDE which is estimated to improve the metric the most in a given window of time) under some mild and some strong assumptions. I begin by assuming teams have a model of likely effect sizes for their interventions vis a vis a Bayesian model like I have done in my past post Forecasting Experimental Lift Using Hierarchical Bayesian Modelling.\nI begin with an illustration of the larger idea, at a level I hope would be appropriate for a product manager. Then, I formally describe the procedure and implement it in R, making some illustrative assumptions about a hypothetical team running experiments. Lastly, I demonstrate how various assumptions can effect the estimate of the optimal MDE and estimate a linear model for the optimal MDE as a function of the assumptions.\nAlthough imperfect, I believe this approach offers a superior approach to determining the MDE for experimenting teams, and also has the benefit of being re-estimable experiment over experiment thanks to the Bayesian modelling driving the approach."
  },
  {
    "objectID": "posts/2023-03-31-optimal-mde/index.html#big-idea",
    "href": "posts/2023-03-31-optimal-mde/index.html#big-idea",
    "title": "Choosing the Optimal MDE for Experimentation",
    "section": "Big Idea",
    "text": "Big Idea\nLet’s consider how many experiments a team can run in a year. Teams usually have some upper limit for the number of experiments because they need time to tend to their other responsibilities. Let’s assume a team can run 24 experiments in a year (~2 experiments a month on average). While the team has a maximum number of experiments they can run, the number of experiments they actually run will depend on how the experiments are planned. For example, if the team has on average 10,000,000 unique visitors each year, and each experiment needs 1, 500, 000 users, then the team can run 6 experiments (you can’t run half an experiment so you have to round down to the nearest number). So the number of experiments that can be run for this team is the smaller of the ratio of unique visitors per year to total sample size per experiment and 24. The key insight here is that because the MDE determines the sample size per experiment (again, larger/smaller MDEs mean smaller/larger sample sizes), then the MDE implicitly determines the number of experiments we can.\nEach change to the product has some true effect that is unknown to us. In fact, the whole reason we run an experiment is to estimate that effect. Because the MDE determines the sample size per experiment, it also determines the probability we detect the effect. Increase the MDE, and the probability we detect a given effect decreases (because the sample size for the experiment decreases). Conversely, decreasing the MDE increases the probability we detect an effect.\nWe need to choose an MDE so that we can run lots of experiments and have a good chance of detecting positive effects in those experiments. The problem is that we don’t know what kinds of effects our experiments are going to have, which is why we use an MDE. The MDE basically is a stand in for what we think the effect of the change is going to be at its smallest. If the effect of the change is bigger than the MDE, then we have a really good chance to detect the effect.\nHowever, we actually can estimate what kinds of effects our changes will produce. It isn’t worth getting into, but we can estimate a distribution of likely effect sizes, meaning we can reasonably guess what kinds of effects we are going to see in future experiments. This means that we can use this distribution of plausible future effects to simulate future experiments. These simulations can then be used to determine the MDE which strikes the balance we need. A good way to determine which MDE is best is to consider the “cumulative impact” on the metric. Think of it this way; if we run lots of experiments and they all have a reasonable chance of detecting effects then the metric we’re seeking to improve is going to change in a big way. So our goal is to find the MDE which results in the largest improvement to our metric over a given window of time in which we can experiment. This MDE which results in the largest improvement is called the “optimal MDE”.\nLet’s formalize the optimization problem using some math."
  },
  {
    "objectID": "posts/2023-03-31-optimal-mde/index.html#mathematical-details",
    "href": "posts/2023-03-31-optimal-mde/index.html#mathematical-details",
    "title": "Choosing the Optimal MDE for Experimentation",
    "section": "Mathematical Details",
    "text": "Mathematical Details\nLet \\(N_{uuid}\\) be the number of unique subjects available for experimentation in a given time frame, let \\(\\delta \\in \\mathbb{R}_+\\) be an arbitrary MDE, and let \\(K\\) be the maximum number of experiments for a given team. Let \\(n_{ss}: \\mathbb{R}_+ \\to \\mathbb{N}\\) be a function which maps MDEs to sample sizes for experiments. The number of experiments which can be run in a given time frame is \\(n_{\\exp}(\\delta) = \\min\\left(K,  \\lfloor\\frac{N_{uuid}}{n_{ss}(\\delta)} \\rfloor \\right)\\). Here, I have included an explicit dependency of \\(n_{\\exp}\\) on \\(\\delta\\) to remind us that the MDE implicitly determines the number of experiments. I’ve made the assumption that experiments can be run back to back.\nLet \\(\\theta_k\\) be a latent effect from an intervention, and let \\(\\psi(\\theta_k; \\delta)\\) be the statistical power to detect an effect of \\(\\theta\\) when the experiment is designed with an MDE of \\(\\delta\\). I assume that: a) All interventions are independent of one another, and the effect of one intervention is not changed by the implementation of another, b) effects of interventions are additive (on the appropriate scale), and c) effects persist through time (there is now decay of an effect once implemented).\nThe objective function we seek to optimize is the expected cumulative improvement to the metric we are opting to experiment on. We get \\(n_{\\exp}\\) draws from our population distribution of effects (because we are running that many experiments), and the expected cumulative improvement is the sum of the products of the effects and the probability we detect the effect\n\\[ C(\\delta) = \\sum_{k=1}^{n_{\\exp}(\\delta)} \\theta_k \\psi(\\theta_k; \\delta)  \\] Note here that the MDE \\(\\delta\\) determines both the number of experiments run (\\(n_{\\exp}\\)) and the probability those experiments detect an effect. The optimal MDE is then\n\\[ \\delta_{\\mbox{opt}} = \\underset{\\delta \\in \\mathbb{R}_+}{\\arg\\max}  \\Big\\{ C(\\delta) \\Big \\}\\] While the \\(\\theta\\) are latent, this quantity can still be optimized by using draws from a Bayesian model for experimental effects. See this previous post of mine for an example of what I mean.\nIn the next section, I demonstrate how to estimate \\(\\delta_{\\mbox{opt}}\\) using simulation. To fill in some missing information (e.g. the maximum number of experiments run by the team), I explicitly write out some assumptions."
  },
  {
    "objectID": "posts/2023-03-31-optimal-mde/index.html#additional-assumptions",
    "href": "posts/2023-03-31-optimal-mde/index.html#additional-assumptions",
    "title": "Choosing the Optimal MDE for Experimentation",
    "section": "Additional Assumptions",
    "text": "Additional Assumptions\nImagine a team who runs experiments. I make the following assumptions about the team:\n\nThe team’s entire job is running experiments. Due to resourcing constraints, they can only run a finite number experiments per year. I assume the team can run 24 experiments a year (or 2 per month on average). The team can run experiments back to back.\nThe team works in a frequentist framework, and they always run 2 tailed tests because there is a chance they could hurt the product, and they would want to know that.\nThe main causal contrast is relative risk. In industry, we call this the “lift”.\nThe outcome is a binary outcome, and the baseline rate is 8%.\n10,000,000 unique visitors to your website per year.\nThe team generates lift fairly reliably and these lifts sustain through time. There is no decay of the effect, no interaction between experiments, nor is there any seasonality. These are blatantly false, but they simplify enough for us to get traction.\nThe population level lift distribution is log normal, with parameters \\(\\mu=\\log(1.01)\\) and \\(\\sigma=0.1\\) on the log scale. This means the team increases the metric by approximately 1% on average.\nThe team is really only interested in positive effects (lift &gt; 1) so they will not implement anything with lift &lt; 1, and if the null fails to be rejected they will stick with the status quo.\nThe same MDE is used to plan all experiments.\n\nUnder these assumptions, a procedure can be devised to optimize the cumulative improvement to the metric of interest."
  },
  {
    "objectID": "posts/2023-03-31-optimal-mde/index.html#results-from-a-simulation-experiment",
    "href": "posts/2023-03-31-optimal-mde/index.html#results-from-a-simulation-experiment",
    "title": "Choosing the Optimal MDE for Experimentation",
    "section": "Results from a Simulation Experiment",
    "text": "Results from a Simulation Experiment\nShown in the code cell below is simulation of the process for finding the optimal MDE under the assumptions listed above. Rather than simulate every experiment (e.g. by drawing random numbers and performing a statistical test), we can draw a binomial random variable with probability of success equal to the statistical power of detecting the latent lift with the indicated MDE and hence sample size.\nThe optimal lift is somewhere between 5% and 6%. Explicit optimization methods could be used find the optima, but I think for the purposes of experimentation you just want to be in the right ballpark, so a plot is more than enough.\n\n\nCode\none_sided_power = function(real_lift, n_per_group){\n  # Only interested in the case when the estimated lift is\n  # Greater than one, which corresponds to a one sided test.\n  # However, you always run 2 tailed tests, so the significance level\n  # is half of what is typically is.\n  pwr.2p.test(h = ES.h(real_lift*baseline, baseline), \n              n = n_per_group,\n              alternative = 'greater',\n              sig.level = 0.025\n              )$power\n}\n\nf = function(mde, baseline=0.08, n_uuids=2500000, latent_lift = 1.01){\n  \n  # Draw lifts for experiments from this distribution\n  lift_dist &lt;-\\(n) rlnorm(n, log(latent_lift), 0.1)\n  \n  # Given the MDE, here is how many users you need per group in each experiment.\n  n_per_group = ceiling(pwr.2p.test(h = ES.h(mde*baseline, baseline), power = 0.8)$n)\n  \n  # Here is how many experiments you could run per year\n  # Why the factor of 2?  Because the computation above is the szie of each group.\n  n_experiments_per_year &lt;- pmin(24, floor(n_uuids/(2*n_per_group)))\n  \n  # Here is a grid of experiments.  Simulate \n  # Running these experiments 1000 times\n  # each experiment has n_per_group users in each group\n  simulations &lt;- crossing(\n    sim = 1:4000, \n    experiment = 1:n_experiments_per_year,\n    n_per_group = n_per_group\n  )\n  \n  simulations %&gt;% \n    mutate(\n      # draw a real lift for each experiment from your lift distribution\n      real_lift = lift_dist(n()),\n      # Compute the power to detect that lift given the sample size you have\n      actual_power = map2_dbl(real_lift, n_per_group, one_sided_power),\n      # Simulate detecting the lift\n      detect = as.logical(rbinom(n(), 1, actual_power)),\n      # Did you implement the result or not?\n      # If you didn't, this is equivalent to a lift of 1\n      # and won't change the product.\n      result = if_else(detect, real_lift, 1),\n    ) %&gt;% \n    group_by(sim) %&gt;% \n    #finally, take the product, grouping among simulations.\n    summarise(lift = prod(result)) \n  \n}\n\n\nmdes &lt;- tibble(mde = seq(1.01, 1.2, 0.01)) %&gt;% \n        mutate(mde_id = as.character(seq_along(mde)))\n\nresults = map_dfr(mdes$mde, f, .id = 'mde_id')  %&gt;% \n          left_join(mdes)\n\n\nresults %&gt;% \nggplot(aes(mde, lift)) + \n  stat_summary(fun.data = \\(x) mean_se(x, 2)) + \n  scale_x_continuous(labels = \\(x) scales::percent(x-1, 0.01)) + \n  scale_y_continuous(labels = \\(x) scales::percent(x-1, 0.01)) +\n  labs(x='MDE', y='Cumulative Improvement Over all Experiments',\n       title = 'Swing for the Fences',\n       subtitle = 'The optimal MDE is not the expected lift the team generates')"
  },
  {
    "objectID": "posts/2023-03-31-optimal-mde/index.html#how-do-the-various-parts-of-the-problem-change-the-objective-function",
    "href": "posts/2023-03-31-optimal-mde/index.html#how-do-the-various-parts-of-the-problem-change-the-objective-function",
    "title": "Choosing the Optimal MDE for Experimentation",
    "section": "How Do The Various Parts of The Problem Change The Objective Function?",
    "text": "How Do The Various Parts of The Problem Change The Objective Function?\nChanging the baseline of the metric moves the optima, with larger MDEs being considered optimal for smaller baselines.\n\n\n\n\n\n\n\n\n\nAs the number of unique visitors to the website increases, the optimal MDE decreases, but only slightly.\n\n\n\n\n\n\n\n\n\nAs the expectation of the latent lift increases, the optima does not move but the expected cumulative improvement to the metric increases. This is unsurprising."
  },
  {
    "objectID": "posts/2023-03-31-optimal-mde/index.html#footnotes",
    "href": "posts/2023-03-31-optimal-mde/index.html#footnotes",
    "title": "Choosing the Optimal MDE for Experimentation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “M” in MDE is really misleading. A better term would be Smallest Effect Size of Interest, because we can detect smaller effects than the MDE, albeit with lower probability.↩︎"
  },
  {
    "objectID": "posts/2022-07-20-pooling-experiments/index.html",
    "href": "posts/2022-07-20-pooling-experiments/index.html",
    "title": "Forecasting Experimental Lift Using Hierarchical Bayesian Modelling",
    "section": "",
    "text": "You’re part of a team at a company who is tasked with improving conversion on some web page. You’ve run a few experiments already with mixed results and now it is time to set some goals for the next year. Here is a question:\nMaybe your approach for your end of year targets would look like\n\\[ \\Big( \\mbox{Average Lift}\\Big)^{\\mbox{Number of Planned Experiments}} \\]\nIts a good back-of-the-napkin approach to the problem. But if you come up short is that neccesarily a failure? Or, could it be well within expectation?\nThis post is forecasting how much a given team can move a metric within some time frame. You’re going to forecast the lift the team can generate given some of their past performance. The forecasting is Bayesian, but assumes the team works within a frequentist framework."
  },
  {
    "objectID": "posts/2022-07-20-pooling-experiments/index.html#assumptions",
    "href": "posts/2022-07-20-pooling-experiments/index.html#assumptions",
    "title": "Forecasting Experimental Lift Using Hierarchical Bayesian Modelling",
    "section": "Assumptions",
    "text": "Assumptions\nYour team can run approximately 1 experiment per month or 12 in a calendar year (but the method we develop can be extended to an arbitrary number of experiments per month). Let’s say you start experimenting on January 1 and will evaluate your performance December 31. In addition to this, assume:\n\nAll your experiments are A/B tests with two and only two groups: test and control.\nYour main metric is a conversion rate and the baseline value is 1%.\nEvery intervention has an effect, though it may be small. The null is never true.\nYour site sees 100,000 unique users per month. You split all 100,000 into two groups at random, and\nYou measure lift in a relative sense (this is sometimes called relative risk in epidemiology).\n\nLet’s make some additional assumptions about experiments:\n\nYour team is relatively reliable. They don’t get better at thinking up interventions over time, so the effects they generate do not change over time, except for random variation.\nExperiments effects are independent of one another, so the implementation of one change does not alter the effect of the next experiment."
  },
  {
    "objectID": "posts/2022-07-20-pooling-experiments/index.html#scenario",
    "href": "posts/2022-07-20-pooling-experiments/index.html#scenario",
    "title": "Forecasting Experimental Lift Using Hierarchical Bayesian Modelling",
    "section": "Scenario",
    "text": "Scenario\nShown in the table below are your results over the last year. Nice job, lots of wins, a few failures to reject the null, but overall very good. Using the estimated relative lifts where you did , you managed to increase conversion by 80%. Now, you’re PM is asking you to shoot for 2x conversion this year.\nIs that reasonable1? How probable are you to generate at least 2x lift over 12 months given your past performance? I mean, it’s only a little better than you did this past year, right? Luckily, you’re a good data scientist. Even though your team uses frequentism to evaluate their A/B tests, you are not beholden to one ideology over another. So, you decide to use a hierarchical Bayesian model to estimate what kinds of lifts your team is likely to generate in the future.\n\n\n\n\n\nN\nTreatment Conversions\nControl Conversions\nRelative Lift\np\n\n\n\n\n50,000\n541\n496\n1.09\n0.08\n\n\n50,000\n557\n524\n1.06\n0.16\n\n\n50,000\n559\n486\n1.15\n0.01\n\n\n50,000\n556\n500\n1.11\n0.04\n\n\n50,000\n530\n516\n1.03\n0.34\n\n\n50,000\n532\n475\n1.12\n0.04\n\n\n50,000\n516\n507\n1.02\n0.40\n\n\n50,000\n532\n475\n1.12\n0.04\n\n\n50,000\n528\n490\n1.08\n0.12\n\n\n50,000\n544\n506\n1.08\n0.13\n\n\n50,000\n519\n512\n1.01\n0.43\n\n\n50,000\n552\n489\n1.13\n0.03"
  },
  {
    "objectID": "posts/2022-07-20-pooling-experiments/index.html#hierarchical-model",
    "href": "posts/2022-07-20-pooling-experiments/index.html#hierarchical-model",
    "title": "Forecasting Experimental Lift Using Hierarchical Bayesian Modelling",
    "section": "Hierarchical Model",
    "text": "Hierarchical Model\nLet \\(\\widehat{RR}_i\\) be the estimated relative lift2 from experiment \\(i\\). The sampling distribution of relative lift is asymptotically normally distributed on the log scale. Assuming we know the standard error exactly (using the delta rule), this means\n\\[ \\log \\Big(\\widehat{RR}_i \\Big) \\sim \\mathcal{N}(\\log(\\theta_i), \\sigma)\\]\nHere, \\(\\log(\\theta_i)\\) is the relative lift on the log scale for experiment \\(i\\) (whereas \\(\\widehat{RR}_i\\) is just the estimated relative lift). We can model the \\(\\theta\\) hierarchically as\n\\[ \\log(\\theta_i) \\sim \\mathcal{N}(\\mu, \\tau) \\]\nNow, you just need to place priors on \\(\\mu\\) and \\(\\tau\\) (assume you used good priors)."
  },
  {
    "objectID": "posts/2022-07-20-pooling-experiments/index.html#forcasting-lift",
    "href": "posts/2022-07-20-pooling-experiments/index.html#forcasting-lift",
    "title": "Forecasting Experimental Lift Using Hierarchical Bayesian Modelling",
    "section": "Forcasting Lift",
    "text": "Forcasting Lift\nOnce you fit your model, you can generate hypothetical relative lifts by sampling from the model. Let \\(\\psi\\) be a relative lift, so that\n\\[ \\log(\\psi) \\sim \\mathcal{N}(\\mu, \\sigma) \\&gt;. \\]\nIf your team were to implement an experiment for which had a relative lift of \\(\\psi\\), you would get an estimated relative lift. Depending on the size of that estimate, you may or may not reject the null hypothesis. The probability you reject the null hypothesis is when it is false (and it is always false by assumption) is known as the statistical power. Since you have a fixed sample size in each experiment, and every experiment is a 50/50 split, you can calculate the statistical power that you detect a relative lift of \\(\\psi\\). Call that \\(p_{\\psi}\\).\nNow for the fun part. Say you run \\(n\\) experiments per month for \\(K\\) months. The lift you generate in month \\(k\\), \\(LG_k\\), would be\n\\[ LG_k = \\exp\\Bigg( \\sum_{j=1}^n \\log(\\psi_j) p_{\\psi, j} \\Bigg) \\]\nand the forecasted lift, \\(FL\\), up to and including month \\(k\\) is\n\\[ FL_k = \\prod_{i=1}^{k} LG_i \\]\nThink this through. If you were to implement every intervention, your lift would simply be \\(\\prod_{j=1}^k \\psi_j\\), or on the log scale \\(\\sum_j \\log(\\psi_j)\\). But you don’t detect every effect. The probability you detect the effect of the \\(j^{th}\\) intervention is \\(p_{\\psi, j}\\). So \\(\\sum_j \\log(\\psi_j) p_{\\psi, j}\\) is the expected lift you would accrue over the \\(k\\) experiments. Take the exponential to convert this sum back to a product and you’ve got a generated lift after \\(n\\) experiments in a given month. Multiply the lift month over month to get a forecasted lift. Now, because there is uncertainty in the \\(\\psi\\), there is uncertainty in the forecasted lift. However, your hierarchical model will make it more or less easy to integrate over that uncertainty. Just sample from the model and average over the samples."
  },
  {
    "objectID": "posts/2022-07-20-pooling-experiments/index.html#modelling",
    "href": "posts/2022-07-20-pooling-experiments/index.html#modelling",
    "title": "Forecasting Experimental Lift Using Hierarchical Bayesian Modelling",
    "section": "Modelling",
    "text": "Modelling\nLuckily, all of the computation above – even the power calculation – can be done inside Stan (and you’re pretty good at writing Stan code3).\nShown in Figure 1 is the forecasted lift as compared to baseline after the \\(k^{th}\\) month Good job, if you keep doing things as you’re doing, you’re going to probably increase conversion rate by a little more than 50% (a little less than the 80% but still nothing to sneeze at). The shaded blue regions indicate the uncertainty in that estimate. Note that although your forecasted lift seems to always be increasing, that isn’t necessarily the case. You could implement a change which hurts our conversion because of chance, so if you were to plot simulated trajectories you might see some decreases in the metric.\n\n\n\n\n\nFigure 1: Forecasted lift after the 12 months. Shown in blue are credible interval estimates. The conditional distirbution is log-normal since the forecasted lift is the sum of normals on the log scale.\n\n\n\n\n\n\n\n\n\n\nClick to see individual trajectories\n\n\n\n\n\n\nThe red lines are draws where you would have implemented a change to hurt the conversion rate. See how sometimes those lines actually decrease? Such is life, can’t win em all!\n\n\n\n\n\n\n\nNow, what about that goal of increasing conversion by 2x? Well, it isn’t looking good. Looks like there is only a 12% chance you meet or exceed the 2x goal. Could it be your performance last year was just extreme? The distribution of forecasted lifts is long tailed. Maybe you’re just going to regress to the mean. Sounds like a good time to push back on your boss and come prepared with data.\n\n\n\n\n\nFigure 2: Conditional posterior distirbution of forecasted lifts after completing the 12 experiments you had planned this year. Indicated point/interval is mean and 95% credible interval."
  },
  {
    "objectID": "posts/2022-07-20-pooling-experiments/index.html#conclusion",
    "href": "posts/2022-07-20-pooling-experiments/index.html#conclusion",
    "title": "Forecasting Experimental Lift Using Hierarchical Bayesian Modelling",
    "section": "Conclusion",
    "text": "Conclusion\nYou’re clever and realized you could use a hierarchical model to simulate future experiment results and use those to forecast your team’s performance. Your boss’ goal of a 2x increase is nice in so far as it shows they have confidence in you and your team, but the model says it isn’t super achievable.\nIf 2x isn’t achievable, what is a better target? Or maybe, what is a better range of targets. I’m not sure, that isn’t the point of the post. The post was to equip you with a means of answering that question yourself, and I know you’re capable of answering it. I mean…look at all this cool modelling you did."
  },
  {
    "objectID": "posts/2022-07-20-pooling-experiments/index.html#post-script",
    "href": "posts/2022-07-20-pooling-experiments/index.html#post-script",
    "title": "Forecasting Experimental Lift Using Hierarchical Bayesian Modelling",
    "section": "Post Script",
    "text": "Post Script\nOk, breaking away from the narrative for a moment…this is a continuous approximation to a discrete process. We should simulate this to see how real experiments would stack up against my forecast. I’ve gone ahead and actually simulated running the 12 tests and computed the lift after the 12 tests. Shown below is the forecasted lift versus relative error as compared to simulation. I’ll let you come to your own conclusion about the quality of the approximation."
  },
  {
    "objectID": "posts/2022-07-20-pooling-experiments/index.html#footnotes",
    "href": "posts/2022-07-20-pooling-experiments/index.html#footnotes",
    "title": "Forecasting Experimental Lift Using Hierarchical Bayesian Modelling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nJust go with it, its a work of fiction, who knows if it is reasonable. Let’s pull out the forecasted lifts after the final experiment.↩︎\n\\(RR\\) for relative risk, sorry my epidemiology is showing↩︎\nSpeaking of Stan code, the Stan file in the github repo for this post (see the “Edit this page on github” on the right hand side).↩︎"
  },
  {
    "objectID": "posts/2017-12-29-coins/index.html",
    "href": "posts/2017-12-29-coins/index.html",
    "title": "Coins and Factors",
    "section": "",
    "text": "I love Fivethirtyeight’s Riddler column. Usually, I can solve the problem with computation, but on some rare occasions I can do some interesting math to get the solution without having to code. Here is the first puzzle I ever solved. It is a simple puzzle, yet it has an elegant computational and analytic solution. Let’s take a look.\nThe puzzle says:"
  },
  {
    "objectID": "posts/2017-12-29-coins/index.html#computing-the-solution",
    "href": "posts/2017-12-29-coins/index.html#computing-the-solution",
    "title": "Coins and Factors",
    "section": "Computing the Solution",
    "text": "Computing the Solution\nThis is really easy to program. Here is a little python script to compute the solution:\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom itertools import product\n\n\n# Array of 100 True.  True is Heads up\nNcoins = 100\ncoins = np.ones(Ncoins,dtype = bool)\nindex = np.arange(1,Ncoins+1)\n\n#Go through the coins\nfor N in range(1,Ncoins+1):\n    \n    coins[index%N==0] = ~coins[index%N==0]  #Flip the coin.  \nShown below is the solution. In dark blue are the coins face down (I’ve arranged them in a 10 by 10 grid and annotated them with their position for clarity). When we take a look at the coins variable, we see that those coins in positions which are perfect squares pop out. That is an interesting result, but what is more interesting is reasoning out the solution without doing any computation at all!\n\n\n\n\n\n\n\n\nFigure 1: Coins after flipping. Dark squares are face down coins."
  },
  {
    "objectID": "posts/2017-12-29-coins/index.html#reasoning-out-the-solution.",
    "href": "posts/2017-12-29-coins/index.html#reasoning-out-the-solution.",
    "title": "Coins and Factors",
    "section": "Reasoning Out the Solution.",
    "text": "Reasoning Out the Solution.\nFirst, let’s think why a coin would end up face down. If all coins start heads up, then it would take an odd number of flips for the coin to end up face down. Since coins are only flipped when we pass a factor of a coin’s position, then those coins in positions with an odd number of factors will be heads down at the end.\nSo 9 would end up heads down because it has factors 1 (flip face down), 3 (flip face up), and 9 (flip face down), while 6 would be heads up because it has factors 1, 2, 3, and 6.\nSo which numbers have an odd number of factors? Here is where we get to do some interesting math. The Fundamental Theorem of Arithmetic says that every integer \\(N&gt;1\\) is either prime or can be uniquely factored as a product of primes\n\\[N = \\prod_{j} p_j^{a_j} \\&gt;.\\]\nIf \\(N\\) can be factored like this, that means it has\n\\[\\prod_{j} (a_j +1)\\]\nunique factors.\nIt is straight forward to argue that a composite odd number must be the product of odd numbers, so we know that the \\(a_j+1\\) must be odd \\(\\forall j\\), and so that means the \\(a_j\\) are even and can be written as \\(a_j = 2n_j\\). Thus, our factorization becomes\n\\[N = \\prod_j p_j^{2n_j} = \\prod_j (p_j^{n_j})^2 = \\left(\\prod_j p_j^{n_j} \\right)^2 \\&gt;,\\]\nwhich means that if \\(N\\) has an odd number of factors, it must be a perfect square! All done.\nI love trying to solve the Riddler’s puzzle without coding. It makes me draw upon knowledge I haven’t used in a while, and may force me to learn something new."
  },
  {
    "objectID": "posts/2019-05-21-odes/index.html",
    "href": "posts/2019-05-21-odes/index.html",
    "title": "Gradient Descent with ODEs",
    "section": "",
    "text": "Gradient descent usually isn’t used to fit Ordinary Differential Equations (ODEs) to data (at least, that isn’t how the Applied Mathematics departments to which I have been a part have done it). Nevertheless, that doesn’t mean that it can’t be done. For some of my recent GSoC work, I’ve been investigating how to compute gradients of solutions to ODEs without access to the solution’s analytical form. In this blog post, I describe how these gradients can be computed and how they can be used to fit ODEs to synchronous data with gradient descent."
  },
  {
    "objectID": "posts/2019-05-21-odes/index.html#up-to-speed-with-odes",
    "href": "posts/2019-05-21-odes/index.html#up-to-speed-with-odes",
    "title": "Gradient Descent with ODEs",
    "section": "Up To Speed With ODEs",
    "text": "Up To Speed With ODEs\nI realize not everyone might have studied ODEs. Here is everything you need to know:\nA differential equation relates an unknown function \\(y \\in \\mathbb{R}^n\\) to it’s own derivative through a function \\(f: \\mathbb{R}^n \\times \\mathbb{R} \\times \\mathbb{R}^m \\rightarrow  \\mathbb{R}^n\\), which also depends on time \\(t \\in \\mathbb{R}\\) and possibly a set of parameters \\(\\theta \\in \\mathbb{R}^m\\). We usually write ODEs as\n\\[y' = f(y,t,\\theta) \\quad y(t_0) = y_0\\]\nHere, we refer to the vector \\(y\\) as “the system”, since the ODE above really defines a system of equations. The problem is usually equipped with an initial state of the system \\(y(t_0) = y_0\\) from which the system evolves forward in \\(t\\). Solutions to ODEs in analytic form are often very hard if not impossible, so most of the time we just numerically approximate the solution. It doesn’t matter how this is done because numerical integration is not the point of this post. If you’re interested, look up the class of Runge-Kutta methods."
  },
  {
    "objectID": "posts/2019-05-21-odes/index.html#computing-gradients-for-odes",
    "href": "posts/2019-05-21-odes/index.html#computing-gradients-for-odes",
    "title": "Gradient Descent with ODEs",
    "section": "Computing Gradients for ODEs",
    "text": "Computing Gradients for ODEs\nIn this section, I’m going to be using derivative notation rather than \\(\\nabla\\) for gradients. I think it is less ambiguous.\nIf we want to fit an ODE model to data by minimizing some loss function \\(\\mathcal{L}\\), then gradient descent looks like\n\\[ \\theta_{n+1} = \\theta_n - \\alpha \\dfrac{\\partial \\mathcal{L}}{\\partial \\theta} \\]\nIn order to compute the gradient of the loss, we need the gradient of the solution, \\(y\\), with respect to \\(\\theta\\). The gradient of the solution is the hard part here because it can not be computed (a) analytically (because analytic solutions are hard AF), or (b) through automatic differentiation without differentiating through the numerical integration of our ODE (which seems computationally wasteful).\nThankfully, years of research into ODEs yields a way to do this (that is not the adjoint method. Surprise! You thought I was going to say the adjoint method didn’t you?). Forward mode sensitivity analysis calculates gradients by extending the ODE system to include the following equations:\n\\[ \\dfrac{d}{dt}\\left( \\dfrac{\\partial y}{\\partial \\theta} \\right) = \\mathcal{J}_f \\dfrac{\\partial y}{\\partial \\theta} +\\dfrac{\\partial f}{\\partial \\theta} \\]\nHere, \\(\\mathcal{J}\\) is the Jacobian of \\(f\\) with respect to \\(y\\). The forward sensitivity analysis is just another differential equation (see how it relates the derivative of the unknown \\(\\partial y / \\partial \\theta\\) to itself?)! In order to compute the gradient of \\(y\\) with respect to \\(\\theta\\) at time \\(t_i\\), we compute\n\\[ \\dfrac{\\partial y}{\\partial \\theta} = \\int_{t_0}^{t_i} \\mathcal{J}_f \\dfrac{\\partial y}{\\partial \\theta} + \\dfrac{\\partial f}{\\partial \\theta} \\, dt \\]\nI know this looks scary, but since forward mode sensitivities are just ODEs, we actually just get this from what we can consider to be a black box\n\\[\\dfrac{\\partial y}{\\partial \\theta} = \\operatorname{BlackBox}(f(y,t,\\theta), t_0, y_0, \\theta)\\]\nSo now that we have our gradient in hand, we can use the chain rule to write\n\\[\\dfrac{\\partial \\mathcal{L}}{\\partial \\theta} =\\dfrac{\\partial \\mathcal{L}}{\\partial y} \\dfrac{\\partial y}{\\partial \\theta} \\]\nWe can use automatic differentiation to compute \\(\\dfrac{\\partial \\mathcal{L}}{\\partial y}\\).\nOK, so that is some math (interesting to me, maybe not so much to you). Let’s actually implement this in python."
  },
  {
    "objectID": "posts/2019-05-21-odes/index.html#gradient-descent-for-the-sir-model",
    "href": "posts/2019-05-21-odes/index.html#gradient-descent-for-the-sir-model",
    "title": "Gradient Descent with ODEs",
    "section": "Gradient Descent for the SIR Model",
    "text": "Gradient Descent for the SIR Model\nThe SIR model is a set of differential equations which govern how a disease spreads through a homogeneously mixed closed populations. I could write an entire thesis on this model and its various extensions (in fact, I have), so I’ll let you read about those on your free time.\nThe system, shown below, is parameterized by a single parameter:\n\\[ \\dfrac{dS}{dt} = -\\theta SI \\quad S(0) = 0.99 \\]\n\\[ \\dfrac{dI}{dt} = \\theta SI - I \\quad I(0) = 0.01 \\]\nLet’s define the system, the appropriate derivatives, generate some observations and fit \\(\\theta\\) using gradient descent. Here si what you’ll need to get started:\nimport autograd\nfrom autograd.builtins import tuple\nimport autograd.numpy as np\n\n#Import ode solver and rename as BlackBox for consistency with blog\nfrom scipy.integrate import odeint as BlackBox\nimport matplotlib.pyplot as plt\nLet’s then define the ODE system\ndef f(y,t,theta):\n    '''Function describing dynamics of the system'''\n    S,I = y\n    ds = -theta*S*I\n    di = theta*S*I - I\n\n    return np.array([ds,di])\nand take appropriate derivatives\n#Jacobian wrt y\nJ = autograd.jacobian(f,argnum=0)\n#Gradient wrt theta\ngrad_f_theta = autograd.jacobian(f,argnum=2)\nNext, we’ll define the augmented system (that is, the ODE plus the sensitivities).\ndef ODESYS(Y,t,theta):\n\n    #Y will be length 4.\n    #Y[0], Y[1] are the ODEs\n    #Y[2], Y[3] are the sensitivities\n\n    #ODE\n    dy_dt = f(Y[0:2],t,theta)\n    #Sensitivities\n    grad_y_theta = J(Y[:2],t,theta)@Y[-2::] + grad_f_theta(Y[:2],t,theta)\n\n    return np.concatenate([dy_dt,grad_y_theta])\nWe’ll optimize the \\(L_2\\) norm of the error\ndef Cost(y_obs):\n    def cost(Y):\n        '''Squared Error Loss'''\n        n = y_obs.shape[0]\n        err = np.linalg.norm(y_obs - Y, 2, axis = 1)\n\n        return np.sum(err)/n\n\n    return cost\nCreate some observations from which to fit\n\nnp.random.seed(19920908)\n## Generate Data\n#Initial Condition\nY0 = np.array([0.99,0.01, 0.0, 0.0])\n#Space to compute solutions\nt = np.linspace(0,5,101)\n#True param value\ntheta = 5.5\n\nsol = BlackBox(ODESYS, y0 = Y0, t = t, args = tuple([theta]))\n\n#Corupt the observations with noise\ny_obs = sol[:,:2] + np.random.normal(0,0.05,size = sol[:,:2].shape)\n\nplt.scatter(t,y_obs[:,0], marker = '.', alpha = 0.5, label = 'S')\nplt.scatter(t,y_obs[:,1], marker = '.', alpha = 0.5, label = 'I')\n\n\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f8e9d0b3d30&gt;\n\n\n\n\n\n\n\n\n\nPerform Gradient Descent\n\ntheta_iter = 1.5\ncost = Cost(y_obs[:,:2])\ngrad_C = autograd.grad(cost)\n\nmaxiter = 100\nlearning_rate = 1 #Big steps\nfor i in range(maxiter):\n\n    sol = BlackBox(ODESYS,y0 = Y0, t = t, args = tuple([theta_iter]))\n\n    Y = sol[:,:2]\n\n    theta_iter -=learning_rate*(grad_C(Y)*sol[:,-2:]).sum()\n\n    if i%10==0:\n        print(\"Theta estimate: \", theta_iter)\n\nTheta estimate:  1.697027594337629\n\n\nTheta estimate:  3.9189060278370365\n\n\nTheta estimate:  4.810038385538704\n\n\nTheta estimate:  5.251499985105974\n\n\nTheta estimate:  5.427206219478129\n\n\nTheta estimate:  5.46957706068474\n\n\nTheta estimate:  5.47744643541383\n\n\nTheta estimate:  5.4792194685272095\n\n\nTheta estimate:  5.479636817124458\n\n\nTheta estimate:  5.47973599525063\n\n\nAnd lastly, compare our fitted curves to the true curves\n\nsol = BlackBox(ODESYS, y0 = Y0, t = t, args = tuple([theta_iter]))\ntrue_sol = BlackBox(ODESYS, y0 = Y0, t = t, args = tuple([theta]))\n\n\nplt.plot(t,sol[:,0], label = 'S', color = 'C0', linewidth = 5)\nplt.plot(t,sol[:,1], label = 'I', color = 'C1', linewidth = 5)\n\nplt.scatter(t,y_obs[:,0], marker = '.', alpha = 0.5)\nplt.scatter(t,y_obs[:,1], marker = '.', alpha = 0.5)\n\n\nplt.plot(t,true_sol[:,0], label = 'Estimated ', color = 'k')\nplt.plot(t,true_sol[:,1], color = 'k')\n\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f8e9de4e5e0&gt;"
  },
  {
    "objectID": "posts/2019-05-21-odes/index.html#conclusions",
    "href": "posts/2019-05-21-odes/index.html#conclusions",
    "title": "Gradient Descent with ODEs",
    "section": "Conclusions",
    "text": "Conclusions\nFitting ODEs via gradient descent is possible, and not as complicated as I had initially thought. There are still some relaxations to be explored. Namely: what happens if we have observations at time \\(t_i\\) for one part of the system but not the other? How does this scale as we add more parameters to the model? Can we speed up gradient descent some how (because it takes too long to converge as it is, hence the maxiter variable). In any case, this was an interesting, yet related, divergence from my GSoC work. I hope you learned something."
  },
  {
    "objectID": "posts/2023-02-11-iv-risk-ratio/index.html",
    "href": "posts/2023-02-11-iv-risk-ratio/index.html",
    "title": "Causal Risk Ratios in AB Tests with One Sided Non-Compliance",
    "section": "",
    "text": "Sometimes we run AB tests where engaging with the treatment (and hence being treated) is optional. Since assignment to treatment is random, we can use the randomization as an instrumental variable (assuming there is a strong correlation between the instrument and the treatment, and that there are no backdoor paths between randomization and the final outcome).\nThere are a few libraries to do estimate the LATE from an instrumental variable. However, none of them report a causal risk ratio, which is usually our choice of causal contrast for better or worse.\nIn this post, I’m putting together some much appreciated advice from @guilhermejd1 and dimitry on cross validated on how to estimate a causal risk ratio in a randomized experiment with one sided non-compliance. I’ll first demonstrate how to do this with a simulation where we will actually have potential outcomes with which to compare. Then, I’ll apply this approach to a real experiment I helped run at Zapier."
  },
  {
    "objectID": "posts/2023-02-11-iv-risk-ratio/index.html#simulation",
    "href": "posts/2023-02-11-iv-risk-ratio/index.html#simulation",
    "title": "Causal Risk Ratios in AB Tests with One Sided Non-Compliance",
    "section": "Simulation",
    "text": "Simulation\nLet \\(A\\) be a binary indicator for assignment to treatment (1) or control (0). Let \\(d_j^a\\) be the potential outcome for engaging with the treatment under treatment \\(A=a\\) for user \\(j\\). Due to construction of the experiment, \\(d_j^0 = 0  \\forall j\\) because users in control can not engage in the treatment by design. Dimitry writes\n\n“This is different than the typical experiment in labor economics, where people can take up job training somewhere else even if they are in the control group”.\n\nThis means that we have two types of users in treatment group: \\(d_j^1 = 1\\) is a “complier” and \\(d_j^1 = 0\\) is a “never taker”. If our outcome is \\(y\\), then LATE is the ATE for compliers and is also the ATT.\nLet’s set up a simulation. Here are some details:\n\n\\(A\\) is decided by a coinflip (i.e. bernoulli with probability of success 0.5)\nThere is an unmeasured confounder \\(w\\), which is also distributed in the population via a coinflip.\nThe confounder and the treatment effect the probability of engaging with the treatment (being a complier). \\(P\\left(d_j^1 = 1 \\mid A=1, w\\right) = 0.4 - 0.2w\\). Because of the one sided compliance, \\(P(d_j^0=1) = 0\\).\nProbability of the outcome is \\(P\\left( y^a_j=1 \\mid d^{a}_j, w \\right) = 0.1 + 0.5w + 0.1d_j^{a}\\). So the instrument only effects the outcome through compliance.\n\nLet’s simuilate this in R\n\nlibrary(tidyverse)\nlibrary(kableExtra)\nmy_blue &lt;- rgb(45/250, 62/250, 80/250, 1)\ntheme_set(theme_classic()) \n\nupdate_geom_defaults(\"point\",   list(fill = my_blue, shape=21, color = 'black'))\n\n\nset.seed(0)\nN&lt;- as.integer(1e6) # Lots of precision\nA &lt;- rbinom(N, 1, 0.5)\nw &lt;- rbinom(N, 1, 0.5)\n\n# Potential outcomes\nd_0 &lt;- 0\nd_1 &lt;- rbinom(N, 1, 0.3 - 0.2*w + 0.1*A)\ny_0 &lt;- rbinom(N, 1, 0.1 + 0.5*w)\ny_1 &lt;- rbinom(N, 1, 0.1 + 0.5*w + 0.1*d_1)\n\n# and now the observed data via switching equation\ny &lt;- A*y_1 + (1-A)*y_0\nd &lt;- A*d_1 + (1-A)*d_0\ncomplier &lt;- d==1\n\nFrom our setup, \\(LATE = 0.1\\). Let’s compute that from our potential outcomes and estimate it using \\(A\\) as an instrument.\n\nsample_late &lt;- mean(y_1[d_1==1]) - mean(y_0[d_1==1])\nest_late &lt;- cov(y, A)/cov(d, A)\n\n\n\n\n\n\n\nTrue LATE\n\n\nSample LATE\n\n\nIV Estimate of LATE\n\n\n\n\n\n\n0.1\n\n\n0.101\n\n\n0.103\n\n\n\n\nTrue LATE, sample LATE, and estimated LATE. All 3 agree to within 3 decimal places and any differences are just sampling variability."
  },
  {
    "objectID": "posts/2023-02-11-iv-risk-ratio/index.html#estimating-the-causal-risk-ratio",
    "href": "posts/2023-02-11-iv-risk-ratio/index.html#estimating-the-causal-risk-ratio",
    "title": "Causal Risk Ratios in AB Tests with One Sided Non-Compliance",
    "section": "Estimating The Causal Risk Ratio",
    "text": "Estimating The Causal Risk Ratio\nIn order to compute the causal risk ratio we need two quantities:\n\nAn estimate of \\(E[y^1_j \\mid d^1]\\), and\nAn estimate of \\(E[y^0_j \\mid d^1]\\).\n\n\\(E[y^1_j \\mid d^1]\\) is easy to estimate; just compute the average outcome of those users in treatment who engaged with the treatment. Now because \\(LATE = E[y^1_j \\mid d^1] - E[y^0_j \\mid d^1]\\), the second estimate we need is obtained from some algebra.\n\n# Estimate from the data\nE_y1 &lt;- mean(y[d==1])\nE_y0 &lt;- E_y1 - est_late # use the estimate, not the truth\n\nE_y1/E_y0\n\n[1] 1.389267\n\n\nLet’s compare this to the true estimate of the causal risk ratio using potential outcomes\n\nmean(y_1[complier])/mean(y_0[complier])\n\n[1] 1.381259\n\n\nWhich is pretty damn close."
  },
  {
    "objectID": "posts/2023-02-11-iv-risk-ratio/index.html#oh-shit-i-forgot-a-bound",
    "href": "posts/2023-02-11-iv-risk-ratio/index.html#oh-shit-i-forgot-a-bound",
    "title": "Causal Risk Ratios in AB Tests with One Sided Non-Compliance",
    "section": "Oh Shit, I Forgot a Bound",
    "text": "Oh Shit, I Forgot a Bound\nNote that there is no bound on the LATE because it is estimated via OLS (sure, there are realistic bounds on how big this can be, but OLS isn’t enforcing those). In particular, what if \\(LATE = E[y^1 \\mid d^1]\\)? Then the denominator of the causal risk ratio would be 0. That’s…bad.\nMore over, what if \\(LATE \\approx E[y^1 \\mid d^1]\\) so that the denominator was really small? Then the causal risk ratio would basically blow up (that’s a technical term for “embiggen”).\nThe only reason I bring this up is because it happens in this example. Let’s bootstrap the estimated causal risk ratio (what we call “lift”) and look at the distribuion of bootstrap replicates.\n\n\n\n\n\n\n\n\n\nLMAO look at that tail! The long tail us due to the problems I’ve highlighted. In fact, we can highlight the “Oh shit zone” on a plot of the bootstraps (below in the figure below). The red line is where the tail behavior comes from; if you have a bootstrap replicate on that line, you should be saying “oh shit”.\n\n\n\n\n\n\n\n\n\nIn fact, there are some estimates from the bootstrap which yield \\(E[y^0\\mid d^1]&lt;0\\) so… what was the point if this?"
  },
  {
    "objectID": "posts/2023-02-11-iv-risk-ratio/index.html#what-was-the-point-of-this-post",
    "href": "posts/2023-02-11-iv-risk-ratio/index.html#what-was-the-point-of-this-post",
    "title": "Causal Risk Ratios in AB Tests with One Sided Non-Compliance",
    "section": "What Was The Point Of This Post",
    "text": "What Was The Point Of This Post\nOk, so estimating the causal risk ratio in a randomized experiment with one sided non-compliance is technically possible, but the math can get…weird. In particular, bootstrapping the standard errors (which is probably the most sensible way of estimating the standard errors unless you’re a glutton for delta method punishment) shows that we can get non-nonsensical bootstrapped estimates of the counterfacutal average outcome for compliers.\nHonestly…I’m not sure where to go from here. Point estimates are possible but incomplete. Bootstrapping is a sanest way to get standard errors, but have no way of ensuring the estimates are bounded appropriately. All is not lost, its nice to know this sort of thing can happen. Maybe the most sensible thing to say here is “do not ask for causal risk ratios for these types of experiments” and that is worth its weight in gold."
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "MIT License",
    "section": "",
    "text": "MIT License\nCopyright (c) 2023 dpananos.github.io authors\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "posts/2024-06-23-confound/index.html",
    "href": "posts/2024-06-23-confound/index.html",
    "title": "What Is Confounding?",
    "section": "",
    "text": "Thinking back to my Introduction to Epidemiology class, confounding was one of the topics of central interest. However, it took the remainder of my PhD (and then some) to really get what confounding was. Confounding was introduced as exposure and outcome having common cause. You’ve probably seen the idea motivated with the following DAG:\n\n\n\n\n\n\n\n\n\nHere, S is a common cause of \\(D\\) and \\(Y\\), and we’re typically interested in estimating the effect of \\(D\\) on \\(Y\\). Granted, the effect of \\(D\\) on \\(Y\\) is confounded. But like…why? DAG’s aside, what is happening here? That much isn’t answered in introductory classes, and I think it would behoove most people to learn the answer. As a digression, I know the simple difference in means can be decomposed into ATE + Selection Bias + Heterogeneous treatment effect, but I don’t think that really anwers what I want since that decomposition is in terms of potential outcomes and I want an answer which involves the data we have in hand and how we choose to summarize it.\nTo provide such an answer we need a little but of math, but only a little. In particular, we need the The Law of Total Expectation, which writes an expected value, \\(E[X]\\), as a weighted average of conditional expectations, namely\n\\[ E[X] = \\sum_a E[X \\mid A=a] P(A=a) \\&gt;. \\] In what follows, I motivate confounding with a fairly typical example involving how a binary exposure affects risk of a binary outcome. We’ll discuss the “Right Way” to estimate the effect of the exposure as well as “The Wrong Way”. We’ll find that the confounding of the relationship between exposure and outcome comes down to a single fact: the wrong weights are applied to the right estimates."
  },
  {
    "objectID": "posts/2024-06-23-confound/index.html#introduction",
    "href": "posts/2024-06-23-confound/index.html#introduction",
    "title": "What Is Confounding?",
    "section": "",
    "text": "Thinking back to my Introduction to Epidemiology class, confounding was one of the topics of central interest. However, it took the remainder of my PhD (and then some) to really get what confounding was. Confounding was introduced as exposure and outcome having common cause. You’ve probably seen the idea motivated with the following DAG:\n\n\n\n\n\n\n\n\n\nHere, S is a common cause of \\(D\\) and \\(Y\\), and we’re typically interested in estimating the effect of \\(D\\) on \\(Y\\). Granted, the effect of \\(D\\) on \\(Y\\) is confounded. But like…why? DAG’s aside, what is happening here? That much isn’t answered in introductory classes, and I think it would behoove most people to learn the answer. As a digression, I know the simple difference in means can be decomposed into ATE + Selection Bias + Heterogeneous treatment effect, but I don’t think that really anwers what I want since that decomposition is in terms of potential outcomes and I want an answer which involves the data we have in hand and how we choose to summarize it.\nTo provide such an answer we need a little but of math, but only a little. In particular, we need the The Law of Total Expectation, which writes an expected value, \\(E[X]\\), as a weighted average of conditional expectations, namely\n\\[ E[X] = \\sum_a E[X \\mid A=a] P(A=a) \\&gt;. \\] In what follows, I motivate confounding with a fairly typical example involving how a binary exposure affects risk of a binary outcome. We’ll discuss the “Right Way” to estimate the effect of the exposure as well as “The Wrong Way”. We’ll find that the confounding of the relationship between exposure and outcome comes down to a single fact: the wrong weights are applied to the right estimates."
  },
  {
    "objectID": "posts/2024-06-23-confound/index.html#an-example",
    "href": "posts/2024-06-23-confound/index.html#an-example",
    "title": "What Is Confounding?",
    "section": "An Example",
    "text": "An Example\nSuppose a new drug \\(D\\) is introduced to prevent death. In truth, the drug decreases the risk of death in both men and women by 10 percentage points, but men are more likely to take the drug and men are more likely to die. Hence, the effect of the drug is confounded by sex. Let \\(D=1\\) be exposure to the drug, \\(S=1\\) indicate males, and \\(Y=1\\) indicate death. I’m going to simulate this scenario using the following data generating process\n\\[ S \\sim \\operatorname{Bernoulli}(0.5) \\&gt;, \\] \\[ D \\mid S \\sim \\operatorname{Bernoulli}(0.6 + 0.3S) \\&gt;,\\]\n\\[ Y \\mid D, S \\sim \\operatorname{Bernoulli}(0.4 - 0.1D + 0.4S) \\&gt;.\\]\nI’ll simulate a million observations so that our precision is big enough to not worry about sampling variability. In R, that can be done with\n\nsim_data &lt;- function(x=0, n=1e6){\n  withr::with_seed(x, {\n    s &lt;- rbinom(n, 1, 0.5)\n    d &lt;- rbinom(n, 1, 0.6 + 0.3*s)\n    y &lt;- rbinom(n, 1, 0.4 - 0.1*d + 0.4*s)\n  })\n  data.frame(s, d, y)\n}\n\nWe’ll use this data to explore the “Right Way” and teh “Wrong Way” of analyzing the causal effect of \\(D\\) on \\(Y\\)."
  },
  {
    "objectID": "posts/2024-06-23-confound/index.html#the-right-way",
    "href": "posts/2024-06-23-confound/index.html#the-right-way",
    "title": "What Is Confounding?",
    "section": "The “Right Way”",
    "text": "The “Right Way”\nOpen up a book like “What If” and you’ll find that the way to correctly estimate the effect of \\(D\\) on \\(Y\\) would be to first estimate the risk of death within strata defined by sex and then weight the stratified estimates by the prevalence of the strata. This can be done in the following way\n\n\nCode\nlibrary(marginaleffects)\n\nmodel &lt;- glm(y~d*s, data=sim_data(), family = binomial())\n\ncrossing(d=0:1, s=0:1) %&gt;% \n  modelr::add_predictions(model, type='response') %&gt;% \n  summarise(pred = mean(pred), .by = d) %&gt;% \n  pull(pred) %&gt;% \n  diff\n\n\n[1] -0.09910701\n\n\nor, perhaps more elegantly, using {marginaleffects}\n\n\nCode\nlibrary(marginaleffects)\n\n# If I didn't know P(S) maybe I wouldn't use datagrid in this way\nnd &lt;- datagrid(model = model, d=0:1, s=0:1)\n\navg_comparisons(model, variables = 'd', newdata = nd)\n\n\n\n Term Contrast Estimate Std. Error     z Pr(&gt;|z|)   S  2.5 %  97.5 %\n    d    1 - 0  -0.0991    0.00119 -83.6   &lt;0.001 Inf -0.101 -0.0968\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\nRegardless of approach, the estimate is very close to the actual effect of the drug; a reduction in risk of death of 10 percentage points. Why exactly this procedure works relies on the consistency and ignorability assumptions, and the law of total expectation.\nBriefly, if we have consistency and ignorability then \\(E[Y(d) \\mid S=s] = E[Y\\mid D=d, S=s]\\), and by the law of total expectation\n\\[ E[Y(d)] = \\sum_{s} E[Y(d) \\mid S=s] P(S=s) =\\sum_{s} E[Y \\mid D=d, S=s] P(S=s) \\] which is a weighted average of within strata estimates. ’Nuff said, let’s take an extended look at the wrong way of doing things."
  },
  {
    "objectID": "posts/2024-06-23-confound/index.html#the-wrong-way",
    "href": "posts/2024-06-23-confound/index.html#the-wrong-way",
    "title": "What Is Confounding?",
    "section": "The “Wrong Way”",
    "text": "The “Wrong Way”\nSo we know the “Right Way” uses the law of total expectation to get the right weights for the within strata estimates. What does the “Wrong Way” estimate then if not the true causal contrast? Let’s use avg_comparisons to see\n\n\nCode\nmodel &lt;- glm(y~d, data=sim_data(), family = binomial())\n\nnd &lt;- datagrid(model = model, d=0:1)\n\navg_comparisons(model, variables = 'd', newdata = nd)\n\n\n\n Term Contrast Estimate Std. Error    z Pr(&gt;|z|)   S  2.5 % 97.5 %\n    d    1 - 0   0.0599    0.00115 51.8   &lt;0.001 Inf 0.0576 0.0621\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\nInteresting, the wrong answer is really wrong. Without adjusting for sex, we would conclude that the drug increases risk of death by 6%. Of course, this is a result of the fact that men are more likely to take the drug and more likely to die.\nThe “Wrong Way” is a simple difference in means between exposure groups. In doing so, we marginalize over the distribution of \\(S\\) conditional on \\(D\\). Since the law of total expectation is used to obtain the right answer, it might be beneficial to use it here as well. In the “Wrong Way” we compute\n\\[ E[Y \\mid D=d] \\&gt;. \\]\nwhich can be re-written using the law of total expectation as\n\\[ E[Y \\mid D=d] = \\sum_{s} E[Y \\mid D=d, S=s] P(S=s\\mid D=d) \\]\nAha! Now we see what confounding really is. Note that \\(E[Y \\mid D=d, S=s]\\) appears in both the “Right Way” and the “Wrong Way”, but in the “Wrong Way” the weights are wrong! They should be \\(P(S=s)\\) but instead they are \\(P(S=s\\mid D=d)\\)! Confounding is (partially) an improper weighting of the correct estimates!\nAs an interesting digression, we can actually compute what the wrong approach would produce in the limit of infinite data if we just leverage Bayes rule to rewrite \\(P(S=s\\mid D=d)\\) as\n\\[P(S=s\\mid D=d) = \\dfrac{P(D=d \\mid S=s) P(S=s)}{P(D=d)} \\&gt;. \\]\n\nEy &lt;- function(d, s){\n  0.4 - 0.1*d + 0.4*s\n}\n\nEd &lt;- function(s){\n  0.6 + 0.3*s  \n}\n\nPd1 &lt;- Ed(1)*0.5 + Ed(0)*0.5\n\nEyd1 &lt;- Ey(1, 1)*Ed(1)*0.5/Pd1 + Ey(1, 0)*Ed(0)*0.5/Pd1\nEyd0 &lt;- Ey(0, 1)*(1-Ed(1))*0.5/(1-Pd1) + Ey(0, 0)*(1-Ed(0))*0.5/(1-Pd1)\n\nwrong_answer &lt;- Eyd1 - Eyd0\n#0.06\n\nThis really drives the point home that confounding is not a matter of more data, rather its a matter of the right data."
  },
  {
    "objectID": "posts/2024-06-23-confound/index.html#more-wrong",
    "href": "posts/2024-06-23-confound/index.html#more-wrong",
    "title": "What Is Confounding?",
    "section": "More Wrong",
    "text": "More Wrong\nSo confounding is partially the wrong weights applied to the right estimates. Earlier, I mentioned that the simple difference in means can be decomposed into selection bias and heterogeneous treatment effects. Now, I’ve designed this example to have a homogeneous treatment effect, so we know that the error comes from selection bias. Let’s determine where exactly selection bias lives in the wrong weights applied to the right estimates, \\(P(S=s\\mid D=d)\\).\nAccording to Causal Inference: The Mixed Tape, the selection bias in the simple difference in means is\n\\[ E[Y(0) \\mid D=1] - E[Y(0) \\mid D=0] \\&gt;. \\]\nAgain, let’s use the law of total expectation to re-write \\(E[Y(d^\\prime) \\mid D=d]\\) as a marginalization over \\(S\\).\n\\[ \\begin{align*}E[Y(d^\\prime) \\mid D=d] &= \\sum_{s} E[Y(d^\\prime) \\mid D=d, S=s]P(S=s\\mid D=d) \\\\ &= \\sum_{s}E[Y(d^\\prime) \\mid D=d, S=s]\\frac{P(D=d \\mid S=s) P(S=s)}{P(D=d)} \\end{align*}\\]\nIts easier to spot where the bias comes from (mathematically) if you consider what the implication would be were exposure to be independent of sex. If exposure were independent of sex, as would be the case in a randomized experiment, then \\(P(D=d \\mid S=s) = P(D=d)\\). The weight in this sum then collapses to \\(P(S=s)\\), which is the right weight. However, in this example, \\(D\\) is not independent of \\(S\\) and so \\(P(D=d \\mid S=s) \\neq P(D=d)\\)! This probability has a name, the propensity score, and that it is not the same between strata defined by sex is the reason that the selection bias is non zero in our example. If the propensity score was the same within strata, then strata defined by \\(D\\) would be exchangeable and \\(E[Y(d^\\prime) \\mid D=d] = E[Y(d^\\prime)]\\)."
  },
  {
    "objectID": "posts/2024-06-23-confound/index.html#conclusion",
    "href": "posts/2024-06-23-confound/index.html#conclusion",
    "title": "What Is Confounding?",
    "section": "Conclusion",
    "text": "Conclusion\nConfounding, at least exemplified by the DAG at the top of this post, can be seen as the improper weighting of the right estimates. When viewed through this lens, approaches like re-weighting (via inverse propensity scores or some other mechanism) make a lot more intuitive sense as possible remidies."
  },
  {
    "objectID": "posts/2023-05-14-ab-brain-dump/index.html",
    "href": "posts/2023-05-14-ab-brain-dump/index.html",
    "title": "A Practical A/B Testing Brain Dump",
    "section": "",
    "text": "Over the past year or so, I’ve helped move Zapier towards doing better experiments and more of them. I wanted to document some of this for posterity; what I did, what worked, how we know it worked, and where I might want to go in the future. This is more of a story rather than a set of instructions.\nMy hope is that there is some sort of generalizable knowledge here. I don’t want to turn this into a set of instructions, but I do think there is something to learn (even if this has been written about before. I mean, I couldn’t find anything I needed, so here I am making it). As such, its going to be a long post. I might periodically update this post from time to time so that I can just put something out there initially."
  },
  {
    "objectID": "posts/2023-05-14-ab-brain-dump/index.html#what-is-this-post",
    "href": "posts/2023-05-14-ab-brain-dump/index.html#what-is-this-post",
    "title": "A Practical A/B Testing Brain Dump",
    "section": "",
    "text": "Over the past year or so, I’ve helped move Zapier towards doing better experiments and more of them. I wanted to document some of this for posterity; what I did, what worked, how we know it worked, and where I might want to go in the future. This is more of a story rather than a set of instructions.\nMy hope is that there is some sort of generalizable knowledge here. I don’t want to turn this into a set of instructions, but I do think there is something to learn (even if this has been written about before. I mean, I couldn’t find anything I needed, so here I am making it). As such, its going to be a long post. I might periodically update this post from time to time so that I can just put something out there initially."
  },
  {
    "objectID": "posts/2023-05-14-ab-brain-dump/index.html#what-this-post-is-not",
    "href": "posts/2023-05-14-ab-brain-dump/index.html#what-this-post-is-not",
    "title": "A Practical A/B Testing Brain Dump",
    "section": "What This Post Is Not",
    "text": "What This Post Is Not\nThis is not a list of statistical tests to run (spoiler: its all regression anyway). Nor is this a technical post about feature flagging, persisting experiences across sessions, caching, or any other technical aspects of running online experiments."
  },
  {
    "objectID": "posts/2023-05-14-ab-brain-dump/index.html#what-am-i-going-to-cover",
    "href": "posts/2023-05-14-ab-brain-dump/index.html#what-am-i-going-to-cover",
    "title": "A Practical A/B Testing Brain Dump",
    "section": "What Am I Going To Cover",
    "text": "What Am I Going To Cover\nRoughly, I want to go through the steps I took over the last year. Much of it was off the cuff and improvisation, but a lot of it was inspired by working with clinicians and contents of the book Trustworthy Online Controlled Experiments. Topics will include:\n\nUnderstanding where we are and where we came from\nDetermining the quickest way to improve the situation\nBuilding credibility and trust\nScaling experimentation – even if it means buying rather than building\n(Counter intuitive) signs we are moving in the right direction\nNext Steps\n\nYou should know: This was not a one man effort. There are lots of people to thank for any success I’ve had. Guidance from leadership, all the people who let me touch their projects, and all the data scientists who tapped me for help or a consult."
  },
  {
    "objectID": "posts/2023-05-14-ab-brain-dump/index.html#footnotes",
    "href": "posts/2023-05-14-ab-brain-dump/index.html#footnotes",
    "title": "A Practical A/B Testing Brain Dump",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n1↩︎\n1↩︎"
  },
  {
    "objectID": "posts/2023-09-03-did-in-ab/index.html",
    "href": "posts/2023-09-03-did-in-ab/index.html",
    "title": "Difference in Difference Estimates Can Be Inaccurate When Adjusting For Detected Pre-Treatment Differences in Randomized Experiments",
    "section": "",
    "text": "Edits:"
  },
  {
    "objectID": "posts/2023-09-03-did-in-ab/index.html#introduction",
    "href": "posts/2023-09-03-did-in-ab/index.html#introduction",
    "title": "Difference in Difference Estimates Can Be Inaccurate When Adjusting For Detected Pre-Treatment Differences in Randomized Experiments",
    "section": "Introduction",
    "text": "Introduction\nI’ve run into people randomizing units to treatment and control and then looking to see if there are pre-treatment differences with a hypothesis test. If there are, I’ve heard – at Zapier and cross validated – that a difference in difference (DiD) should be performed. After all, there are baseline differences! We need to adjust for those.\nTo be clear – using DiD to analyze randomized experiments is fine. The resulting estimate of the ATE should be unbiased assuming the experiment was run without a hitch. You don’t need to do difference in difference because prior to treatment the two groups will have the same distribution of potential outcomes. Their pre-treatment differences are 0 in expectation. Any detection of a difference – again, assuming the experiment was run well – is sampling variability.\nRunning DiD because we found baseline differences is a form of deciding on analysis based on the observed data, and we all know that is a statistical faux pas. But how bad could it be? Are our estimates of the treatment effect biased? Are they precise? What do we lose when we let the data decide if we should run a DiD or a t-test?"
  },
  {
    "objectID": "posts/2023-09-03-did-in-ab/index.html#simulation",
    "href": "posts/2023-09-03-did-in-ab/index.html#simulation",
    "title": "Difference in Difference Estimates Can Be Inaccurate When Adjusting For Detected Pre-Treatment Differences in Randomized Experiments",
    "section": "Simulation",
    "text": "Simulation\nTo find out, let’s simulate a very simple example. Let’s assume that I run an experiment and measure units before and after. The observations on each unit are iid and have standard normal distribution in the absence of the treatment. If \\(A\\) is a binary indicator for treatment (1 for treatment, 0 else) then the data are\n\\[ y_{pre} \\sim \\mbox{Normal}\\left(0, \\sigma^2\\right) \\&gt;, \\] \\[ y_{post} \\sim \\mbox{Normal}\\left(\\tau \\cdot A, \\sigma^2 \\right) \\&gt;. \\]\nWe will allow for the pre and post observations to be correlated with correlation \\(\\rho\\).\nI’ll run 5, 000 simulations of a simple randomized experiment. Each time, I’ll sample \\(N\\) units in each arm, enough to detect a treatment effect from a t-test with 80% power. I’ll then run a t-test via OLS and a DiD. I’ll record the pre-treatment difference in each group and if it was statistically significant at the 5% level. I’m also going to carry around the estimates from an ANCOVA since this seems to be the recommended approach for when we have pre-treatment information that is correlated with the outcome. For these simulations, I’ll set \\(\\tau=1\\) and \\(\\sigma=1\\) which means I need \\(N=17\\) users per arm to achieve 80% power.\nWe’ll plot some treatment effect estimates and see what is happening when we choose to do DiD when the data suggest we do. Now importantly, I’m making very strong assumptions about the experiment being run. In particular, I’m making assumptions that all went well, there is no funny business with timing or randomization, etc. In terms of a medical trial, I got 34 people to all stand in a line, randomly gave each placebo or drug, watched them all take it at the same time, and recorded outcomes. The purpose of these simulation and blog post is to investigate statistical properties and not to wring about whatabouts.\nIn the code cell below is the code to run these simulations\n\n\nCode\nsimulate_data &lt;- function(N_per_arm=17, tau=1, sigma=1, rho=0, ...){\n  \n  \n  Sigma &lt;- diag(rep(sigma, 2)) %*% matrix(c(1, rho, rho, 1), nrow=2) %*% diag(rep(sigma, 2))\n  \n  A &lt;- rep(0:1, N_per_arm)\n  Y &lt;- cbind(0, tau*A) + MASS::mvrnorm(2*N_per_arm, mu = c(0, 0), Sigma = Sigma)\n  \n  y_pre &lt;- Y[, 1]\n  y_post &lt;- Y[, 2]\n  \n  pre &lt;- tibble(i = seq_along(A), y=y_pre, trt=A, period=0)\n  post &lt;- tibble(i = seq_along(A), y=y_post, trt=A, period=1)\n  \n  bind_rows(pre, post)\n      \n}\n\ndo_analysis &lt;- function(...){\n  \n  d &lt;- simulate_data(...)\n  \n  dwide &lt;- d %&gt;% \n           mutate(period = if_else(period==1, 'post','pre'))  %&gt;% \n          pivot_wider(id_cols = c(i, trt), names_from = period, values_from = y)\n  \n  #DiD \n  did &lt;- lm(y ~ trt*period, data=d)\n  # t-test, only on post data\n  tt &lt;- lm(y ~ trt, data=filter(d, period==1))\n  ## Ancova\n  ancova &lt;- lm(post ~ pre + trt, data=dwide)\n  \n  tt_ate &lt;- coef(tt)['trt']\n  did_ate &lt;- coef(did)['trt:period']\n  ancova_ate &lt;- coef(ancova)['trt']\n  \n  pre_test &lt;- t.test(y~trt, data = filter(d, period==0))\n  \n  pre_period_diff &lt;- diff(pre_test$estimate)\n  detected &lt;- if_else(pre_test$p.value&lt;0.05, 'Pre-Period Difference', 'No Pre-Period Difference')\n  \n  tibble(\n    tt_ate, \n    did_ate, \n    ancova_ate,\n    pre_period_diff, \n    detected\n  )\n}\n\n\nresults &lt;- map_dfr(1:5000, ~do_analysis(), .id = 'sim')"
  },
  {
    "objectID": "posts/2023-09-03-did-in-ab/index.html#case-1-uncorrelated-pre-and-post-observations",
    "href": "posts/2023-09-03-did-in-ab/index.html#case-1-uncorrelated-pre-and-post-observations",
    "title": "Difference in Difference Estimates Can Be Inaccurate When Adjusting For Detected Pre-Treatment Differences in Randomized Experiments",
    "section": "Case 1: Uncorrelated Pre and Post Observations",
    "text": "Case 1: Uncorrelated Pre and Post Observations\nShown below are the ATEs from each analysis when the pre and post are uncorrelated. Nothing too surprising here, the ATEs are unbiased (the histograms are centered at \\(\\tau=1\\)). It seems that the t-test has lower sampling variance/higher precision, which means lower MSE.\n\n\nCode\nplot &lt;- results %&gt;% \n        ggplot(aes(tt_ate, did_ate)) + \n        geom_point(alpha = 0.5) + \n        labs(\n          x= 'T-test ATE',\n          y='DiD ATE'\n        ) + \n  theme(aspect.ratio = 1)\n\nggMarginal(plot, type='histogram')\n\n\n\n\n\n\n\n\n\nLet’s now plot the ATEs for each method against the pre-period differences. Because all observations are assumed independent, I’m going to expect that the ATEs for the t-test are uncorrelated with the pre-period difference. However, because the DiD uses pre-period information, I’m going to expect a correlation (I just don’t know how big).\n\n\nCode\nplot &lt;- results %&gt;% \n  pivot_longer(tt_ate:ancova_ate, names_to = 'analysis', values_to = 'ate') %&gt;% \n  mutate(\n    analysis = case_when(\n      analysis == 'tt_ate' ~ 'T-test',\n      analysis == 'did_ate' ~ 'DiD',\n      analysis == 'ancova_ate' ~ 'Ancova'\n    )\n  ) %&gt;% \n  ggplot(aes(pre_period_diff, ate)) + \n  geom_point(alpha=0.5) + \n  facet_grid( ~ analysis) + \n  labs(x='Pre period difference',\n       y = 'ATE') +\n  theme(aspect.ratio = 1)\n\n\nplot \n\n\n\n\n\n\n\n\n\nGreat, this makes sense. The ATE is for the t-test is uncorrelated with the pre-period difference, as expected. The ATE DiD is correlated with the pre-period difference, and that’s likely due to regression to the mean. Now, let’s stratify by cases when the pre-period difference is (erroneously) thought to be non-zero.\n\n\nCode\nplot + facet_grid(detected ~ analysis)\n\n\n\n\n\n\n\n\n\nIt isn’t unsurprising that the tails of each of these blobs is cut off. After all, the pre-period difference needs to be extreme enough to reject the null. Let’s first talk about that bottom right cell – the t test when there is a detected pre-period difference. Because there is no correlation between pre-period difference and the ATE, the ATEs are still unbiased. That’s great. What about DiD (bottom middle cell)?\nNote that the correlation means that those blobs don’t have the same mean. In fact, if you run K-means on those blobs, you can very easily separate them and estimate the ATE and its very far from 1! Technically, this is unbiased because we have to average over each blob. So it isn’t that the estimates are biased from DiD, but they are inaccurate. Take a look at the MSE for each estimate (shown below). Using DiD when you detect a pre-treatment difference may be unbiased, but it has very high MSE as compared to the two other methods.\nWhile those estimates are unbiased, the low precision/high MSE is a good argument against checking for pre-treatment differences and then deciding on analysis style. This would be like saying “the river is a foot deep” when in reality the river is an inch deep in most places, and a mile deep in others. While the estimates are unbiased, any one particular estimate is fairly far from the true ATE.\n\n\nCode\nresults %&gt;% \n  pivot_longer(tt_ate:ancova_ate, names_to = 'analysis', values_to = 'ate') %&gt;% \n  mutate(\n    analysis = case_when(\n      analysis == 'tt_ate' ~ 'T-test',\n      analysis == 'did_ate' ~ 'DiD',\n      analysis == 'ancova_ate' ~ 'Ancova'\n    )\n  ) %&gt;% \n  group_by(analysis, detected) %&gt;% \n  summarise(\n    MSE = mean((ate-1)^2)\n  ) %&gt;% \n  ggplot(aes(analysis, MSE, fill=detected)) + \n  geom_col(position = position_dodge2()) + \n  theme(legend.position = 'top') + \n  labs(x='Analysis', fill='')"
  },
  {
    "objectID": "posts/2023-09-03-did-in-ab/index.html#case-2-moderate-correlation-between-pre-and-post",
    "href": "posts/2023-09-03-did-in-ab/index.html#case-2-moderate-correlation-between-pre-and-post",
    "title": "Difference in Difference Estimates Can Be Inaccurate When Adjusting For Detected Pre-Treatment Differences in Randomized Experiments",
    "section": "Case 2: Moderate Correlation Between Pre and Post",
    "text": "Case 2: Moderate Correlation Between Pre and Post\nLet’s now let pre and post correlation be non-zero, but still moderate in size. We’ll let \\(\\rho = 0.45\\) which is still much smaller than most correlation coefficients I’ve seen in real AB tests at Zapier.\nMaking the same plot as we did before, we see that DiD still suffers from poor accuracy as measured by MSE, bow now the T-test is starting to demonstrate some positive correlation between pre-period difference and ATE. Its worth mentioning again that we would not really do anything different with the information that there was a pre-period difference when using the t-test. The estimate of the ATE would still be unbiased and have low MSE.\n\n\nCode\nresults &lt;- map_dfr(1:5000, ~do_analysis(N_per_arm=17, tau=1, sigma=1, rho=0.45), .id = 'sim')\n\nresults %&gt;% \n  pivot_longer(tt_ate:ancova_ate, names_to = 'analysis', values_to = 'ate') %&gt;% \n  mutate(\n    analysis = case_when(\n      analysis == 'tt_ate' ~ 'T-test',\n      analysis == 'did_ate' ~ 'DiD',\n      analysis == 'ancova_ate' ~ 'Ancova'\n    )\n  ) %&gt;% \n  ggplot(aes(pre_period_diff, ate)) + \n  geom_point(alpha=0.5, color = 'gray 45') + \n  facet_grid( ~ analysis) + \n  labs(x='Pre period difference',\n       y = 'ATE') +\n  theme(aspect.ratio = 1) + \n  facet_grid(detected ~ analysis)"
  },
  {
    "objectID": "posts/2023-09-03-did-in-ab/index.html#case-3-strong-correlation-between-pre-and-post",
    "href": "posts/2023-09-03-did-in-ab/index.html#case-3-strong-correlation-between-pre-and-post",
    "title": "Difference in Difference Estimates Can Be Inaccurate When Adjusting For Detected Pre-Treatment Differences in Randomized Experiments",
    "section": "Case 3: Strong Correlation Between Pre and Post",
    "text": "Case 3: Strong Correlation Between Pre and Post\nIn AB testing, it isn’t uncommon to see \\(\\rho&gt;0.9\\) between pre and post outcomes. When we run experiments and seek to have an effect on existing user’s monthly payments, many users won’t change at all meaning they are typically paying the same amount this month as they were last. That results in very high \\(\\rho\\).\nLet’s rerun the simulation, but this time specify that pre and post have a correlation of 0.95.\n\n\nCode\nresults &lt;- map_dfr(1:5000, ~do_analysis(N_per_arm=17, tau=1, sigma=1, rho=0.95), .id = 'sim')\n\n\nmyblue &lt;- rgb(48/255, 61/255, 78/255, 0.5)\n\nplot &lt;- results %&gt;% \n        ggplot(aes(tt_ate, did_ate)) + \n        geom_point( color=myblue) + \n        labs(\n          x= 'T-test ATE',\n          y='DiD ATE'\n        ) + \n  theme(aspect.ratio = 1)\n\nggMarginal(plot, type='histogram')\n\n\n\n\n\n\n\n\n\nI’m fairly surprised to see that the results almost completely switch. First, the DiD estimator becomes much more efficient (see the joint distribution of ATEs) which is cool. Second, now the estimates from DiD become unbiased and accurate which is a nice change. Ancova seems to do just as well as DiD in terms of precision and accuracy, which was not expected simply because I didn’t think DiD would work this well under these assumptions.\n\n\nCode\nresults %&gt;% \n  pivot_longer(tt_ate:ancova_ate, names_to = 'analysis', values_to = 'ate') %&gt;% \n  mutate(\n    analysis = case_when(\n      analysis == 'tt_ate' ~ 'T-test',\n      analysis == 'did_ate' ~ 'DiD',\n      analysis == 'ancova_ate' ~ 'Ancova'\n    )\n  ) %&gt;% \n  ggplot(aes(pre_period_diff, ate)) + \n  geom_point(color=myblue) + \n  facet_grid( ~ analysis) + \n  labs(x='Pre period difference',\n       y = 'ATE') +\n  theme(aspect.ratio = 1) + \n  facet_grid(detected ~ analysis)"
  },
  {
    "objectID": "posts/2023-09-03-did-in-ab/index.html#conclusion",
    "href": "posts/2023-09-03-did-in-ab/index.html#conclusion",
    "title": "Difference in Difference Estimates Can Be Inaccurate When Adjusting For Detected Pre-Treatment Differences in Randomized Experiments",
    "section": "Conclusion",
    "text": "Conclusion\nRandomization creates two artificial groups who have the same distribution of potential outcomes. There is no need to correct for baseline differences in randomized groups a la DiD, and there is certainly no need to check for baseline differences after randomizing groups.\nHowever, deciding to adjust for baseline differences with DiD can result in unbiased but inaccurate estimates of the treatment effect under some circumstances. From this post, those conditions seemed to be when pre and post treatment outcomes were uncorrelated. That can happen when the outcomes are very noisy or when outcomes are homogeneous across units. While unbiased, any one particular estimate from this procedure was very far away from the true ATE, as evidenced by the larger MSE.\nHowever, when outcomes are correlated between pre and post (like they can be in AB tests), then the story flips. DiD becomes very efficient, nearly as efficient as Ancova, with good accuracy and unbiased estimates.\nGenerally it seems then that using DiD for a randomized experiment is fine. In cases where the outcomes are highly correlated, you could even check for baseline differences with out much harm. But it isn’t needed, and there are circumstances where deciding to use DiD because of detected baseline differences can hurt your estimates.\nIf you want unbiasedness and low variance/high precision, the results from this post (and advice elsewhere) seems to be to stick with Ancova."
  },
  {
    "objectID": "posts/2022-12-06-journeyman/index.html",
    "href": "posts/2022-12-06-journeyman/index.html",
    "title": "Journeyman Statistics",
    "section": "",
    "text": "A number of people have asked me how to learn statistics. I don’t have a good answer for them. I find all books are deficient in some way: some too theoretical, others not theoretical enough. Some too focused on pen and paper calculation, others provide code that was very likely written decades ago and do not use modern packages or tidy principles (e.g they begin their analysis with rm(list=ls() or use attach and detatch). I can’t recommend my own path to learning statistics either, mostly because I did a Ph.D in statistics and had the benefit of a Masters in Applied Mathematics.\nI want to write a book for people who are not statisticians but need to make use of statistics anyway. The medical residents who need to do logistic regression for a research project, the social science grad student (save econometricians, I’d say we could learn something from them but we probably know all the same things by different names) who is more interested in their research than their statistical models, the business intelligence analyst who has to to analyze a (poorly planned) A/B test and would love nothing than to improve the experiment the next time around.\nThis sequence of blog posts is going to be a sort of first go at that book. Not even an alpha or a rough draft, but rather somewhere to put some thoughts that might eventually make it into the book. This first post is about the motivation for the book, which I am tentatively calling “Journeyman Statistics”."
  },
  {
    "objectID": "posts/2022-12-06-journeyman/index.html#what-is-journeyman-statistics",
    "href": "posts/2022-12-06-journeyman/index.html#what-is-journeyman-statistics",
    "title": "Journeyman Statistics",
    "section": "What is Journeyman Statistics?",
    "text": "What is Journeyman Statistics?\nA journeyperson/woman/man is\n\n“a worker, skilled in a given building trade or craft, who has successfully completed an official apprenticeship qualification. Journeymen are considered competent and authorized to work in that field as a fully qualified employee. They earn their license by education, supervised experience and examination. Although journeymen have completed a trade certificate and are allowed to work as employees, they may not yet work as self-employed master craftsmen.\n\nWhat do I mean then by “journeyman statistics” and who are “journeyman statisticians”? Borrowing heavily from the definition above, journeyman statisticians are people who are trained in some field and are currently doing statistics in service of someone or something else. Journeyman statistics are then the statistical analyses performed by these people. I don’t think its tough to pick our journeyman statisticians; they are to a first approximation those who perform statistical analysis but are not statisticians first and foremost. They are biologists, medical residents, sociologists, analysts of several varietys, etc. To them, statistics is the means whereas statistics are the end to research (perhaps “pure”) statisticians.\nIts important to further distinguish journeyman statisticians from applied statisticians. Applied statisticians can, as Tukey once said, “play in everyone’s backyard”. They possess the necessary mathematical maturity and statistical expertise to move from field to field. A journeyman statistician, though they may be well versed in statistics, would likely stay in their own backyard (to continue the metaphor) in order to tackle problems there.\nOf course, I don’t mean to place people in boxes. You don’t need to subscribe to my taxonomy of statisticians (in fact, outside of this book I don’t think its a particularly useful taxonomy), and I think there are edge cases which threaten the taxonomy as a whole. The taxonomy is simply a model, and this model is useful for one thing: understanding motivations for learning statistics, and designing a path through statistical literature so as to serve those motivations."
  },
  {
    "objectID": "posts/2022-12-06-journeyman/index.html#why-do-we-need-a-book-on-journeyman-statistics",
    "href": "posts/2022-12-06-journeyman/index.html#why-do-we-need-a-book-on-journeyman-statistics",
    "title": "Journeyman Statistics",
    "section": "Why Do We Need a Book on Journeyman Statistics?",
    "text": "Why Do We Need a Book on Journeyman Statistics?\nThe taxonomy allows us to understand who journeyman statisticians are, what their intentions are, what they may lack in terms of statistical understanding, what is enough to satiate their desire to learn statistics, and what details contribute “noise” rather than “signal”. As an example, I don’t think biology grad students need to know what \\(\\operatorname{plim}\\) means, or any of the other topics adjacent to mathematical analysis in Casella and Berger’s Statistical Inference. They do need to go slightly beyond their sophmore classes which tell them they can use the t-test when \\(N&gt;30\\). Likewise, medical residents need to go beyond Martin Bland’s An Introduction to Medical Statistics and need to be able to confidently say “we shouldn’t do that” when their supervisors or superiors insist on a clearly flawed mode of analysis. However, they may get bogged down by the integrals in Frank Harrell’s Regression Modelling Strategies (as well as the sea of references to methodological papers). There are few books for people like that. I find that books are typically for sophmore students learning statistics for the first time, or applied statisticians. Journeyman statisticians need something in the middle. I hope this book is that something."
  },
  {
    "objectID": "posts/2022-12-06-journeyman/index.html#what-will-this-book-contain",
    "href": "posts/2022-12-06-journeyman/index.html#what-will-this-book-contain",
    "title": "Journeyman Statistics",
    "section": "What Will This Book Contain?",
    "text": "What Will This Book Contain?\nStatisticians like to joke “its all regression”. There is truth in that phrase, and so this book will take the perspective that regression is the primary means of estimation. We’ll cover all the typical analyses as regression methods. This includes estimation of the mean, its just a one parameter regression. I want to get to GLM’s as quickly as possible while not getting bogged down by mathematical details, like whatever “mild regularity conditions” means. GLMs are the workhorse of applied statistics, and I see no point in leaving GLMs to later chapters. One thing that will be absent from the book is p values. The book will take an estimation approach and report only confidence intervals.\nThe book will also contain code in both python and R, though I encourage readers to use R rather than python. Python’s statistical tools have a distinct econometrics flavor."
  },
  {
    "objectID": "posts/2022-12-06-journeyman/index.html#what-benefit-is-there-to-reading-this-book",
    "href": "posts/2022-12-06-journeyman/index.html#what-benefit-is-there-to-reading-this-book",
    "title": "Journeyman Statistics",
    "section": "What Benefit is There to Reading This Book?",
    "text": "What Benefit is There to Reading This Book?\nBefore discussing why you should read this book, I want to discuss what I call “The Precision-Usefulness Tradeoff”, as I anticipate I will refer to this many times. In short, the tradeoff states\n\nPerfectly precise statements are completely useless. In order to become useful, the statement must be made less precise. The less precise, the more useful.\n\nAgain, this is a model rather than a law. As an example, consider the definition for a 95% confidence interval.\n\nA 95% confidence interval is an interval estimate which, upon repeated construction under identical and ideal conditions, contains the estimand 95% of the time.\n\nI can make this statement more precise by writing it down mathematically\n\\[ P \\left( \\bar{x} - z_{0.975} \\sigma/\\sqrt{n} \\leq \\mu \\leq \\bar{x} + z_{0.975} \\sigma/\\sqrt{n} \\right) = 0.95 \\]\nbut it loses usefulness. This doesn’t really tell me what a confidence interval is, when to use one, or how to interpret one. However, the definition I initially presented perhaps permits some pathological cases. We can make the definition more useful by removing precision further\n\nA 95% confidence interval contains parameters consistent with the data.\n\nNow, we have a better idea how to interpret the interval, but the 95% is opaque to us.\nThe reason I bring up this trade off is because the book is intended to give journeyman statisticians the tools required to move in Precision-Usefulness space. My hope is that when the time comes, you will be able to artfully trade precision for usefulness (e.g. to be pedantic when it is necessary, or to break the rules precisely because you know how and when they can be broken safely). This is the primary benefit of the book. Obviously, I can’t tell people when or where to move within that space. I will have to leave that to their best judgement.\nThe second benefit is to solve the problem we began with; to answer “how to I go about learning statistics” in a satisfactory way."
  },
  {
    "objectID": "posts/2023-09-11-right/index.html",
    "href": "posts/2023-09-11-right/index.html",
    "title": "Right In Ways You Don’t Care About",
    "section": "",
    "text": "I’ve typically been long on the value of data scientists but I think I’m starting to lose my optimism.\nI was fucking around in git late one night and ran into merge conflicts. The first step when you get a merge conflict is to ask yourself “jesus christ, how did I keep this job?” which reminded me on this tweet from Vicky Boykis. Long story short, its a screen grab from a HN comment section in which some user laments how engineering gets instant errors while things like data science see errors pass silently.\nTune hyperparameters on the test set? Everyone else does it.\nRun experiments “until significance”? Have a promotion.\nRun a pre-post? If numbers are good, yay us! It numbers are bad, did you try data dreding until they look good?\nData Scientists are mentioned by name in David Graeber’s Bullshit Jobs and it is only now that I’ve begun to work (again) that I understand why. The irony is that because errors pass silently in data work, it is often difficult to assign blame or even to determine that someone should be blamed at all. You could be terrible at your job and keep it so long as you’re terrible in the right way. It makes me wonder why I put so much effort into being right in ways people don’t seem to care about. I should get an MBA or something."
  },
  {
    "objectID": "posts/2022-05-07-flippin/index.html",
    "href": "posts/2022-05-07-flippin/index.html",
    "title": "Flippin’ Fun!",
    "section": "",
    "text": "I wrote an answer to a question about sequences of coin flips a couple days back that I was quite chuffed with. In short, the question asked for statistical ways to determine if a sequence of coin flips was from an unbiased coin or a human trying to appear random. The resulting model turned into a fun game on twitter centered around determining if people who follow me could simulate a sequence of coin flips that looked random (without using a real RNG, or some funny workaround. I have a lot of faith in my twitter followers…maybe too much).\nAnyway, then I thought “I should fit a hierarchical model to this data”. So that’s what I’m doing"
  },
  {
    "objectID": "posts/2022-05-07-flippin/index.html#initial-model",
    "href": "posts/2022-05-07-flippin/index.html#initial-model",
    "title": "Flippin’ Fun!",
    "section": "Initial Model",
    "text": "Initial Model\nTo understand the hierarchical model, we first need to understand the model I initially built. Let me give you a quick rundown on that.\nLet \\(S\\) be a sequence of bernoulli experiments so that \\(S_i\\) is either 1 or a 0 (a heads or a tails if you wish). The question I answered concerns detecting if a given sequence \\(S\\) could have been created by a human (or by a non-random process posing as random). I interpreted that as a call to estimate the lag-1 autocorrelation of the flips. The hypothesis being that humans probably perceive long streaks of one outcome or the other as signs of non-randomness and will intentionally switch the outcome if they feel the run is too long. I initially chose a Bayesian approach because I’m a glutton for punishment and someone else already gave a pretty good answer.\nThe model is quite straight forward to write down. Let \\(\\rho\\) be the correlation between \\(S_i\\) and \\(S_{i+1}\\), and let \\(q\\) be the expected number of heads in the sequence. We can write down the conditional probabilities that we see a 0/1 given the last element in the sequence was a 1/0. Those conditional probabilities are derived here and they are…\n\\[ P(1 \\vert 1) = q + \\rho(1-q) \\]\n\\[ P(1 \\vert 0) = q(1-\\rho) \\]\n\\[ P(0 \\vert 1) = (1-q)(1-\\rho) \\]\n\\[ P(0 \\vert 0) = 1 - q + \\rho \\cdot q \\]\nThe trick is to then count the subsequences of (1, 1), (1, 0), (0, 1), and (0, 0). Let \\(p_{i\\vert j} = P(i \\vert j)\\). We can then consider the count of each subsequence as multinomial\n\\[ y \\sim \\mbox{Multinomial}(\\theta) \\&gt;. \\]\nHere, \\(\\theta\\) is the multinomial parameter, wherein each element is \\(\\theta = [p_{1 \\vert 1}, p_{1\\vert 0}, p_{0 \\vert 1}, p_{0\\vert 0}]\\). Equip this with a uniform prior on both \\(\\rho\\) and \\(q\\) and you’ve got yourself a model."
  },
  {
    "objectID": "posts/2022-05-07-flippin/index.html#the-stan-code",
    "href": "posts/2022-05-07-flippin/index.html#the-stan-code",
    "title": "Flippin’ Fun!",
    "section": "The Stan Code",
    "text": "The Stan Code\nThe Stan code for this model is quite easy to understand. The data block will consist of the counts of each type of subsequence. We can then concatenate those counts into an int of length 4 via the transformed data block. The concatenated counts will be what we pass to the multinomial likelihood.\ndata{\n  int y_1_1; //number of concurrent 1s\n  int y_0_1; //number of 0,1 occurences\n  int y_1_0; //number of 1,0 occurences\n  int y_0_0; //number of concurrent 0s\n}\ntransformed data{\n    int y[4] = {y_1_1, y_0_1, y_1_0, y_0_0};\n}\nThe only parameters we are interested in estimating are the autocorrelation rho and the coin’s bias q.\nparameters{\n  real&lt;lower=-1, upper=1&gt; rho;\n  real&lt;lower=0, upper=1&gt; q;\n}\nWe can derive the probabilities we need via the equations above, and that is a job for the transformed parameters block. We can then concatenate the conditional probabilities into a simplex data type object, theta to pass to the multinomial likelihood. Be careful though, we need to multiply theta by 0.5 since we are working with conditional probabilities. Note \\(p_{1\\vert 1} + p_{0 \\vert 1 } + p_{1\\vert 0} + p_{0 \\vert 0 } = 2\\), hence the scaling factor.\ntransformed parameters{\n  real&lt;lower=0, upper=1&gt; prob_1_1 = q + rho*(1-q);\n  real&lt;lower=0, upper=1&gt; prob_0_1 = (1-q)*(1-rho);\n  real&lt;lower=0, upper=1&gt; prob_1_0 = q*(1-rho);\n  real&lt;lower=0, upper=1&gt; prob_0_0 = 1 - q + rho*q;\n  simplex[4] theta = 0.5*[prob_1_1, prob_0_1, prob_1_0, prob_0_0 ]';\n}\nThe model call is then quite simple\nmodel{\n  q ~ beta(1, 1);\n  rho ~ uniform(-1, 1);\n  y ~ multinomial(theta); \n}\nand we can even generate new sequences based off the estimated parameters as a sort of posterior predictive check.\ngenerated quantities{\n    vector[300] yppc;\n    \n    yppc[1] = bernoulli_rng(q);\n    \n    for(i in 2:300){\n        if(yppc[i-1]==1){\n            yppc[i] = bernoulli_rng(prob_1_1);\n        }\n        else{\n        yppc[i] = bernoulli_rng(prob_1_0);\n        }\n    }\n}\nAll in all the model is\ndata{\n\n  int y_1_1; //number of concurrent 1s\n  int y_0_1; //number of 0,1 occurences\n  int y_1_0; //number of 1,0 occurences\n  int y_0_0; //number of concurrent 0s\n  \n}\ntransformed data{\n    int y[4] = {y_1_1, y_0_1, y_1_0, y_0_0};\n}\nparameters{\n  real&lt;lower=-1, upper=1&gt; rho;\n  real&lt;lower=0, upper=1&gt; q;\n}\ntransformed parameters{\n  real&lt;lower=0, upper=1&gt; prob_1_1 = q + rho*(1-q);\n  real&lt;lower=0, upper=1&gt; prob_0_1 = (1-q)*(1-rho);\n  real&lt;lower=0, upper=1&gt; prob_1_0 = q*(1-rho);\n  real&lt;lower=0, upper=1&gt; prob_0_0 = 1 - q + rho*q;\n  simplex[4] theta = 0.5*[prob_1_1, prob_0_1, prob_1_0, prob_0_0 ]';\n}\nmodel{\n  q ~ beta(1, 1);\n  rho ~ uniform(-1, 1);\n  y ~ multinomial(theta);\n  \n}\ngenerated quantities{\n    vector[300] yppc;\n    \n    yppc[1] = bernoulli_rng(q);\n    \n    for(i in 2:300){\n        if(yppc[i-1]==1){\n            yppc[i] = bernoulli_rng(prob_1_1);\n        }\n        else{\n        yppc[i] = bernoulli_rng(prob_1_0);\n        }\n    }\n}"
  },
  {
    "objectID": "posts/2022-05-07-flippin/index.html#run-from-python",
    "href": "posts/2022-05-07-flippin/index.html#run-from-python",
    "title": "Flippin’ Fun!",
    "section": "Run From Python",
    "text": "Run From Python\nWith the model written down, all we need to do is add some python code to create the counts of each subsequence and then run the stan model. Here is teh python code I used to create the response tweets for that game I ran on twitter.\n\nimport cmdstanpy\nimport matplotlib.pyplot as plt\n\ny_1_1 = 0 # count of (1, 1)\ny_0_0 = 0 # count of (0, 0)\ny_0_1 = 0 # count of (0, 1)\ny_1_0 = 0 # count of (1, 0)\n\nsequence = list('TTHHTHTTHTTTHTTTHTTTHTTHTHHTHHTHTHHTTTHHTHTHTTHTHHTTHTHHTHTTTHHTTHHTTHHHTHHTHTTHTHTTHHTHHHTTHTHTTTHHTTHTHTHTHTHTTHTHTHHHTTHTHTHHTHHHTHTHTTHTTHHTHTHTHTTHHTTHTHTTHHHTHTHTHTTHTTHHTTHTHHTHHHTTHHTHTTHTHTHTHTHTHTHHHTHTHTHTHHTHHTHTHTTHTTTHHTHTTTHTHHTHHHHTTTHHTHTHTHTHHHTTHHTHTTTHTHHTHTHTHHTHTTHTTHTHHTHTHTTT'.upper())\n\n# Do a rolling window trick I saw somewhere on twitter.\n# This implements a rollowing window of 2\n# In python 3.10, this would be a great use case for match\nfor pairs in zip(sequence[:-1], sequence[1:]):\n    if pairs == ('H','H'):\n        y_1_1 +=1\n    elif pairs == ('T','H'):\n        y_0_1 +=1\n    elif pairs == ('H', 'T'):\n        y_1_0 +=1\n    else:\n        y_0_0 +=1\n\n# Write the stan model as a string.  We will then write it to a file\nstan_code = '''\ndata{\n\n  int y_1_1; //number of concurrent 1s\n  int y_0_1; //number of 0,1 occurences\n  int y_1_0; //number of 1,0 occurences\n  int y_0_0; //number of concurrent 0s\n  \n}\ntransformed data{\n    int y[4] = {y_1_1, y_0_1, y_1_0, y_0_0};\n}\nparameters{\n  real&lt;lower=-1, upper=1&gt; rho;\n  real&lt;lower=0, upper=1&gt; q;\n}\ntransformed parameters{\n  real&lt;lower=0, upper=1&gt; prob_1_1 = q + rho*(1-q);\n  real&lt;lower=0, upper=1&gt; prob_0_1 = (1-q)*(1-rho);\n  real&lt;lower=0, upper=1&gt; prob_1_0 = q*(1-rho);\n  real&lt;lower=0, upper=1&gt; prob_0_0 = 1 - q + rho*q;\n  simplex[4] theta = 0.5*[prob_1_1, prob_0_1, prob_1_0, prob_0_0 ]';\n}\nmodel{\n  q ~ beta(1, 1);\n  rho ~ uniform(-1, 1);\n  y ~ multinomial(theta);\n  \n}\ngenerated quantities{\n    vector[300] yppc;\n    \n    yppc[1] = bernoulli_rng(q);\n    \n    for(i in 2:300){\n        if(yppc[i-1]==1){\n            yppc[i] = bernoulli_rng(prob_1_1);\n        }\n        else{\n        yppc[i] = bernoulli_rng(prob_1_0);\n        }\n    }\n}\n'''\n\n\n\n# Write the model to a temp file\nwith open('model_file.stan', 'w') as model_file:\n    model_file.write(stan_code)\n    \n# Compile the model\nmodel = cmdstanpy.CmdStanModel(stan_file='model_file.stan')\n\n# data to pass to Stan\ndata = dict(y_1_1 = y_1_1, y_0_0 = y_0_0, y_0_1 = y_0_1, y_1_0 = y_1_0)\n\n# Plotting stuff.\nfig, ax = plt.subplots(dpi = 120, ncols=2, figsize = (15, 5))\n\nax[0].set_title('Auto-correlation')\nax[1].set_title('Bias')\n\nax[0].set_xlim(-1, 1)\nax[1].set_xlim(0, 1)\n\nax[0].axvline(0, color = 'red')\nax[1].axvline(0.5, color = 'red')\n\nax[0].annotate('Uncorrelated Flips', xy=(0.475, 0.5), xycoords='axes fraction', rotation = 90)\nax[1].annotate('Unbiased Flips', xy=(0.475, 0.5), xycoords='axes fraction', rotation = 90)\n\n# MCMC go brrrr\nfit = model.sample(data)\n\nax[0].hist(fit.stan_variable('rho'), edgecolor='k', alpha = 0.5)\nax[1].hist(fit.stan_variable('q'), edgecolor='k', alpha = 0.5)\n\nautocorr = fit.stan_variable('rho').mean()\nbias = fit.stan_variable('q').mean()\n\ntweet = f\"Your flips have an expected correlation of {autocorr:.2f} and your coin's bias is about {bias:.2f}\"\n\nprint(f\"Your sequence was {''.join(sequence)}\")\nprint(tweet)\n\nINFO:cmdstanpy:compiling stan file /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file.stan to exe file /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file\n\n\nINFO:cmdstanpy:compiled model executable: /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file\n\n\nWARNING:cmdstanpy:Stan compiler has produced 1 warnings:\n\n\nWARNING:cmdstanpy:\n--- Translating Stan model to C++ code ---\nbin/stanc  --o=/Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file.hpp /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file.stan\nWarning in '/Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file.stan', line 11, column 4: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.32.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\n\n--- Compiling, linking C++ code ---\nclang++ -std=c++1y -Wno-unknown-warning-option -Wno-tautological-compare -Wno-sign-compare -D_REENTRANT -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.3.9 -I stan/lib/stan_math/lib/boost_1.75.0 -I stan/lib/stan_math/lib/sundials_6.0.0/include -I stan/lib/stan_math/lib/sundials_6.0.0/src/sundials    -DBOOST_DISABLE_ASSERTS          -c -include-pch stan/src/stan/model/model_header.hpp.gch -x c++ -o /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file.o /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file.hpp\nclang++ -std=c++1y -Wno-unknown-warning-option -Wno-tautological-compare -Wno-sign-compare -D_REENTRANT -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.3.9 -I stan/lib/stan_math/lib/boost_1.75.0 -I stan/lib/stan_math/lib/sundials_6.0.0/include -I stan/lib/stan_math/lib/sundials_6.0.0/src/sundials    -DBOOST_DISABLE_ASSERTS                -Wl,-L,\"/Users/demetri/.cmdstan/cmdstan-2.29.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/Users/demetri/.cmdstan/cmdstan-2.29.2/stan/lib/stan_math/lib/tbb\"      /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file.o src/cmdstan/main.o        -Wl,-L,\"/Users/demetri/.cmdstan/cmdstan-2.29.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/Users/demetri/.cmdstan/cmdstan-2.29.2/stan/lib/stan_math/lib/tbb\"   stan/lib/stan_math/lib/sundials_6.0.0/lib/libsundials_nvecserial.a stan/lib/stan_math/lib/sundials_6.0.0/lib/libsundials_cvodes.a stan/lib/stan_math/lib/sundials_6.0.0/lib/libsundials_idas.a stan/lib/stan_math/lib/sundials_6.0.0/lib/libsundials_kinsol.a  stan/lib/stan_math/lib/tbb/libtbb.dylib stan/lib/stan_math/lib/tbb/libtbbmalloc.dylib stan/lib/stan_math/lib/tbb/libtbbmalloc_proxy.dylib -o /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file\nrm -f /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file.o\n\n\n\nINFO:cmdstanpy:CmdStan start processing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \n\n\n                                                                                \n\n\n                                                                                \n\n\n                                                                                \n\n\nINFO:cmdstanpy:CmdStan done processing.\n\n\n\n\n\nYour sequence was TTHHTHTTHTTTHTTTHTTTHTTHTHHTHHTHTHHTTTHHTHTHTTHTHHTTHTHHTHTTTHHTTHHTTHHHTHHTHTTHTHTTHHTHHHTTHTHTTTHHTTHTHTHTHTHTTHTHTHHHTTHTHTHHTHHHTHTHTTHTTHHTHTHTHTTHHTTHTHTTHHHTHTHTHTTHTTHHTTHTHHTHHHTTHHTHTTHTHTHTHTHTHTHHHTHTHTHTHHTHHTHTHTTHTTTHHTHTTTHTHHTHHHHTTTHHTHTHTHTHHHTTHHTHTTTHTHHTHTHTHHTHTTHTTHTHHTHTHTTT\nYour flips have an expected correlation of -0.36 and your coin's bias is about 0.49"
  },
  {
    "objectID": "posts/2022-05-07-flippin/index.html#stack-layerswait-thats-a-deep-learning-thing",
    "href": "posts/2022-05-07-flippin/index.html#stack-layerswait-thats-a-deep-learning-thing",
    "title": "Flippin’ Fun!",
    "section": "Stack Layers…Wait, That’s a Deep Learning Thing",
    "text": "Stack Layers…Wait, That’s a Deep Learning Thing\nNow it’s time to write a hierarchical model, and for that we need to be a little more careful. I initially thought I could just put priors on the population level autocorrelation and bias, but I quickly ran into a problem there, which I will illustrate below.\nSuppose the coin’s bias is 0 (we always get tails). Could the autocorrelation be -1? No, it couldn’t be, because that would mean our next flip would have to be a 1, but the coin’s bias is 0! This illustrates some nuance to the problem I had failed to consider but luckily did not suffer from. The autocorrelation for two binary random variables, \\(X, Y\\), is defined as\n\\[ \\rho = \\dfrac{\\alpha - q}{q(1-q)} \\]\nWhere \\(E(XY) = \\alpha\\). You see, \\(\\alpha\\) can only be so big depending on the value of \\(q\\), and if you place uniform priors on both \\(\\rho\\) and \\(q\\) you can quickly get conditional probabilities outside the unit interval. That’s exactly what happened, and I was banging my head against the wall for a night trying to figure out the bounds on \\(\\rho\\) given \\(q\\) and it quickly became a mess.\nThere is another way. Rather than place priors on \\(\\rho\\) and \\(q\\), we could place priors on the multinomial parameter and then do algebra (two equations, two unknowns) to find out expressions for \\(q\\) and \\(\\rho\\) in terms of \\(p_{1\\vert1}\\) and \\(p_{1\\vert0}\\). This isn’t ideal, because I have very good prior information on what \\(\\rho\\) and \\(q\\) should be, not on what \\(\\theta\\) should be. Whatever, let’s proceed and see how our priors look like with a prior predictive check.\nThe model is actually simpler to write in Stan than the previous model. We will place a Dirichlet prior on the multinomial parameters (one for each person who responded with a sequence), and then each sequence is multinomial with that multinomial parameter.\n\\[ \\theta_j \\sim \\mbox{Dirichlet}(\\alpha) \\]\n\\[ y_j \\sim \\mbox{Multinomial}(\\theta_j)  \\]\nThe quantities we care about can be expressed in terms of \\(\\theta\\)\n\\[ \\rho = p_{1\\vert 1} - p_{1\\vert 0}  \\]\n\\[ q = \\dfrac{2p_{1\\vert 0}}{1 - p_{1\\vert 1} + p_{1\\vert 0}}\\]\nHere is the model in Stan\ndata{\n\n    int N; //How many sequences do we have\n    int y[N, 4]; // matrix of counts of co-occurences of (1,1), (1,0), (0, 1), (0,0)\n    int do_sample; // Flag to do a prior predictive check\n\n}\nparameters{\n    vector&lt;lower=0&gt;[4] a;\n    simplex[4] theta[N];\n}\nmodel{\n  \n  a ~ cauchy(0, 2.5);\n  \n  if(do_sample&gt;0){\n      for(i in 1:N){\n          theta[i] ~ dirichlet(a);\n          y[i] ~ multinomial(theta[i]);\n      }\n    }\n  \n}\ngenerated quantities{\n\n    vector[4] theta_ppc = dirichlet_rng(a);\n    real rho = theta_ppc[1] - theta_ppc[2];\n    real q = 2* theta_ppc[2]/(1 - theta_ppc[1] + theta_ppc[2]); \n    \n    real yppc[N, 4];\n    \n    for(i in 1:N){\n        yppc[i] = multinomial_rng(theta[i], sum(y[i]));\n    }\n}\nShown below are the priors for the autocorrelation and bias based on the priors I’ve used. They are a little too uncertain for my liking. Humans are pretty smart, and I don’t expect for the population average bias to be very far from 0.5. I would prefer that the prior for \\(q\\) be very tightly centered around 0.5, and that the prior for \\(\\rho\\) be tightly centered on 0, but that’s life. The model runs the 76 sequences in about 12 seconds (4 chains, 1000 warmups, 1000 samples) and diagnostics don’t indicate any pathological behavior. Let’s look at the joint posterior.\n\n\n\n\n\n\n\n\n\n\n\n(a) Priors\n\n\n\n\n\n\n\n\n\n\n\n(b) Posteriors\n\n\n\n\n\n\n\nFigure 1: Prior and Posterior distributions for the model.\n\n\n\nThe take home here is that the sequences are largely consistent with an unbiased and uncorrelated coin. The expected correlation is negative (-0.05) meaning humans are more likely to switch from heads to tails, or tails to heads, and the expected bias is 0.53 meaning people seem to favor heads for some reason. The results are largely unsurprising, and I really wish I could place priors on \\(\\rho\\) and \\(q\\) directly so that my model really does reflect the state of my knowledge, but this is good enough for now."
  },
  {
    "objectID": "posts/2022-05-07-flippin/index.html#conclusion",
    "href": "posts/2022-05-07-flippin/index.html#conclusion",
    "title": "Flippin’ Fun!",
    "section": "Conclusion",
    "text": "Conclusion\nI’ve been thinking about this problem for a while after I watched a talk by Gelman where he mentioned estimating the autocorrelation between bernoulli experiments in passing (he was talking about model criticism and offering examples of other things to check our model with). I’m moderately happy with the hierarchical model, and the results make a lot of sense. Humans are pretty smart, and we have an intuitive sense for what random looks like. I’m willing to bet that if people submitted longer sequences, we would have a more precise estimate of the bias/correlation.\nOne thing I’ve shirked is really detailed model criticism, though I have an idea of how I would do that. In that answer on cross validated, COOLSerdash posts a REALLY COOL way to visualize the sequence data. They plot the number of runs (sequences of consecutive heads or tails) against the longest run in the sequence. I think this would make for an excellent way to check that our model has learned the correct autocorrelation for each individual who participated in the game (though I think the sequences were too short to have a precise estimate)."
  },
  {
    "objectID": "posts/2018-08-31-combinatorics/index.html",
    "href": "posts/2018-08-31-combinatorics/index.html",
    "title": "Neat Litle Combinatorics Problem",
    "section": "",
    "text": "I’ll cut right to it. Consider the set \\(S = (49, 8, 48, 15, 47, 4, 16, 23, 43, 44, 42, 45, 46 )\\). What is the expected value for the minimum of 6 samples from this set?\nWe could always just sample form the set to estimate the expected value. Here is a python script to do just that.\n\nimport numpy as np\nx = np.array([49, 8, 48, 15, 47, 4, 16, 23, 43, 44, 42, 45, 46])\n\nmins = []\nfor _ in range(1000):\n    mins.append(np.random.choice(x,size = 6, replace = False).min())\n\nprint(np.mean(mins))\n\n9.098\n\n\nBut that is estimating the mean. We can do better and directly compute it. Here is some python code to create all subsets from \\(S\\) of size 6. Then, we simply take out the minimum from each subset and compute the mean.\n\nimport numpy as np\nfrom itertools import combinations, groupby\n\nx = np.array([49, 8, 48, 15, 47, 4, 16, 23, 43, 44, 42, 45, 46])\nx = np.sort(x)\n\nc = list(combinations(x,6))\n\nmins = list(map(lambda x: x[0], c))\n\ns = 0\nfor k, g in groupby(sorted(mins)):\n    s+=k*(len(list(g))/len(mins))\n\nprint( s )\n\n8.818181818181818\n\n\nThe script returns 8.18 repeating. Great, but we can do even better! If we can compute the probability density function, we can compute the mean analytically. Let’s consider a smaller problem to outline the solution.\nLet our set in question be \\((1,2,3,4,5)\\). Let the minimum of a sample of 3 numbers from this set be the random variable \\(z\\). Now, note there are \\(\\binom{5}{3} = 10\\) ways to choose 3 elements from a set of 5.\nHow many subsets exist where the minimum is 1? Well, if I sampled 1, then I would still have to pick 2 numbers from a possible 4 numbers larger than 1. There are \\(\\binom{4}{2}\\) ways to do this. So \\(p(z=1) = \\binom{4}{2} / \\binom{5}{3}\\).\nIn a similar fashion, there are \\(\\binom{3}{2}\\) subsets where 2 is the minimum, and \\(\\binom{2}{2}\\) subsets where 3 is the minimum. There are no subsets where 4 or 5 are the minimum (why?). So that means the expected minimum value for this set would be\n\\[\\operatorname{E}(z) = \\dfrac{ \\sum_{k = 1}^{3} k\\binom{5-k}{2} }{\\binom{5}{3}}  \\]\nWhatever that sum happens to be. Here is how you could code up the analytic solution to our problem.\n\nimport numpy as np\nfrom scipy.special import binom\n\nx = np.array([ 4, 8, 15, 16, 23, 42, 43, 44, 45, 46, 47, 48, 49])\nx = np.sort(x)\n\nsample_size =6\nsample_space = x[:-(sample_size-1)]\nE = 0\nfor i,s in enumerate(sample_space,start = 1):\n\n    E+= s*binom(x.size-i,sample_size-1)\n\nprint(E/binom(x.size, sample_size))\n\n8.818181818181818\n\n\nFull disclosure, this was on a job application (literally, on the job application), so sorry KiK for putting the answer out there, but the question was too fun not to write up!"
  },
  {
    "objectID": "posts/2024-01-22-curse/index.html",
    "href": "posts/2024-01-22-curse/index.html",
    "title": "The Winner’s Curse Is Easy To Understand From This Picture",
    "section": "",
    "text": "Take a look at the photo below, and it i should be easy to understand why The Winner’s Curse (the general tendency for detected effects to be an over estimate of the truth) is a thing.\nThe plot shows our typical setup for a hypothesis test. In black is the sampling distribution of the test statistic for a difference in means under the null, and in blue is the statistic’s sampling distribution under the alternative. The shaded blue region represents the statistical power, and those effect sizes in the shaded region would be considered “statistically significant”.\n\n\nCode\nx &lt;- seq(-5, 10, 0.1)\nxs &lt;- seq(1.96, 10, 0.1)\nplot(x, dnorm(x), type='l', ylab='', xlab = 'Difference in means', xaxt='n', yaxt='n')\nlines(x, dnorm(x, 3, 1), type='l', col='blue')\npolygon(\n  c(xs, rev(xs)),\n  c(dnorm(xs, 3, 1), rep(0, length(xs))),\n  col=alpha('blue', 0.3),\n  border = F\n)\n\n\n\n\n\n\n\n\n\nThe shaded blue region defines a lower truncated normal distribution. Were the alternative hypothesis true, and were we to run many experiments to estimate the difference in means, our detected effects would be samples from this distribution. Hence, using those samples to estimate the mean of the alternative distribution would result in a biased estimate of the mean.\nThe expectation for a lower truncated normal distribution truncated at \\(x=a\\) is\n\\[\\mu+\\sigma  \\dfrac{\\varphi\\left( \\dfrac{a-\\mu}{\\sigma} \\right)}{1 - \\Phi\\left(\\dfrac{a-\\mu}{\\sigma}\\right)}   \\&gt;.\\]\nHere \\(\\mu\\) and \\(\\sigma\\) are the mean and standard deviation of the distribution under the alternative, \\(\\varphi\\) is the standard normal density, \\(\\Phi\\) is the standard normal cumulative distribution. So our estimate of the mean would be biased upwards by \\(\\sigma  \\frac{\\varphi\\left( \\frac{a-\\mu}{\\sigma} \\right)}{1 - \\Phi\\left(\\frac{a-\\mu}{\\sigma}\\right)}\\). That can be small or large depending on \\(\\mu\\) and \\(\\sigma\\).\nWithout doing any differentiation to understand where this bias is largest, it should be intutituve from the picture that the bias is small/large when statistical power is large/small.\nHence, estimates from underpowered studies should be met with &lt;fry_squint.jpg&gt;."
  },
  {
    "objectID": "posts/2025-03-17-bayes-big-deal/index.html",
    "href": "posts/2025-03-17-bayes-big-deal/index.html",
    "title": "More on Bayesian Statistics",
    "section": "",
    "text": "I recently wrote a blog post for my employer in which I made some claims regarding Bayesian statistics.\n\nClaim 1: Flat Priors in Heirarchal Models Can Lead to Pulling Apart and Implausible Estimates\nOn flat priors in hierarchical models, Gelman says\n\nA noninformative uniform prior on the coefficients is equivalent to a hierarchical N(0,tau^2) model with tau set to a very large value. This is a very strong prior distribution pulling the estimates apart, and the resulting estimates of individual coefficients are implausible.\n\nBy “pulling the estimates apart” I assume Gelman means “not pooling/regularizing estimates” as opposed to literally pulling the estimates apart. As for the line about “implausibility” of individual coefficients, that requires some unpacking.\nGelman is known for picking out poor methodological papers and commenting on them. Consider “The prior can often only be understood in the context of the likelihood” in which Gelman and authors discuss a study which ostensibly demonstrated that “beautiful” people are more likely to have a daughter.\nYou’re free to read what Gelman and authors wrote, I won’t summarize it here, but in short the authors to which Gelman refers report an unusually high differential in the probability of giving birth to a girl if you are a “beautiful” person. Given what we know about the sex ratio of births and how they vary across the globe, this seems fairly extreme. Reading between the lines, I would go so far as to say Gelman would consider this estimate as “implausible”. Hence, I interpret “implausible” to mean “no different than Frequentist estimates”. That is not to say that all frequentist estimates are implausible, just that when certain estimates should be shrunk/regularized, a hierarchical model with a flat prior would fail to do so.\nThis is fairly easily demonstrated. I’m going to simulate 16 A/B tests. Each test is powered to detect a lift of 5%. However, each of the lifts from the 16 tests is drawn from a normal distribution with mean 1 and standard deviation 0.02. This means that, while not impossible to see a 5% lift, it is highly improbable and often our tests are underpowered for the true effect. This, as I understand it, is the prevailing attitude in A/B testing.\nEach A/B test will be fed to a hierarchical model (shown below in Stan). I place a flat prior on the population mean and parameterize the prior on the population variance so that I can study how estimates change when the prior is less/more variable.\n\n\n\ndata{\n  int n;\n  vector[n] log_rr;\n  vector[n] stderr;\n  real beta;\n}\nparameters{\n  real mu;\n  real&lt;lower=0&gt; tau;\n  vector[n] z;\n}\ntransformed parameters {\n  vector[n] theta = mu + tau * z;\n}\nmodel {\n  // No mu in the model block means a uniform prior over the domain.\n  tau ~ inv_gamma(3, beta);\n  z ~ std_normal();\n  log_rr ~ normal(theta, stderr);\n}\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(cmdstanr)\nlibrary(tidybayes)\n\nset.seed(0)\n\ncontrol_conversion &lt;- 0.1\ntarget_lift &lt;- 1.05\ntarget_conversion &lt;- control_conversion * target_lift\ndesign &lt;- power.prop.test(p1 = control_conversion, p2 = target_conversion, power = 0.8)\nn_per_group &lt;- ceiling(design$n)\nn_experiments &lt;- 16\n\nresults &lt;- replicate(n_experiments, {\n  # In reality, we generate smaller effects than we wish we did\n  actual_lift &lt;- rnorm(1, 1, 0.02)\n  actual_log_rr &lt;- log(actual_lift)\n  \n  control &lt;- rbinom(1, n_per_group, control_conversion)\n  treatment &lt;- rbinom(1, n_per_group, control_conversion * actual_lift)\n  \n  # Since the groups are the same size the two factors of log(n_per_group) cancel\n  observed_log_rr &lt;- log(treatment) - log(control)\n  # A Standard result from the Delta method.\n  sampling_var &lt;- 1 / treatment + 1 / control - 2 / n_per_group\n  stderr &lt;- sqrt(sampling_var)\n  \n  c(observed_log_rr, stderr, actual_log_rr)\n  \n})\n\nlog_rr &lt;- results[1, ]\nstderr &lt;- results[2, ]\nactual_log_rr &lt;- results[3, ]\n\n# Keep track of the RMSE from our frequentist estimates.\n# Hopefully, the Hierarchical model will do better than this.\nobserved_rmse &lt;- sqrt(mean((log_rr - actual_log_rr) ^ 2))\n\nmodel &lt;- cmdstan_model('model.stan')\n\nbeta &lt;- 10 ^ seq(-3, 4, 0.5)\nsims &lt;- map_dfr(beta, ~ {\n  stan_data &lt;- list(\n    n = n_experiments,\n    log_rr = log_rr,\n    stderr = stderr,\n    beta = .x\n  )\n  \n  capture.output(\n  fit &lt;- model$sample(\n    stan_data,\n    parallel_chains = 10,\n    adapt_delta = 0.99,\n    chains = 10,\n    refresh = 0, \n    seed=0\n  )\n  )\n  \n  \n  estimated &lt;- fit$summary('theta')$mean\n  error &lt;- actual_log_rr - estimated\n  rmse &lt;- sqrt(mean(error ^ 2))\n  max_rhat &lt;- max(posterior::summarise_draws(fit$draws('theta'), 'rhat')$rhat)\n  divs &lt;- sum(fit$diagnostic_summary()$num_divergent)\n  \n  tibble(rmse, beta = .x, max_rhat, divs)\n  \n})\n\nstopifnot(all(sims$divs==0))\nstopifnot(all(sims$max_rhat&lt;1.01))\n\n\nBelow is a plot of the relative RMSE of the estimates from the Bayesian model. I’ve rescaled the y axis so that the black line (y=1) corresponds to the RMSE from the Frequentist estimates. As you can see, more informative priors do result in better RMSE – to a point. When the prior becomes too informative, then the RMSE increases. This is due to pooling all estimates to the grand mean. Conversely, when the prior on the variance is too large, no pooling happens and results are the same as Frequentist estimates (i.e. implausible if they are noisy a la Gelman).\n\n\n\n\n\n\n\n\n\nSo, let’s take stock. From this little simulation – which adheres the best I can to what I know about A/B testing, and is still imperfect – we have seen:\n\nIt is true that uninformative priors will give you equivalent estimates to Frequentist approaches (at least in terms of RMSE).\nYou probably don’t want to be similar to Frequentist results if you can help it, because you can do better (again in terms of RMSE) by applying a little prior information.\nYou don’t even need to be that accurate with your priors. Any reduction in RMSE is good, you don’t need to find the optimal prior to reduce the RMSE. In this case, anywhere between \\(\\beta = 0.05\\) and \\(\\beta = 3\\) is probably sufficient to reduce RMSE.\nSetting a good prior (in this case) isn’t even that hard. Think about how variable individual experiments vary in results. You aren’t likely to see many experiments 1 lift unit away from the mean. You know, intuitively, that lift estimates are probably somewhere between -10% and 10% on a good day. Even if you were REALLY uncertain and said -50% to 50% for any given experiment, that is still a standard deviation of ~25% and hence well within the range of reducing RMSE (again, in this example. I can’t say if this applies generally).\n\nListen, Bayesian or Frequentist, I don’t care. However, it is absolutely clear that, when you pay careful attention to the nuance, the difference between Bayes and Frequentism is a big deal despite what some will say."
  },
  {
    "objectID": "posts/2022-06-22-new-blog/index.html",
    "href": "posts/2022-06-22-new-blog/index.html",
    "title": "This Is A Quarto Blog",
    "section": "",
    "text": "This is a quarto blog.\nThat means I can write code in either R or python directly in the blog post and have it execute. So when you see something like\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nt = np.linspace(0, 1)\nplt.plot(t, np.sin(2*np.pi*t), color='k')\nplt.plot(t, np.cos(2*np.pi*t), color='red')\nplt.title(\"Here is a plot\")\n\n\n\n\n\n\n\nFigure 1: Whoa, check it out!\n\n\n\n\n\nThat is the code that is actually executed. That means the blog is more reproducible and will have fewer errors. It also means you can go directly to the repo for my blog and clone the post to start tinkering. No more linking to other gitrepos, no more copying and pasting code with errors.\nDid I mention I can write both R and python?\n\nt = seq(0, 1, 0.01)\nplot(t, sin(2*pi*t), main='Here is another plot!', xlab='', ylab='', type='l')\nlines(t, cos(2*pi*t), col='red')\n\n\n\n\n\n\n\nFigure 2: Whoa, check it out again!\n\n\n\n\n\nI can also reference figures (like Figure 2 and Figure 1)"
  },
  {
    "objectID": "posts/2024-01-20-logistic-survival/index.html",
    "href": "posts/2024-01-20-logistic-survival/index.html",
    "title": "Survival Analysis With Logistic Regression",
    "section": "",
    "text": "This question came up on cross validated. In the course of responding to my answer, OP asks “why can’t I adjust for exposure length in a logistic regression” which kind of got me scratching my head. Seems like it could be done.\nI brought the question to twitter, and Dr. Ellie Murray responded reminding me that time to death and death are different outcomes and should be treated differently. A logistic regression could be done, depending on what you’re trying to model.\nFinally, Frank Harrell responded to Dr. Murray, with a link to a paper by Brad Efron (because of course Efron wrote something on this) which showed how this can be done quite elegantly.\nLet’s see how we can use logistic regression for survival analysis."
  },
  {
    "objectID": "posts/2024-01-20-logistic-survival/index.html#the-gist",
    "href": "posts/2024-01-20-logistic-survival/index.html#the-gist",
    "title": "Survival Analysis With Logistic Regression",
    "section": "The Gist",
    "text": "The Gist\nRecall that the Kaplan-Meir product limit estimator looks like\n\\[ \\widehat{S}_{KM} (t) = \\prod_{i: t_i \\leq t} \\left(1-\\dfrac{d_i}{n_i} \\right) \\&gt;. \\]\nHere, \\(d_i\\) is the number of subjects who experience the outcome at time \\(t_i\\), and \\(n_i\\) is the number of individuals known to have survived up to time \\(t_i\\). In essence, the product limit estimator is a product of a sequence of probabilities. What else do we use to model probabilities? Logistic regression.\nEfron writes that the main assumption for using logistic regression to model these probabilities is that the number of events \\(d_i\\) is binomial given \\(n_i\\)\n\\[ d_i \\mid n_i \\sim \\operatorname{Binomial}(h_i; n_i) \\&gt;. \\]\nHere, \\(h_i\\) is the discrete hazard rate (i.e. the probability the subject experiences the outcome during the \\(i^{th}\\) interval given the subject has survived until the beginning of the \\(i^{th}\\) interval).\nThe main problem is that the hazard may not be linear in the exposure time. Efron uses splines to allow \\(h_i\\) to be flexible in exposure time, and then computes the estimated survival function using\n\\[ \\tilde{S}(t) =  \\prod_{i: t_i \\leq t} \\left(1-h_i\\right)\\]\nAs an algorithm, the steps might look like:\n\nStructure your data so that you have number at risk, number of events, and number of censoring events as columns\nFit a logistic regression on the event/at risk columns (in R this is done by making cbind(y, n-y) the outcome in the glm call). Ensure the time variable is modeled flexibly (either with splines or something more exotic).\nPredict the risk of death (i.e. the discrete hazard) from the logistic regression on the observed event times.\nTake the cumulative product of one minus the discrete hazards. These are the survival function estimates."
  },
  {
    "objectID": "posts/2024-01-20-logistic-survival/index.html#replicating-efrons-example",
    "href": "posts/2024-01-20-logistic-survival/index.html#replicating-efrons-example",
    "title": "Survival Analysis With Logistic Regression",
    "section": "Replicating Efron’s Example",
    "text": "Replicating Efron’s Example\nEfron motivates this approach using survival data from a Head-and-Neck cancer study conducted by the Northern California Oncology Group. I’ve extracted this data to replicate the method.\nFirst, let’s show the Kaplan-Meir estimates\n\n\nCode\nlibrary(tidyverse)\nlibrary(survival)\nlibrary(splines)\n\nsource(\"make_data.R\")\n\nsd &lt;- make_data('survival')\nsfit &lt;- survfit(Surv(month, event) ~ strata(arm), data=sd, weights = wt)\n\nsfit_tidy &lt;- broom::tidy(sfit) %&gt;% \n             mutate(strata = str_remove(strata, fixed('strata(arm)=')))\n\n\nbase_plot &lt;- sfit_tidy %&gt;% \n  ggplot(aes(time, estimate, color=strata, shape=strata)) + \n  geom_point(size=1) + \n  theme_minimal() + \n  scale_color_brewer(palette = 'Set1') +\n  scale_y_continuous(labels = scales::percent) + \n  labs(x='Month', y='Survival Probability', color='Arm', shape='Arm', title='Kaplan-Meier Estimated Survival Curves for Head-and-Neck Cancer Study')\n\n\nbase_plot + \n  geom_step(linetype='dashed')\n\n\n\n\n\n\n\n\n\nThe intial model presented expands time in the following basis functions\n\\[ x_i = (1, t_i, (t_i-11)_-^{2}, (t_i-11)_-^{3}) \\]\nHere, \\(t_i\\) is the the midpoint between months. This is equivalent to estimating the risk of the outcome between the start of month \\(i\\) and the start of month \\(i+1\\). The function \\(( z )_- = \\min(z, 0)\\). This particular expansion allows \\(t_i\\) to vary as a cubic function before \\(t_i=11\\), and then as a linear function thereafter.\nAfter fitting the model with an interaction by arm (so that the hazards can differ by arm), we can plot the hazards readily 1.\n\n\nCode\nd &lt;- make_data('logistic') %&gt;% \n     mutate(tm = month - 0.5, \n            txt = as.integer(arm=='A'))\n\n\nf &lt;- function(x, d) pmin((x), 0)\n\nfit &lt;- glm(cbind(s, n-s) ~ arm * (tm + I(f(tm-11)^2) + I(f(tm-11)^3)),\n       data=d, \n       family = binomial())\n\npreds &lt;- d %&gt;% \n  bind_cols(predict(fit, newdata=., se.fit=T)) %&gt;% \n  mutate(\n    p = plogis(fit),\n    p.low = plogis(fit - 2*se.fit),\n    p.high = plogis(fit + 2*se.fit)\n  ) %&gt;% \n  group_by(arm) %&gt;% \n  arrange(arm, month) %&gt;% \n  mutate(S = cumprod(1-p),\n         S.low = cumprod(1-p.low),\n         S.high = cumprod(1-p.high))\n\n\npreds %&gt;% \n  ggplot(aes(tm, p, color=arm)) +\n  geom_line() + \n  coord_cartesian(xlim = c(0, 50)) + \n  theme_minimal() + \n  scale_color_brewer(palette = 'Set1') +\n  scale_y_continuous(labels = scales::percent) + \n  labs(x='Month', y='Hazard', color='Arm', shape='Arm', title='Hazard (Risk Estimate From Logistic Regression)')\n\n\n\n\n\n\n\n\n\nAnd we can also plot the estimated survival function against the Kaplan Meier to compare.\n\n\nCode\nbase_plot + \n  geom_line(data=preds, aes(month, S, color=arm), inherit.aes = F) + \n  labs(title='Kaplan-Meier as Compared to Logistic Regression')"
  },
  {
    "objectID": "posts/2024-01-20-logistic-survival/index.html#final-thoughts",
    "href": "posts/2024-01-20-logistic-survival/index.html#final-thoughts",
    "title": "Survival Analysis With Logistic Regression",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nWhy would we want to parametrically model the hazard when Kaplan-Meir can’t misspecify the hazard? What do we gain from this? In a word: efficiency , at least according to Efron. Additionally, if we are willing to make assumptions about how the hazard evolves into the future (e.g. linearly) then we can use this approach to forecast survival beyond the last observed timepoint.\nIn his paper, Efron has some notes on computing standard errors for this estimator. Its fairly dense, and I haven’t included confidence intervals here lest I naively compute them. That’s a topic for a future blog post."
  },
  {
    "objectID": "posts/2024-01-20-logistic-survival/index.html#footnotes",
    "href": "posts/2024-01-20-logistic-survival/index.html#footnotes",
    "title": "Survival Analysis With Logistic Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI Just can’t get the hazard’s to look the same as Figure 2. Might be an error copying over the data.↩︎"
  },
  {
    "objectID": "posts/2021-11-23-bootstrap/index.html",
    "href": "posts/2021-11-23-bootstrap/index.html",
    "title": "Hacking Sklearn To Do The Optimism Corrected Bootstrap",
    "section": "",
    "text": "Its late, I can’t sleep, so I’m writing a blog post about the optimism corrected bootstrap.\nIn case you don’t know, epidemiology/biostatistics people working on prediction models like to validate their models in a slightly different way than your run-in-the-mill data scientist. Now, it should be unsurprising that this has generated some discussion between ML people and epi/biostats people, but I’m going to ignore this for now. I’m going to assume you have good reason for wanting to do the optimism corrected bootstrap in python, especially with sklearn, and if you don’t and want to discuss the pros and cons fo the method instead then lalalalalala I can’t hear you."
  },
  {
    "objectID": "posts/2021-11-23-bootstrap/index.html#the-optimism-corrected-bootstrap-in-7-steps",
    "href": "posts/2021-11-23-bootstrap/index.html#the-optimism-corrected-bootstrap-in-7-steps",
    "title": "Hacking Sklearn To Do The Optimism Corrected Bootstrap",
    "section": "The Optimism Corrected Bootstrap in 7 Steps",
    "text": "The Optimism Corrected Bootstrap in 7 Steps\nAs a primer, you might want to tread Alex Hayes’ pretty good blog post about variants of the bootstrap for predictive performance. It is more mathy than I care to be right now and in R should that be your thing.\nTo do the optimism corrected bootstrap, follow these 7 steps as found in Ewout W. Steyerberg’s Clinical Prediction Models.\n\nConstruct a model in the original sample; determine the apparent performance on the data from the sample used to construct the model.\nDraw a bootstrap sample (Sample*) with replacement from the original sample.\nConstruct a model (Model) in Sample, replaying every step that was done in the original sample, especially model specification steps such as selection of predictors. Determine the bootstrap performance as the apparent performance of Model* in Sample.\nApply Model* to the original sample without any modification to determine the test performance.\nCalculate the optimism as the difference between bootstrap performance and test performance.\nRepeat steps 1–4 many times, at least 200, to obtain a stable mean estimate of the optimism.\nSubtract the mean optimism estimate (step 6) from the apparent performance (step 1) to obtain the optimism-corrected performance estimate.\n\nThis procedure is very straight forward, and could easily be coded up from scratch, but I want to use as much existing code as I can and put sklearn on my resume, so let’s talk about what tools exist in sklearn to do cross validation and how we could use them to perform these steps."
  },
  {
    "objectID": "posts/2021-11-23-bootstrap/index.html#cross-validation-in-sklearn",
    "href": "posts/2021-11-23-bootstrap/index.html#cross-validation-in-sklearn",
    "title": "Hacking Sklearn To Do The Optimism Corrected Bootstrap",
    "section": "Cross Validation in Sklearn",
    "text": "Cross Validation in Sklearn\nWhen you pass arguments like cv=5 in sklearn’s many functions, what you’re really doing is passing 5 to sklearn.model_selection.KFold. See sklearn.model_selection.cross_validate which calls a function called ‘check_cv’ to verify this. KFold.split returns a generator, which when passed to next yields a pair of train and test indicides. The inner workings of KFold might look something like\nfor _ in range(number_folds):\n    train_ix = make_train_ix()\n    test_ix = make_test_ix()\n    yield (trian_ix, test_ix)\nThose incidies are used to slice X and y to do the cross validation. So, if we are going to hack sklearn to do the optimisim corrected bootstrap for us, we really just need to write a generator to give me a bunch of indicies. According to step 2 and 3 above, the train indicies need to be resamples of np.arange(len(X)) (ask yourself “why?”). According to step 4, the test indicies need to be np.arnge(len(X)) (again….”why?“).\nOnce we have a generator to do give us our indicies, we can use sklearn.model_selection.cross_validate to fit models on the resampled data and predict on the original sample (step 4). If we pass return_train_score=True to cross_validate we can get the bootstrap performances as well as the test performances (step 5). All we need to do then is calculate the average difference between the two (step 6) and then add this quantity to the apparent performance we got from step 1.\nThat all sounds very complex, but the code is decieptively simple."
  },
  {
    "objectID": "posts/2021-11-23-bootstrap/index.html#the-code-i-know-you-skipped-here-dont-lie",
    "href": "posts/2021-11-23-bootstrap/index.html#the-code-i-know-you-skipped-here-dont-lie",
    "title": "Hacking Sklearn To Do The Optimism Corrected Bootstrap",
    "section": "The Code (I Know You Skipped Here, Don’t Lie)",
    "text": "The Code (I Know You Skipped Here, Don’t Lie)\n\nimport numpy as np\nfrom numpy.core.fromnumeric import mean\nfrom sklearn.model_selection import cross_validate, RepeatedKFold\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.utils import resample\n\n# Need some data to predict with\ndata = load_diabetes()\nX, y = data['data'], data['target']\n\nclass OptimisimBootstrap():\n\n    def __init__(self, n_bootstraps):\n\n        self.n_bootstraps = n_bootstraps\n\n    def split(self, X, y,*_):\n\n        n = len(X)\n        test_ix = np.arange(n)\n\n        for _ in range(self.n_bootstraps):\n            train_ix = resample(test_ix)\n            yield (train_ix, test_ix)\n\n# Optimism Corrected\nmodel = LinearRegression()\nmodel.fit(X, y)\napparent_performance = mean_squared_error(y, model.predict(X))\n\nopt_cv = OptimisimBootstrap(n_bootstraps=250)\nmse = make_scorer(mean_squared_error)\ncv = cross_validate(model, X, y, cv=opt_cv, scoring=mse, return_train_score=True)\noptimism = cv['test_score'] - cv['train_score']\noptimism_corrected = apparent_performance + optimism.mean()\nprint(f'Optimism Corrected: {optimism_corrected:.2f}')\n\n# Compare against regular cv\ncv = cross_validate(model, X, y, cv = 10, scoring=mse)['test_score'].mean()\nprint(f'regular cv: {cv:.2f}')\n\n# Compare against repeated cv\ncv = cross_validate(model, X, y, cv = RepeatedKFold(n_splits=10, n_repeats=100), scoring=mse)['test_score'].mean()\nprint(f'repeated cv: {cv:.2f}')\n\nOptimism Corrected: 2999.04\nregular cv: 3000.38\n\n\nrepeated cv: 3009.02\n\n\nThe three estimates (optimism corrected, 10 fold, and repeated 10 fold) should be reasonably close together, but uh don’t run this code multiple times. You might see that the optimism corrected estimate is quite noisy meaning I’m either wrong or that twitter thread I linked to might have some merit."
  },
  {
    "objectID": "posts/2022-11-16-bootstrapping-in-sql/index.html",
    "href": "posts/2022-11-16-bootstrapping-in-sql/index.html",
    "title": "Bootstrapping in SQL",
    "section": "",
    "text": "Remember the “Double Down” from KFC? It was bacon and cheese sandwiched between two deep fried pieces of chicken. I’m willing to bet we all conceived of it independently (as in “LOL wouldn’t it be crazy if…”), realistically could have made it ourselves, but were smart enough not to because “sure we could but… why?”.\nThis blog post is the Double Down of Statistics."
  },
  {
    "objectID": "posts/2022-11-16-bootstrapping-in-sql/index.html#introduction",
    "href": "posts/2022-11-16-bootstrapping-in-sql/index.html#introduction",
    "title": "Bootstrapping in SQL",
    "section": "",
    "text": "Remember the “Double Down” from KFC? It was bacon and cheese sandwiched between two deep fried pieces of chicken. I’m willing to bet we all conceived of it independently (as in “LOL wouldn’t it be crazy if…”), realistically could have made it ourselves, but were smart enough not to because “sure we could but… why?”.\nThis blog post is the Double Down of Statistics."
  },
  {
    "objectID": "posts/2022-11-16-bootstrapping-in-sql/index.html#bootstrapping-in-sql.-no-really.",
    "href": "posts/2022-11-16-bootstrapping-in-sql/index.html#bootstrapping-in-sql.-no-really.",
    "title": "Bootstrapping in SQL",
    "section": "Bootstrapping in SQL. No, Really.",
    "text": "Bootstrapping in SQL. No, Really.\nTwo things which have made my stats like easier:\n\nBootstrapping, and\nTidy data\n\nR’s rsample::bootstraps seems to do one in terms of the other. Take a look at the output of that function. We have, in essence, one bootstrapped dataset per row.\n\nlibrary(tidyverse)\nlibrary(rsample )\n\nrsample::bootstraps(cars)\n\n# Bootstrap sampling \n# A tibble: 25 × 2\n   splits          id         \n   &lt;list&gt;          &lt;chr&gt;      \n 1 &lt;split [50/18]&gt; Bootstrap01\n 2 &lt;split [50/16]&gt; Bootstrap02\n 3 &lt;split [50/16]&gt; Bootstrap03\n 4 &lt;split [50/18]&gt; Bootstrap04\n 5 &lt;split [50/21]&gt; Bootstrap05\n 6 &lt;split [50/19]&gt; Bootstrap06\n 7 &lt;split [50/20]&gt; Bootstrap07\n 8 &lt;split [50/18]&gt; Bootstrap08\n 9 &lt;split [50/20]&gt; Bootstrap09\n10 &lt;split [50/19]&gt; Bootstrap10\n# … with 15 more rows\n\n\nIn theory, I could unnest this and have one observation from each bootstrap per row, with id serving as an indicator to tell me to which resample the observation belongs to. Which means…I could theoretically bootstrap in SQL.\nSo, let’s do that. I’m going to use duckdb because its SQL-like and has some stats functions (whereas SQLite does not).\nLet’s sample some pairs \\((x_i, y_i)\\) from the relationship \\(y_i = 2x_i + 1 + \\varepsilon_i\\), where the \\(\\varepsilon\\) are iid draws from a standard Gaussian Let’s stick that in a dataframe along with a row number column into our database. The data are shown in Table 1.\n\n\n\n\nTable 1: My Data\n\n\n\n\n\n\noriginal_rownum\nx\ny\n\n\n\n\n1\n2.09\n5.73\n\n\n2\n0.97\n3.58\n\n\n3\n0.70\n1.37\n\n\n4\n1.44\n4.84\n\n\n5\n2.07\n4.14\n\n\n6\n1.67\n3.09\n\n\n\n\n\n\n\n\n\n\nTo bootstrap in SQL, we need to emulate what the unnested results of rsample::bootstraps would look like. We need rows of (strap_id, original_data_rownum, and bootstrap_rownum). Let’s discuss the interpretation and purpose of each column.\n\nstrap_id plays the part of id in rsample::bootstraps. We’re just going to group by this column and aggregate the resampled data later.\noriginal_data_rownum doesn’t really serve a purpose. It contains integers 1 through \\(N\\) (where \\(N\\) is our original sample size). We can do a cross join to get pairs (strap_id, original_data_rownum). This means there will be \\(N\\) copies of strap_id, meaning we can get \\(N\\) resamples of our data for each strap_id.\nbootstrap_rownum is a random integer between 1 and \\(N\\). This column DOES serve a purpose, its basically the sampling with replacement bit for the bootstrap. Now, duckdb doesn’t have a function to sample random integers. To do this, I basically sample random numbers on the unit interval do some arithmetic to turn those into integers.\n\nLet’s set that up now. The hardest part really is creating a sequence of numbers, but duckdb makes that pretty easy.\n\nQuery To Make strap_id\n\n-- Set up strap_ids in a table\nCREATE OR REPLACE TABLE strap_ids(strap_id INTEGER);\n-- Do 1000 bootstraps\nINSERT INTO strap_ids(strap_id) select * from range(1, 1001, 1);\n\n\n\n\n\nTable 2: Contents of strap_ids. These play the role of id in the rsample output.\n\n\n\n\n\n\nstrap_id\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n\n\n\n\n\n\n\n\n\n\nQuery To Make original_data_rownum\n\n-- Set up original_data_rownum in a table\nCREATE OR REPLACE TABLE original_data_rownum(original_rownum INTEGER);\n-- I have 2500 observations in my data\nINSERT INTO original_data_rownum(original_rownum) select * from range(1, 2500+1, 1);\n\n\n\n\n\nTable 3: Contents of original_data_rownum. These play the role of id in the rsample output.\n\n\n\n\n\n\noriginal_rownum\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n\n\n\n\n\n\n\n\nOk, now we have the two tables strap_ids and original_data_rownum. All we need to do now is cross join then, and do the random number magic. That’s shown below in table Table 4.\n\n\nQuery To Make bootstrap_rownum\n\ncreate or replace table resample_template as \nselect\n  strap_ids.strap_id,\n  original_data_rownum.original_rownum,\n  -- I have 2500 observations in my data\n  round( -0.5 + 2501*random()) as bootstrap_rownum,\nfrom\n  strap_ids\ncross join \n  original_data_rownum;\n\n\n\n\n\nTable 4: A sample from the table resampel_template.\n\n\n\n\n\n\nstrap_id\noriginal_rownum\nbootstrap_rownum\n\n\n\n\n573\n2216\n4\n\n\n312\n2227\n1792\n\n\n2\n554\n1577\n\n\n440\n381\n688\n\n\n969\n1352\n1840"
  },
  {
    "objectID": "posts/2022-11-16-bootstrapping-in-sql/index.html#actually-doing-the-bootstrapping-its-just-a-left-join",
    "href": "posts/2022-11-16-bootstrapping-in-sql/index.html#actually-doing-the-bootstrapping-its-just-a-left-join",
    "title": "Bootstrapping in SQL",
    "section": "Actually Doing The Bootstrapping: Its Just A Left Join!",
    "text": "Actually Doing The Bootstrapping: Its Just A Left Join!\nNow all we have to do is join the original data onto resample_template. The join is going to happen on original_data.original_rownum = resample_template.bootstrap_rownum.\n\ncreate or replace table resampled_data as\nselect\n  resample_template.strap_id,\n  resample_template.bootstrap_rownum,\n  original_data.x,\n  original_data.y\nfrom \n  resample_template\nleft join \n  original_data on original_data.original_rownum = resample_template.bootstrap_rownum;\n\nAnd congratulations, you have what is in essence an unnested rsample::bootstraps output. This happens shockingly fast in duckdb (actually, a bit faster than rsample does it, but that is anecdote I didn’t actually time them). The hard part now is the aggregation function. Obviously, you can’t do very complex statsitical aggregations in duckdb (or any other SQL dialect), but there are a few you can do. For example, let’s bootstrap the mean of \\(x\\) and \\(y\\), as well as the estimated regression coefficient.\n\nselect\n  'Bootstrap' || lpad(strap_id,4,0) as id,\n  'SQL' as method,\n  avg(x) as mean_x,\n  avg(y) as mean_y,\n  corr(y, x) * stddev(y) / stddev(x) as beta\nfrom resampled_data\ngroup by 1\norder by 1;\n\nWe can easily compare the distributions obtained via the SQL bootstrap with distributions obtained from rsample::bootstrap"
  },
  {
    "objectID": "posts/2022-11-16-bootstrapping-in-sql/index.html#but-does-it-work",
    "href": "posts/2022-11-16-bootstrapping-in-sql/index.html#but-does-it-work",
    "title": "Bootstrapping in SQL",
    "section": "But Does It Work",
    "text": "But Does It Work\nYes…I think. The averages for \\(x\\) and \\(y\\) look really good, but the SQL bootstrap tails for the regression coefficient are a little thin."
  },
  {
    "objectID": "posts/2022-11-16-bootstrapping-in-sql/index.html#conclusion",
    "href": "posts/2022-11-16-bootstrapping-in-sql/index.html#conclusion",
    "title": "Bootstrapping in SQL",
    "section": "Conclusion",
    "text": "Conclusion\nThis is pretty silly, and probably inefficient. I’m no data engineer, I’m just a guy with a Ph.D in stats and a lot of time on the weekend. I should get a hobby or something."
  },
  {
    "objectID": "posts/2024-02-06-logit/index.html",
    "href": "posts/2024-02-06-logit/index.html",
    "title": "You Just Said Something Wrong About Logistic Regression",
    "section": "",
    "text": "Congratulations, you just said something wrong about logistic regression. That’s OK, logistic regression is hard and we all have to learn/re-learn some things from time to time.\nThis is a living blog post intended to address some common misconceptions or flat out wrong statements I’ve seen people make about logistic regression."
  },
  {
    "objectID": "posts/2024-02-06-logit/index.html#the-coefficients-of-logistic-regression-are-interpreted-as-youre-x-times-more-likely-to-see-y-given-factor-z.",
    "href": "posts/2024-02-06-logit/index.html#the-coefficients-of-logistic-regression-are-interpreted-as-youre-x-times-more-likely-to-see-y-given-factor-z.",
    "title": "You Just Said Something Wrong About Logistic Regression",
    "section": "1 The Coefficients of Logistic Regression are Interpreted as “You’re X Times More Likely to See Y Given Factor Z”.",
    "text": "1 The Coefficients of Logistic Regression are Interpreted as “You’re X Times More Likely to See Y Given Factor Z”.\nWrong! The (exponentiated) coefficients of logistic regression are interpreted as “the odds you see Y given Factor Z are X times larger”.\nPhrases like “X times more likely” allude to the the probability of the event. If the probability of getting some event is 10%, and the probability of getting some other event is 20%, then you are 2 times more likely to get the latter event than the former.\nThis kind of comparison between two probabilities is called “the relative risk” (or risk ratio, or lift, it really depends on where you work). The coefficients of a logistic regression are not risk ratios, nor are they log risk ratios; they are log odds ratios.\nNow, it is actually fine to make this mistake under a few conditions. Namely, when a) the odds ratio is sufficiently small, and/or b) the baseline risk is sufficiently small. You can see this on the plot below which plots the odds ratios and corresponds risk ratios for various baseline risks. On the whole though, you shouldn’t interpret any output from logistic regression as a risk ratio unless you explicitly go out of our way to estimate that quantity.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\n\np1 = np.array([0.01, 0.05, 0.1, 0.15, 0.20])\n\nrr = np.linspace(1, 4.9, 500)\n\nfig, ax = plt.subplots(dpi=240)\nax.set_aspect('equal')\nax.set_xlim(1, 5)\nax.set_ylim(1, 5)\n\n# Set light gray background\nax.set_facecolor('#f0f0f0')\n\n# Set white grid lines\nax.grid(color='white', linestyle='-', linewidth=0.5)\n\nax.plot([0, 5], [0, 5], 'k--')\n\ncmap = cm.get_cmap('magma')\ncolors = cmap(np.linspace(0, 1, len(p1)))\n\nfor i, p in enumerate(p1):\n    p2 = rr * p\n    odr = p2 * (1 - p) / ((1 - p2) * p)\n    ax.plot(rr, odr, label=f\"{p:.2f}\", color=colors[i])\n\n# Move legend outside the plot axis to the right\nax.legend(title=\"Baseline risk\", loc='center left', bbox_to_anchor=(1, 0.5))\n\n# Set y ticks to [1, 2, 3, 4, 5]\nax.set_yticks([1, 2, 3, 4, 5])\nax.set_xticks([1, 2, 3, 4, 5])\nax.set_xlabel('Relative Risk')\nax.set_ylabel('Odds Ratio')\n\n# Add zoom inset region axis on the bottom center\naxins = ax.inset_axes(\n    [0.6, 0.1, 0.47*0.75, 0.47*0.75],  # Adjusted position\n    xlim=(1, 1.5), ylim=(1, 1.5), xticks=[1, 1.25, 1.5], yticks=[1, 1.25, 1.5])\n    \naxins.plot([0, 5], [0, 5], 'k--')\naxins.set_facecolor('#f0f0f0')\naxins.grid(color='white', linestyle='-', linewidth=0.5)\n\nfor i, p in enumerate(p1):\n    p2 = rr * p\n    odr = p2 * (1 - p) / ((1 - p2) * p)\n    axins.plot(rr, odr, label=f\"{p:.2f}\", color=colors[i])\n\n# Use ax.indicate_inset_zoom to draw a rectangle around the zoomed-in region\nax.indicate_inset_zoom(axins, edgecolor=\"black\")\n\nplt.show()\n\n\n\n\nHow the relative risk and odds ratio differ for various baseline risks. For each basline risk, we can compute the associated probabiltity given the relative risk. Given that probability, we can also compute the odds ratio."
  },
  {
    "objectID": "posts/2023-06-26-did-you-do-your-homework/index.html",
    "href": "posts/2023-06-26-did-you-do-your-homework/index.html",
    "title": "Did You Do Your Homework?",
    "section": "",
    "text": "This tweet made its way to my corner of the Bird Site. I’m a glutton for math homework punishment so I attempted it (and failed) but do not confuse my interest as legitimizing the tweet.\nFrankly, I think asking questions found in Wooldridge is not a good litmus test of anything aside for your memory from undergrad. But that isn’t the point of this post. The point is to reply to Senior Powerpoint Engineer’s (SPE for short here, or should I call them @ryxcommar) blog post which came as a response to the aforementioned tweet.\nIf I can summarize the post (which you should read), SPE makes the following points:\nThe post is paired nicely with his thread on why 2023 is a bad year to bootcamp your way to a data science job, which hits on a some of the same points.\nIn brief, I agree with SPE and we’ve all heard it before. Data science is suffering from being a bullshit job, but clearly quantitative thinking is useful in business. What should be taught if not XGBoost API’s and linear algebra theory? What are we really looking for?"
  },
  {
    "objectID": "posts/2023-06-26-did-you-do-your-homework/index.html#value-proposition",
    "href": "posts/2023-06-26-did-you-do-your-homework/index.html#value-proposition",
    "title": "Did You Do Your Homework?",
    "section": "Value Proposition",
    "text": "Value Proposition\nMy main contention here is the value proposition of a data scientist. If my summary is a fair, then SPE’s point is that data scientists are not in the trenches working with the data, instead they “twiddle with models in Jupyter”. But even then they don’t understand those models deeply enough (as evidenced by their inability to do homework questions). If they aren’t working with data and they can’t do science, what are they doing? What is their value? You probably don’t know, so think if you need a data scientist or some other position.\nThat feels reductive. Those can’t be the only two measures of a good data scientist, but granted that we need to answer “what do you do here”. I can only speak to my (limited) experience in tech, but many of the problems facing data scientists are not purely tech or math problems, so it doesn’t make sense to value data scientists on their ability to solve just those kinds of problems. In my experience, the majority of problems data scientists face are people problems.\nAlong the lines of people problems, the value proposition of a data scientist– in my humble opinion– is to get people to think about their problems scientifically. That is a lot harder than it sounds when the people you work with have not spent 4-10 years ostensibly1 doing science. But it is also more valuable than just writing dbt models and dashboarding.\nHere is an example from my work about getting people to think scientifically. Data analysts routinely made dashboards for the A/B tests we ran prior to my ownership of experimentation protocols. Many of the top of funnel metrics we measured were measured in terms of one another (e.g. Signup Rate = Signups per Visitor, Activation Rate = Activations per Signup, which is the ratio of Activation per Visitor and Signups per Visitor.) etc. Fine from a business perspective.\nWhen it comes to inference, I don’t need to tell you that if you measure those two metrics as I’ve written them then anything that increases signup rate is going to decrease activation rate, so its going to look like everything is a stalemate. But I did have to share that with most everyone I talked to about experimentation, data analysts included. And it wasn’t just one person or team doing it, it was nearly the entire company. Measuring the right thing (and most importantly, knowing how to explain that to people because it isn’t as easy as mentioning conditioning on a post treatment variable) is like 20% math problem and 80% people problem due to all the stakeholder management that has to go into explanation why we are now changing the metrics, and how the numbers don’t match, no we can’t keep doing it like we’ve been doing it, what the source of truth is now, and on and on and on. It seems cheesy for me to say “look how my scientific thinking has changed the company for the better” but I think this is the type of work we should be looking for and training data scientists for. This is the answer to “what do you do here”, and what data scientists should be doing instead of screwing around with ChatGPT.\nI think SPE would call this a “job perk” – thought leadership for internal decision making. That assumes you’re in a company that thinks scientifically and you’re free to do higher level stuff. We should all be so lucky. Thought leadership isn’t a perk, it is the job. You’re leading people to think scientifically. This is the disconnect I think SPE mentions. If you buy my rant, the title data scientist can imply an advanced understanding of applied math and not be expected to answer homework questions because remembering tricks from econometrics class isn’t used regularly in thinking scientifically. Candidates and interviewers aren’t being measured for/measuring the right thing (the ability to think scientifically and convince/teach others to follow suit) and here we are talking about if remembering \\(E[\\hat{\\beta}_1] = \\frac{\\operatorname{Cov}(X, Y)}{\\operatorname{Var}(X)}\\) is a good barometer for being a data scientist or not."
  },
  {
    "objectID": "posts/2023-06-26-did-you-do-your-homework/index.html#we-agree",
    "href": "posts/2023-06-26-did-you-do-your-homework/index.html#we-agree",
    "title": "Did You Do Your Homework?",
    "section": "We Agree",
    "text": "We Agree\nNone of this undermines SPE’s points, it isn’t like bootcamps are teaching how to think scientifically. But it isn’t news that data science is over hyped, nor is it news that fresh grads aren’t well equipped for the job, nor is it news that correct linear algebra problem sets are not a good indicator for success on the job. I can’t rebut something we all agree is true, but I can ask for solutions, next steps, jeez anything than another “zero interest rate phenomenon” joke.\nIf I could amend SPE’s blog post, it would be as follows:\n\nBut I don’t think “should you ask it?” is the right question. The real question here is: “Should you be hiring a data scientist? If so, are you willing to listen to what they have to say? Are you capable of recognizing scientific ability and probing to further test it?” And the answer to that is probably not. – (Emphasis mine)."
  },
  {
    "objectID": "posts/2023-06-26-did-you-do-your-homework/index.html#footnotes",
    "href": "posts/2023-06-26-did-you-do-your-homework/index.html#footnotes",
    "title": "Did You Do Your Homework?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI say “ostensibly” because I’m not sure what I did was “science” as much as it was survival.↩︎"
  },
  {
    "objectID": "posts/2020-10-06-links/index.html",
    "href": "posts/2020-10-06-links/index.html",
    "title": "Log Link vs. Log(y)",
    "section": "",
    "text": "You wanna see a little gotcha in statistics? Take the following data\n\nset.seed(0)\nN = 1000\ny = rlnorm(N, 0.5, 0.5)\n\nand explain why glm(y ~ 1, family = gaussian(link=log) and lm(log(y)~1) produce different estimates of the coefficients. In case you don’t have an R terminal, here are the outputs\n\nlog_lm = lm(log(y) ~ 1)\nsummary(log_lm)\n\n\nCall:\nlm(formula = log(y) ~ 1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.61028 -0.34631 -0.02152  0.35173  1.64112 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.49209    0.01578   31.18   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.499 on 999 degrees of freedom\n\n\n\nglm_mod = glm(y ~ 1 , family = gaussian(link=log))\nsummary(glm_mod)\n\n\nCall:\nglm(formula = y ~ 1, family = gaussian(link = log))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.5282  -0.6981  -0.2541   0.4702   6.5869  \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.61791    0.01698    36.4   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.9918425)\n\n    Null deviance: 990.85  on 999  degrees of freedom\nResidual deviance: 990.85  on 999  degrees of freedom\nAIC: 2832.7\n\nNumber of Fisher Scoring iterations: 5\n\n\nAnswer is the same as the difference between \\(E(g(X))\\) and \\(g(E(X))\\) which are not always the same. Let me explain.\nFirst, let’s start with the lognormal random variable. \\(y \\sim \\operatorname{Lognormal}(\\mu, \\sigma)\\) means \\(\\log(y) \\sim \\operatorname{Normal}(\\mu, \\sigma)\\). So \\(\\mu, \\sigma\\) are the parameters of the underlying normal distribution. When we do lm(log(y) ~ 1), we are modelling \\(E(\\log(y)) = \\beta_0\\). So \\(\\beta_0\\) is an estimate of \\(\\mu\\) and \\(\\exp(\\mu)\\) is an estimate of the median of the lognormal. That is an easy check\n\nmedian(y)\n\n[1] 1.600898\n\n\n\n#Meh, close enough\nexp(coef(log_lm))\n\n(Intercept) \n   1.635723 \n\n\nIf I wanted an estimate of the mean of the lognormal, I would need to add \\(\\sigma^2/2\\) to my estimate of \\(\\mu\\).\n\nmean(y)\n\n[1] 1.855038\n\n\n\n#Meh, close enough\nsigma = var(log_lm$residuals)\nexp(coef(log_lm) + sigma/2)\n\n(Intercept) \n   1.852594 \n\n\nOk, onto the glm now. When we use the glm, we model \\(\\log(E(y)) = \\beta_0\\), so we model the mean of the lognormal directly. Case in point\n\nmean(y)\n\n[1] 1.855038\n\n\n\nexp(coef(glm_mod))\n\n(Intercept) \n   1.855038 \n\n\nand if I wanted the median, I would need to consider the extra factor of \\(\\exp(\\sigma^2/2)\\)\n\nmedian(y)\n\n[1] 1.600898\n\n\n\nexp(coef(glm_mod) - sigma/2)\n\n(Intercept) \n   1.637881 \n\n\nLog link vs. log outcome can be tricky. Just be sure to know what you’re modelling when you use either."
  },
  {
    "objectID": "posts/2024-02-08-peek/index.html",
    "href": "posts/2024-02-08-peek/index.html",
    "title": "Peeking Sets You Up For Dissapointment",
    "section": "",
    "text": "Peeking (looking for significance in an AB test before the experiment has enough samples to reach desired power) is a “no no”. Rationales for not peeking typically mention inflated type 1 error rate.\nUnless you’re just randomizing into two groups and not changing anything, the null is unlikely to be true. So inflated type one error rate is really not the primary concern. Rather, if we peek then we are setting ourselves up for disappointment. Detected effects from peeking will typically not generalize, and we will be overstating out impact. The reason why is fairly clear when considering the Winner’s Curse.\nThe long and the short of it is as follows:\n\nWhen you peak, your tests are under powered.\nStatistically significant results from under powered tests generally over estimate the truth (see my post on the Winner’s curse for why).\nSo when you detect an effect from peeking, you are very likely over estimating your impact. When you roll out the change globally, you’re probably not going to see the impact you expected. This can lead to disappointment (and a lot of questions from everyone when they don’t see changes to the numbers on a dashboard).\n\nAs a concrete example, say you design an experiment to detect a 3% lift in a binary outcome, and say your intervention truly does improve the outcome by 3%. The baseline is 10%, and you design your experiment to have 80% power with a 5% false positive rate. You’re going to need a total of 318,132 users in your experiment…which sounds like a lot. What if we instead checked for statistical significance each time the groups hit a multiple of 20, 000. Depending on how fast we can accrue users into the experiment, this could save time…right?\nShown below is a line plot of the relative error between what you would expect to detect at each peek and the true lift. The plot speaks for itself; conditional on finding an effect early, you’re likely over estimating the truth.\n\n\nCode\nn &lt;- c(seq(20000, N, 20000), N)\npwr &lt;- sapply(n, \\(x) power.prop.test(n=x, p1=pc, p2=pt, sig.level = 0.05)$power)\nlog_rr_mean &lt;- log(pt) - log(pc)\nlog_rr_var &lt;- (1-pt)/(pt*n) + (1-pc)/(pc*n)\nlog_rr_sd &lt;- sqrt(log_rr_var)\n\nz = log_rr_mean - log_rr_sd * qnorm(pwr)\n\n\n\n\n# expectation from truncated normal\ntrunc = log_rr_mean + log_rr_sd * dnorm((z - log_rr_mean)/ log_rr_sd) / (1 - pnorm((z - log_rr_mean)/ log_rr_sd))\nerr &lt;- 100 * abs( log_rr_mean-trunc)/log_rr_mean\n\nsims &lt;- sapply(n, \\(nn) {\n  \n  \n  r &lt;-replicate(10000, {\n    \n  x &lt;- rbinom(1, nn, pc)\n  y &lt;- rbinom(1, nn, pt)\n  \n  test &lt;- prop.test(c(y, x), c(nn, nn))\n  \n  ifelse(test$p.value &lt; 0.05, log(y) - log(x), NA)\n  })\n  \n  (mean(r, na.rm=T) - log_rr_mean) / log_rr_mean\n  \n  \n})\n\n\n\n\n\n\n\n\n\n\n\nIn the best case scenario, where you end the experiment on the first peek, you’re going to vastly over estimate the impact you had (almost by a factor of 2.3x!). The worst part is that these errors compound, so if you peek on every experiment you’re going to grossly over estimate the total impact you had. Maybe something to talk to your PMs about."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Numbers, Letters, Sometimes Both",
    "section": "",
    "text": "Let’s Take About 15% Of The Top There\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 10, 2025\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nMore on Bayesian Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 18, 2025\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nA Brief Tour of CUPED and Related Methods (Pt. 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 3, 2025\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nCalibration of Mind Changing for Bayesian AB Tests\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2024\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nWhat Is Confounding?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 26, 2024\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nInteresting Interview Questions\n\n\n\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\nMay 21, 2024\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nPeeking Sets You Up For Dissapointment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2024\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nYou Just Said Something Wrong About Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2024\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nThe Winner’s Curse Is Easy To Understand From This Picture\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2024\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival Analysis With Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2024\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nThe Generalized Gamma Distribution For Parametric Survival\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2023\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nBrain Teaser About Frequentist Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating the Shifted Beta Geometric Model in Stan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2023\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nRegularization of Noisy Multinomial Counts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2023\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nRight In Ways You Don’t Care About\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 11, 2023\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nDifference in Difference Estimates Can Be Inaccurate When Adjusting For Detected Pre-Treatment Differences in Randomized Experiments\n\n\n\n\n\n\nAB Testing\n\n\nStatistics\n\n\nCausal Inference\n\n\n\n\n\n\n\n\n\nSep 3, 2023\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nDid You Do Your Homework?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 26, 2023\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nA Practical A/B Testing Brain Dump\n\n\n\n\n\n\nAB Tests\n\n\nStatistics\n\n\n\n\n\n\n\n\n\nMay 14, 2023\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nChoosing the Optimal MDE for Experimentation\n\n\n\n\n\n\nAB Tests\n\n\nStatistics\n\n\n\n\n\n\n\n\n\nMar 31, 2023\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nCausal Risk Ratios in AB Tests with One Sided Non-Compliance\n\n\n\n\n\n\nAB Tests\n\n\nStatistics\n\n\nCausal Inference\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nJourneyman Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 31, 2022\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nWay Too Many Taylor Series\n\n\n\n\n\n\nStatistics\n\n\nAB Testing\n\n\n\n\n\n\n\n\n\nNov 25, 2022\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nBootstrapping in SQL\n\n\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\nNov 16, 2022\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nPCA on The Tags for Cross Validated\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nInterim Analysis & Group Sequential Designs Pt 2\n\n\n\n\n\n\nStatistics\n\n\nAB Testing\n\n\n\n\n\n\n\n\n\nJul 30, 2022\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nForecasting Experimental Lift Using Hierarchical Bayesian Modelling\n\n\n\n\n\n\nStatistics\n\n\nAB Testing\n\n\nBayes\n\n\n\n\n\n\n\n\n\nJul 20, 2022\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nInterim Analysis & Group Sequential Designs Pt 1\n\n\n\n\n\n\nStatistics\n\n\nAB Testing\n\n\n\n\n\n\n\n\n\nJul 6, 2022\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nThis Is A Quarto Blog\n\n\n\n\n\n\nNews\n\n\n\n\n\n\n\n\n\nJun 22, 2022\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nFlippin’ Fun!\n\n\n\n\n\n\nBayes\n\n\nStan\n\n\nPython\n\n\n\n\n\n\n\n\n\nMay 7, 2022\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nHacking Sklearn To Do The Optimism Corrected Bootstrap\n\n\n\n\n\n\nStatistics\n\n\nMachine Learning\n\n\nPython\n\n\nScikit-Learn\n\n\n\n\n\n\n\n\n\nNov 23, 2021\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nOn Interpretations of Confidence Intervals\n\n\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\nApr 3, 2021\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nLog Link vs. Log(y)\n\n\n\n\n\n\nR\n\n\nStatistics\n\n\n\n\n\n\n\n\n\nOct 6, 2020\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nGradient Descent with ODEs\n\n\n\n\n\n\nPython\n\n\nMachine Learning\n\n\nStatistics\n\n\n\n\n\n\n\n\n\nMay 21, 2019\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nNeat Litle Combinatorics Problem\n\n\n\n\n\n\nPython\n\n\nProbability\n\n\n\n\n\n\n\n\n\nAug 31, 2018\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\nCoins and Factors\n\n\n\n\n\n\nPython\n\n\nRiddler\n\n\n\n\n\n\n\n\n\nDec 29, 2017\n\n\nDemetri Pananos\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "ai/index.html",
    "href": "ai/index.html",
    "title": "How I Use AI",
    "section": "",
    "text": "Text\nLLMs play the role of interlocutor and editor in my writing. I may ask LLMs to read my arguments and act as though they are someone with opposing perspectives in order to consider how best to respond to possible objections. Additionally, I may ask LLMs to edit chunks of text to make them more concise (mostly because I have a tendency to be verbose).\nAll text on this blog, and in publications in which I am listed as an author, is written by me. You can verify this because there are several typos across my blog. While I use LLMs in the ways mentioned above, I will never copy-and-paste from an LLM, nor will I ask an LLM to write a blog post, section of a paper, or email for me.\n\n\nImages\nI will not use LLMs to generate images in my blog or any publications. I may use AI to help me construct code for visualizations (see the Code section below).\n\n\nCode\nI will absolutely use LLMs to write code. My philosophy is that LLMs are another tool to improve code quality (such as an IDE, a linter, or intellisense). I personally have had much success using LLMs to contribute to codebases written in languages in which I am not proficient (e.g. Go and Typescript). That being said, I take the time to review the code myself and take full responsibility for what is written. I personally am not a fan of vibe coding as I like to have a high level understanding of what each part of the code is doing. Additionally, the automcomplete functionality gets in the way when I know the language very well, and at times I may turn that off in order to focus on doing what needs to be done.\nAs an example, I may write functionality in python or pseudocode and ask LLMs to translate into Go. Where I don’t understand syntax, I may ask LLMs to explain (e.g. by using analogies to python). Cursor is particularly good at this with the CMD+K and Option+Return for asking quick questions.\nI also use LLMs to write code for which I find consider there is low benefit to memorizing (e.g. how to write a Dockerfile, or docker CLI commands). Docker is an especially good example. Since my role does not concern creating or managing infrastructure, I find learning more than what I already know about docker to be a low return on investment. As such, most Dockerfiles that I use in personal projects are written by LLMs. Another example is very extensive APIs, such as matplotlib. My approach to using LLMs here is to create a very basic version of the plot I’d like to share, and then use LLMs to add certain features which would require more time than I would like to spend reading documentation. Consider the plot in this post. I used ChatGPT to help me make the embedded plot and move the legend outside of the plotting region. While I could have read matplotlib’s documentation on how to do this, it was much more efficient (for my purposes) to ask an LLM to accomplish this and move on with writing my post.\n\n\nResearch\nI may use LLMs to write aspects of code for research (as explained above), but I will not use LLMs to do research or summarize research papers. If I reference a paper, it is because I have read at least some parts of that paper and can justify why I am citing said paper.\nI will use LLMs to remind me of mathematical details (e.g. how are robust standard errors computed using linear algebra). The benefit here is staying in a flow state. I know which books on my shelf contain this information, should I ever need to reference them, but rather than get the book, and flip through the pages to find the right passage, I instead ask LLMs to remind me. In this way, an LLM is like a desk mate in my laboratory in grad school."
  }
]