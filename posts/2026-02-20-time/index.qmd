---
title: How Long To Run an A/B Test as a Bayesian
date: 2026-02-20
code-fold: false
echo: false
fig-cap-location: top
categories: []
number-sections: false
draft: false
cache: false
---

**Disclaimer**: This blog post is inspired, almost entirely, by Tyler Buffington's _excellent_ statistics reading group presentation today at Datadog on the value of information in A/B testing.  The ideas presented here are mainly his, I've just gone ahead and done a little more math (but not much more).  


## Introduction

In this post, I discuss decision criteria for Bayesian A/B testing and a way to think about experiment run times for Bayesian A/B tests. I'll also present some analytic results that can help us avoid doing simulations in practice at the cost of some mild assumptions.

First, I'll discuss a little background on some straightforward ways to analyze A/B tests as a Bayesian and discuss a few options for ship/no-ship decision rules for A/B tests.  Then, I'll introduce a way to think about expected impact as a Bayesian, and finally share some analytic results.


## Bayesian Approaches to A/B Testing

Most A/B tests target the lift in a metric, defined as

$$ \lambda = \dfrac{E[Y(1)]}{E[Y(0)]} - 1 \>. $$

We can obtain an estimate of the lift, $\hat \lambda$, via the plug in principle and the associated standard error, $s_\lambda$ via the delta method. Assuming $\hat \lambda$ is drawn from a normal distribution (I've talked about this before [here](../2025-11-21-kuethe/index.qmd)), and that $s_\lambda$ is known exactly, we can use a conjugate normal-normal model to obtain the posterior distribution.  Assuming the prior mean and variance is $\mu$ and $\tau^2$ respectively, the posterior variance and mean, $V$ and $M$, are

$$ V^{-1} = \dfrac{1}{s^2_\lambda} + \dfrac{1}{\tau^2} \>, $$
$$ M = V \times \left(\dfrac{\hat \lambda}{s^2_\lambda} + \dfrac{\mu}{\tau^2}\right) \>. $$

Whereas a Frequentist would make their ship/no-ship decision based on statistical significance, a Bayesian has a few options. Assuming increases to the metric are favorable, many of the decision rules are based on inequalities of the posterior mean.  Here are just three I have seen in the wild:

* Ship if $c \lt \Pr(\lambda \gt 0 \mid \hat \lambda)$, which is equivalent to $\sqrt{V} \Phi^{-1}(c) \lt M$, where $\Phi$ is the standard normal CDF.  This is sometimes called "Probability to beat control".
* Ship if $z_{1-\alpha/2}\sqrt{V} \lt M$.  This is like a Bayesian p-value being less than $\alpha$.
* Ship if $0 \lt M$. Ship whenever the posterior mean is positive.

Now, these are just a few ways to make decisions (they are certainly not exhaustive).  Prior to Tyler's talk, I didn't really have a great way to answer how long an A/B test should take (save some ramblings about EVSI [here](../2024-09-01-ab-length/index.qmd)) were you to use a Bayesian analysis method. However, with these details, I think I can now give an answer.


## How Long To Run an A/B Test as a Bayesian

The question of how long to run an A/B test is really a question of sample size.  Assuming you accrue some number subjects per week, you can do some back of the napkin math to say how long the test should run to sufficiently power the experiment given an MDE and some information on the metric. Often, we flip the script and present MDEs as a function of time (e.g. if you want a 3% MDE, run this experiment for 2 weeks.  Want a smaller MDE?  Run it longer). Run length is now a knob one can tune. 

This is do-able as a Frequentist because we have a very clear relationship between power, MDE, and sample size (which is a function of time).  Although there are Bayesian conceptions of power (e.g. conditional power, unconditional power, etc etc), the formulae for these don't so straightforwardly relate time and desired effect sizes.  This is further complicated by the prior -- if your prior says a 5% lift is not probable, you probably shouldn't design experiments to detect 5% lifts.

Rather than seek a formula right now, we can first draw random numbers to see what happens under different scenarios.  Consider the following algorithm:

* Draw a true lift from your prior, $\lambda$.
* Draw a control group mean from the implied sampling distribution (you need the control mean and standard error to do this, but you should know this if you were going to do a power calculation anyway).
* Draw a treatment group mean from the implied sampling distribution.  
* Compute $\hat \lambda$ and $s_\lambda$
* Compute $M$ and $V$.
* Apply your decision rule.  If you reach ship criteria, then return the true lift, else return 0.
* Repeat a few thousand times.

Here, time affects the sampling distributions of treatment and control.  The longer the experiment runs, the more samples we accrue, the larger the precision in the group means will be.  This procedure is shown for 4 scenarios below.  The histograms show the distribution of true impacts to the product under the decision rule.  The large spike at 0 indicates those A/B tests which resulted in no ship decisions, and the remainder are the impacts from shipping.

Note a few things.  First, while difficult to see, there are some cases (especially in small sample sizes) where we ship harm to the product!  This makes sense, when we have noisy measurements sometimes we can make type $S$ errors.  As the sample size increases, the probability we make these errors decreases as does the proportion of no-ship decisions (the height of the spike decreases).  Second, the mean of this distribution is the "average impact to the product".  This mean changes _slightly_ due to my first point, and eventually the distribution of these impacts stabilizes to some limiting distribution (Tyler referred to this as the _Clairvoyant distribution_, essentially the distribution of impacts were you to know the true lift perfectly). Hence, there is some upper bound to the mean.

```{r}

# =============================================================================
# Shared helpers
# =============================================================================

# Simulate data and compute posterior for a given n and prior
sim_posterior <- function(n, prior_mean = 0, prior_sd = 0.05,
                          control_mean = 2.0, prob_treatment = 0.5, nsims = 100000) {
  sigma <- 0.5
  se_c <- sigma / sqrt(n * (1 - prob_treatment))
  se_t <- sigma / sqrt(n * prob_treatment)

  true_lift <- rnorm(nsims, prior_mean, prior_sd)
  control   <- rnorm(nsims, control_mean, se_c)
  treatment <- rnorm(nsims, control_mean * (1 + true_lift), se_t)

  estimated_lift    <- (treatment / control) - 1
  estimated_lift_se <- sqrt(se_t^2 / control^2 + se_c^2 * treatment^2 / control^4)

  V <- 1 / (1 / estimated_lift_se^2 + 1 / prior_sd^2)
  M <- V * (estimated_lift / estimated_lift_se^2 + prior_mean / prior_sd^2)
  S <- sqrt(V)

  list(true_lift = true_lift, M = M, S = S)
}

# SE of lift via delta method (approximation at lambda ~ 0)
compute_se_lift <- function(n, control_mean = 2.0, prob_treatment = 0.5, sigma = 0.5) {
  se_c <- sigma / sqrt(n * (1 - prob_treatment))
  se_t <- sigma / sqrt(n * prob_treatment)
  sqrt((se_t^2 + se_c^2) / control_mean^2)
}

# Analytic expected impact given a critical hurdle lambda_crit
analytic_impact_from_crit <- function(lambda_crit, se_lift, prior_mean = 0, prior_sd = 0.05) {
  integrand <- function(delta) {
    prob_ship <- 1 - pnorm(lambda_crit, mean = delta, sd = se_lift)
    delta * prob_ship * dnorm(delta, prior_mean, prior_sd)
  }
  integrate(integrand, lower = -Inf, upper = Inf)$value
}

# Closed-form expected impact: bivariate normal truncation moment
# E[lambda * 1(lambda > S)] where lambda ~ N(prior_mean, prior_sd^2)
# and S is the threshold adjusted for measurement noise (se_lift)
closedform_impact_from_crit <- function(lambda_crit, se_lift, prior_mean = 0, prior_sd = 0.05) {
  tau <- sqrt(prior_sd^2 + se_lift^2)
  a   <- (prior_mean - lambda_crit) / tau
  prior_mean * pnorm(a) + (prior_sd^2 / tau) * dnorm(a)
}

# =============================================================================
# Decision Rule 1: Ship if P(lift > 0 | data) > cutoff
# =============================================================================

sim_decisions <- function(n = 5000, cutoff = 0.95, prior_mean = 0, prior_sd = 0.05,
                          control_mean = 2.0, prob_treatment = 0.5, nsims = 100000) {
  p <- sim_posterior(n, prior_mean, prior_sd, control_mean, prob_treatment, nsims)
  prob_positive <- pnorm(0, mean = p$M, sd = p$S, lower.tail = FALSE)
  mean(ifelse(prob_positive > cutoff, p$true_lift, 0))
}

analytic_decisions <- function(n = 5000, cutoff = 0.95, prior_mean = 0, prior_sd = 0.05,
                                control_mean = 2.0, prob_treatment = 0.5) {
  se_lift <- compute_se_lift(n, control_mean, prob_treatment)
  post_sd_inv <- sqrt(1 / prior_sd^2 + 1 / se_lift^2)
  lambda_crit <- qnorm(cutoff) * se_lift^2 * post_sd_inv
  analytic_impact_from_crit(lambda_crit, se_lift, prior_mean, prior_sd)
}

closedform_decisions <- function(n = 5000, cutoff = 0.95, prior_mean = 0, prior_sd = 0.05,
                                  control_mean = 2.0, prob_treatment = 0.5) {
  se_lift     <- compute_se_lift(n, control_mean, prob_treatment)
  post_sd_inv <- sqrt(1 / prior_sd^2 + 1 / se_lift^2)
  lambda_crit <- qnorm(cutoff) * se_lift^2 * post_sd_inv
  closedform_impact_from_crit(lambda_crit, se_lift, prior_mean, prior_sd)
}

# =============================================================================
# Decision Rule 2: Ship if lower 95% credible interval > 0
# =============================================================================

sim_decisions_ci <- function(n = 5000, ci_level = 0.95, prior_mean = 0, prior_sd = 0.05,
                              control_mean = 2.0, prob_treatment = 0.5, nsims = 100000) {
  p <- sim_posterior(n, prior_mean, prior_sd, control_mean, prob_treatment, nsims)
  lower_ci <- qnorm((1 - ci_level) / 2, mean = p$M, sd = p$S)
  mean(ifelse(lower_ci > 0, p$true_lift, 0))
}

analytic_decisions_ci <- function(n = 5000, ci_level = 0.95, prior_mean = 0, prior_sd = 0.05,
                                   control_mean = 2.0, prob_treatment = 0.5) {
  se_lift <- compute_se_lift(n, control_mean, prob_treatment)
  post_sd_inv <- sqrt(1 / prior_sd^2 + 1 / se_lift^2)
  lambda_crit <- qnorm((1 + ci_level) / 2) * se_lift^2 * post_sd_inv
  analytic_impact_from_crit(lambda_crit, se_lift, prior_mean, prior_sd)
}

closedform_decisions_ci <- function(n = 5000, ci_level = 0.95, prior_mean = 0, prior_sd = 0.05,
                                     control_mean = 2.0, prob_treatment = 0.5) {
  se_lift     <- compute_se_lift(n, control_mean, prob_treatment)
  post_sd_inv <- sqrt(1 / prior_sd^2 + 1 / se_lift^2)
  lambda_crit <- qnorm((1 + ci_level) / 2) * se_lift^2 * post_sd_inv
  closedform_impact_from_crit(lambda_crit, se_lift, prior_mean, prior_sd)
}

# =============================================================================
# Decision Rule 3: Ship if M > 0
# =============================================================================

sim_decisions_M <- function(n = 5000, prior_mean = 0, prior_sd = 0.05,
                             control_mean = 2.0, prob_treatment = 0.5, nsims = 100000) {
  p <- sim_posterior(n, prior_mean, prior_sd, control_mean, prob_treatment, nsims)
  mean(ifelse(p$M > 0, p$true_lift, 0))
}

analytic_decisions_M <- function(n = 5000, prior_mean = 0, prior_sd = 0.05,
                                  control_mean = 2.0, prob_treatment = 0.5) {
  se_lift <- compute_se_lift(n, control_mean, prob_treatment)
  lambda_crit <- -prior_mean * se_lift^2 / prior_sd^2
  analytic_impact_from_crit(lambda_crit, se_lift, prior_mean, prior_sd)
}

closedform_decisions_M <- function(n = 5000, prior_mean = 0, prior_sd = 0.05,
                                    control_mean = 2.0, prob_treatment = 0.5) {
  se_lift     <- compute_se_lift(n, control_mean, prob_treatment)
  lambda_crit <- -prior_mean * se_lift^2 / prior_sd^2
  closedform_impact_from_crit(lambda_crit, se_lift, prior_mean, prior_sd)
}

```



```{r example-procedure}
#| echo: false
#| fig-cap: "Distribution of shipped lifts under the probability to beat control decision rule ($c = 0.95$) for increasing sample sizes. Prior is $\\operatorname{Normal}(0, 0.05^2)$, $\\sigma = 0.5$, and equal allocation ($p = 0.5$). At small $N$, most simulations result in no-ship (the spike at 0). As $N$ grows, the posterior tightens and more true positives are shipped."

sigma <- 0.5
prob_treatment <- 0.5
prior_mean <- 0
prior_sd <- 0.05
control_mean <- 1
nsims <- 1e6
cutoff <- 0.95

brks <- seq(-0.2, 0.3, 0.0125)
sample_sizes <- c(1000, 10000, 100000, 1000000)

# First pass: compute all histograms to find shared y-axis
hists <- lapply(sample_sizes, function(n) {
  se_c <- sigma / sqrt(n * (1 - prob_treatment))
  se_t <- sigma / sqrt(n * prob_treatment)

  true_lift <- rnorm(nsims, prior_mean, prior_sd)
  control   <- rnorm(nsims, control_mean, se_c)
  treatment <- rnorm(nsims, control_mean * (1 + true_lift), se_t)

  estimated_lift    <- (treatment / control) - 1
  estimated_lift_se <- sqrt(se_t^2 / control^2 + se_c^2 * treatment^2 / control^4)

  V <- 1 / (1 / estimated_lift_se^2 + 1 / prior_sd^2)
  M <- V * (estimated_lift / estimated_lift_se^2 + prior_mean / prior_sd^2)
  S <- sqrt(V)

  prob_positive <- pnorm(0, mean = M, sd = S, lower.tail = FALSE)
  impact <- ifelse(prob_positive > cutoff, true_lift, 0)
  hist(impact, breaks = brks, plot = FALSE)
})

ymax <- max(sapply(hists, \(h) max(h$counts)))

# Second pass: plot with shared y-axis
par(mfrow = c(2, 2))
for (i in seq_along(sample_sizes)) {
  plot(hists[[i]], xlim = c(-0.1, 0.25), ylim = c(0, ymax),
       main = stringr::str_c("N=", scales::comma(sample_sizes[i])),
       xlab = "impact", ylab = "Frequency")
}
```


Below is a plot of the mean of this distribution as a function of sample size, and you can see this limiting behavior quite clearly.  The proposal is to base your run time on the expectation of the Clairvoyant distribution. From the plot, running A/B tests too short means the impact is going to be very small on average, but running longer means that (under the prior) you will impact the product more.  There comes a point where running the test longer is not helpful because you encounter diminishing returns.  This is a nice way to think about run time in my opinion! You know what your average impact would be were you to have perfect information, and you can collect enough samples so that you are sufficiently close to this without wasting time obtaining additional samples.  I think that is pretty elegant!

```{r example-mde-curve}
#| fig-cap: "Average impact to the product as a function of sample size under the probability to beat control rule ($c = 0.95$). The curve flattens as $N$ grows, reflecting diminishing returns from longer experiments. Each dot is based on 100,000 simulations at each sample size."
lns <- seq(2, 5, 0.25)
ns <- 10^lns
sim_prob <- sapply(ns, \(x) sim_decisions(n = x))


plot(lns, sim_prob, yaxt='n', xlab=expression(log[10](N)), ylab='Expected Impact')
axis(2, at = axTicks(2), labels = scales::percent(axTicks(2)))
```


Note that I've just shown simulations for probability to beat, but the simulation approach is sufficiently flexible to accommodate any decision rule (expected loss, Bayesian p values, or whatever you can cook up).  If you're an analyst, you can do this fairly readily on your computer.  What if you wanted to make this a product many people can use, and use with reproducible results?  Simulating is fine in my opinion, generating random numbers and counting should be pretty cheap, but it would be cool to have a formula for the expected value of this distribution as a function of the total sample size.

## An Analytic Solution

Simulation is great to get some intuition for what is happening, but we can actually recover these curves analytically with a little effort.  First, note that all three of the decision rules are of the form

$$ \sqrt{V}\Phi^{-1}(p) \lt M \>. $$

For the three decision rules I've listed above, $p = c$, $p=1-\alpha/2$, and $p=0.5$.  

Now, let's talk about our prior.  The literature and discourse on A/B testing is consistent on two claims: 1) Most A/B tests do not have impact, and 2) when there is an impact then it is usually modest (that is to say, we do not routinely see large hurt/help to the product, usually because the low hanging fruit have been picked).  This allows us to assume that $\mu \approx 0$ and $\left\lvert \hat \lambda \right\rvert \approx 0$.  Given this, we can simplify the inequality a little.  When $\mu = 0$ the posterior mean reduces to $M = V\hat \lambda / s^2_\lambda$, and

$$ s^2_\lambda \approx \left( \dfrac{s^2_t + s^2_c}{\bar{y}^2_c}\right) $$

Here, $s^2$ is the sampling variance of the treatment or control group (indicated by the subscript)  and $\bar y _c$ is the average from the control group. Note there is an implicit reliance on $N$ here since $s^2 \propto 1/N$. I'm going to indicate reliance on $N$ going forward to make this discussion a little easier.  We can re-write the inequality based only on $\hat \lambda$ as 

$$ S(N) = \Phi^{-1}(p) \cdot s^2_\lambda \cdot \sqrt{\dfrac{1}{s^2_\lambda} + \dfrac{1}{\tau^2}} \lt \hat \lambda $$

With this inequality in hand, we can recover the expectation of the Clairvoyant distribution and obtain a closed form expression for the expected impact as a function of $N$. We want to know

$$
E[\text{impact} \mid N]
=
E\!\left[\lambda \,\mathbf 1\{\hat\lambda > S(N)\}\right].
$$

Under the normalâ€“normal model we have

$$
\lambda \sim \mathcal N(\mu,\tau^2),
\qquad
\hat\lambda \mid \lambda
\sim
\mathcal N(\lambda, s_\lambda^2).
$$

Since everything is Gaussian, we can rewrite the shipping event as

$$
\hat\lambda > S(N)
\;\;\Longleftrightarrow\;\;
\lambda - Y > 0,
$$

where

$$
Y \sim \mathcal N(S(N), s_\lambda^2),
\qquad
Y \perp \lambda.
$$

Define

$$
D = \lambda - Y.
$$

Then

$$
D \sim \mathcal N(\mu - S(N),\, \omega^2),
\qquad
\omega^2 = \tau^2 + s_\lambda^2.
$$

So the expected impact becomes

$$
E[\lambda \mathbf 1(D>0)].
$$

This is a standard bivariate normal moment. Using linear projection geometry,

$$
E[\lambda \mathbf 1(D>0)]
=
\mu \Phi(a)
+
\frac{\tau^2}{\omega}\phi(a),
$$

where

$$
a
=
\frac{\mu - S(N)}{\omega},
\qquad
\omega^2=\tau^2+s_\lambda^2.
$$

Because we assumed $\mu \approx 0$, this simplifies to

$$
E[\text{impact} \mid N]
=
\frac{\tau^2}{\omega}
\phi\!\left(
\frac{-S(N)}{\omega}
\right).
$$

Substituting the decision threshold

$$
S(N)
=
\Phi^{-1}(p)\, s_\lambda^2
\sqrt{\frac{1}{s_\lambda^2}+\frac{1}{\tau^2}},
$$

gives a fully analytic expression for expected impact as a function of $N$ (through $s_\lambda^2 \propto 1/N$).

Thus, for any of the three decision rules, the entire curve of expected impact versus sample size collapses to

$$
E[\text{impact} \mid N]
=
\mu \Phi(a)
+
\frac{\tau^2}{\sqrt{\tau^2+s_\lambda^2}}
\phi(a),
\qquad
a=
\frac{\mu-S(N)}{\sqrt{\tau^2+s_\lambda^2}}.
$$

The only difference across rules is the constant $p$ embedded inside $S(N)$. As a corollary, we can obtain the expectation of the Clairvoyant distribution by noting that $S(N) \to 0$ as $N \to \infty$, so $\omega \to \tau$ and $a \to \mu/\tau$.  Hence the expected impact with perfect information is

$$
E[\text{Clairvoyant impact}]
=
\mu \Phi\!\left(\frac{\mu}{\tau}\right)
+
\tau\,\phi\!\left(\frac{\mu}{\tau}\right).
$$

```{r final-comparison}
#| fig-cap: "Simulated (open markers) and closed-form analytic (black lines) expected impact for all three decision rules. The red horizontal line is the Clairvoyant upper bound. Prior is $\\operatorname{Normal}(0, 0.05^2)$, $\\sigma = 0.5$, equal allocation."
lns <- seq(2, 5, 0.25)
ns  <- 10^lns

sim_prob <- sapply(ns, \(x) sim_decisions(n = x))
sim_ci   <- sapply(ns, \(x) sim_decisions_ci(n = x))
sim_M    <- sapply(ns, \(x) sim_decisions_M(n = x))

closed_prob <- sapply(ns, \(x) closedform_decisions(n = x))
closed_ci   <- sapply(ns, \(x) closedform_decisions_ci(n = x))
closed_M    <- sapply(ns, \(x) closedform_decisions_M(n = x))

clairvoyant <- closedform_impact_from_crit(lambda_crit = 0, se_lift = 0)

ylim <- range(0, sim_prob, sim_ci, sim_M, closed_prob, closed_ci, closed_M, clairvoyant)

plot(lns, sim_prob, type = 'p', pch = 1, col = 'black',
     xlab = expression(log[10](N)), ylab = 'Expected Impact', yaxt = 'n', ylim = ylim)
axis(2, at = axTicks(2), labels = scales::percent(axTicks(2)))
grid()
points(lns, sim_ci, pch = 2, col = 'black')
points(lns, sim_M,  pch = 0, col = 'black')
lines(lns, closed_prob, col = 'black', lwd = 2, lty = 1)
lines(lns, closed_ci,   col = 'black', lwd = 2, lty = 2)
lines(lns, closed_M,    col = 'black', lwd = 2, lty = 3)
abline(h = clairvoyant, col = 'red', lwd = 2)
legend('bottomright',
       legend = c(
         expression(P(lambda > 0 ~ "|" ~ hat(lambda)) > 0.95),
         expression("95% CI"),
         expression("M > 0"),
         "Clairvoyant"
       ),
       pch  = c(1,  2,  0,  NA),
       lty  = c(1,  2,  3,  1),
       col  = c('black', 'black', 'black', 'red'),
       lwd  = 2, cex = 0.8)
```
