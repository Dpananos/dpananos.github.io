---
title: How Long To Run an A/B Test as a Bayesian
date: 2026-02-20
code-fold: false
echo: false
fig-cap-location: top
categories: []
number-sections: false
draft: false
cache: false
---

**Disclaimer**: This blog post is inspired, almost entirely, by Tyler Buffington's _excellent_ statistics reading group presentation today at Datadog on the value of information in A/B testing.  The ideas presented here are mainly his, I've just gone ahead and done a little more math (but not much more).  


## Introduction

In this post, I discuss decision criteria for Bayesian A/B testing and a way to think about experiment run times for Bayesian A/B tests. I'll also present some analytic results that can help us avoid doing simulations in practice at the cost of some mild assumptions.

First, I'll discuss a little background on some straight forward ways to analyze A/B tests as a Bayesian and discuss a few options for ship/no-ship decision rules for A/B tests.  Then, I'll introduce a way to think about expected impact as a Bayesian, and finally share some analytic results.


## Bayesian Approaches to A/B Testing

Most A/B tests target the lift in a metric, defined as

$$ \lambda = \dfrac{E[Y(1)]}{E[Y(0)]} - 1 \>. $$

We can obtain an estimate of the lift, $\hat \lambda$, via the plug in principle and the associated standard error, $s_\lambda$ via the delta method. Assuming $\hat \lambda$ is drawn from a normal distribution (I've talked about this before [here](../2025-11-21-kuethe/index.qmd)), and that $s_\lambda$ is known exactly, we can use a conjugate normal-normal model to obtain the posterior distribution.  Assuming the prior mean and variance is $\mu$ and $\tau^2$ respectively, the posterior variance and mean, $V$ and $M$, are

$$ V^{-1} = \dfrac{1}{s^2_\lambda} + \dfrac{1}{\tau^2} \>, $$
$$ M = V \times \left(\dfrac{\hat \lambda}{s^2_\lambda} + \dfrac{\mu}{\tau^2}\right) \>. $$

Whereas a Frequentist would make their ship/no-ship decision based on statistical significance, a Bayesian has a few options. Assuming increases to the metric are favorable, the following are some of the most popular I have seen in the wild:

* Ship if $c \lt \Pr(\lambda \gt 0 \mid \hat \lambda)$.  This is sometimes called "Probability to beat control".
* Ship if $0 \lt M - z_{1-\alpha/2}\sqrt{V}$.  This is like a Bayesian p-value being less than $\alpha$.
* Ship if $0 \lt M$. Ship whenever the posterior mean is positive.

Now, these are just a few ways to make decisions (they are certainly not exhaustive).  Prior to Tyler's talk, I didn't really have a great way to answer how long an A/B test should take (save some ramblings about EVSI [here](../2024-09-01-ab-length/index.qmd)) were you to use a Bayesian analysis method. However, with these details, I think I can now give an answer.


## How Long To Run an A/B Test as a Bayesian

The question of how long to run an A/B test is really a question of sample size.  Assuming you accrue some number subjects per week, you can do some back of the napkin math to say how long the test should run to sufficiently power the experiment given an MDE and some information on the metric. Often, we flip the script and present MDEs as a function of time (e.g. if you want a 3% MDE, run this experiment for 2 weeks.  Want a smaller MDE?  Run it longer). Run length is now a knob one can tune. 

This is do-able as a Frequentist because we have a very clear relationship between power, MDE, and sample size (which is a function of time).  Although there are Bayesian conceptions of power (e.g. conditional power, unconditional power, etc etc), the formulae for these don't so straight forwardly relate time and desired effect sizes.  This is further complicated by the prior -- if your prior says a 5% lift is not probable, you probably shouldn't design experiments to detect 5% lifts.

Rather than seek a formula, we can simply draw random numbers to see what happens under different scenarios.  Consider the following algorithm:

* Draw a true lift from your prior, $\lambda$.
* Draw a control group mean from the implied sampling distribution (you need the control mean and standard error to do this, but you should know this if you were going to do a power calculation anyway).
* Draw a treatment group mean from the implied sampling distribution.  
* Compute $\hat \lambda$ and $s_\lambda$
* Compute $M$ and $V$.
* Apply your decision rule.  If you reach ship criteria, then return the true lift, else return 0.
* Repeat a few thousand times.

Here, time affects the sampling distributions of treatment and control.  The longer the experiment runs, the more samples we accrue, the larger the precision in the group means will be.  This procedure is shown for 4 scenarios below.  The histograms show the distribution of true impacts to the product under the decision rule.  The large spike at 0 indicates those A/B tests which resulted in no ship decisions, and the remainder are the impacts from shipping.

Note a few things.  First, while difficult to see, there are some cases (especially in small sample sizes) where we ship harm to the product!  This makes sense, when we have noisy measurements sometimes we can make type $S$ errors.  As the sample size increases, the probability we make these errors decreases as does the proportion of no-ship decisions (the height of the spike decreases).  Second, the mean of this distribution is the "average impact to the product".  This mean changes _slightly_ due to my first point, and eventually the distribution of these impacts stabilizes to some limiting distribution (Tyler referred to this as the _Clairvoyant distribution_, essentially the distribution of impacts were you to know the true lift perfectly). Hence, there is some upper bound to the mean.

```{r}

# =============================================================================
# Shared helpers
# =============================================================================

# Simulate data and compute posterior for a given n and prior
sim_posterior <- function(n, prior_mean = 0, prior_sd = 0.05,
                          control_mean = 2.0, prob_treatment = 0.5, nsims = 100000) {
  sigma <- 0.5
  se_c <- sigma / sqrt(n * (1 - prob_treatment))
  se_t <- sigma / sqrt(n * prob_treatment)

  true_lift <- rnorm(nsims, prior_mean, prior_sd)
  control   <- rnorm(nsims, control_mean, se_c)
  treatment <- rnorm(nsims, control_mean * (1 + true_lift), se_t)

  estimated_lift    <- (treatment / control) - 1
  estimated_lift_se <- sqrt(se_t^2 / control^2 + se_c^2 * treatment^2 / control^4)

  V <- 1 / (1 / estimated_lift_se^2 + 1 / prior_sd^2)
  M <- V * (estimated_lift / estimated_lift_se^2 + prior_mean / prior_sd^2)
  S <- sqrt(V)

  list(true_lift = true_lift, M = M, S = S)
}

# SE of lift via delta method (approximation at lambda ~ 0)
compute_se_lift <- function(n, control_mean = 2.0, prob_treatment = 0.5, sigma = 0.5) {
  se_c <- sigma / sqrt(n * (1 - prob_treatment))
  se_t <- sigma / sqrt(n * prob_treatment)
  sqrt((se_t^2 + se_c^2) / control_mean^2)
}

# Analytic expected impact given a critical hurdle lambda_crit
analytic_impact_from_crit <- function(lambda_crit, se_lift, prior_mean = 0, prior_sd = 0.05) {
  integrand <- function(delta) {
    prob_ship <- 1 - pnorm(lambda_crit, mean = delta, sd = se_lift)
    delta * prob_ship * dnorm(delta, prior_mean, prior_sd)
  }
  integrate(integrand, lower = -Inf, upper = Inf)$value
}

# =============================================================================
# Decision Rule 1: Ship if P(lift > 0 | data) > cutoff
# =============================================================================

sim_decisions <- function(n = 5000, cutoff = 0.95, prior_mean = 0, prior_sd = 0.05,
                          control_mean = 2.0, prob_treatment = 0.5, nsims = 100000) {
  p <- sim_posterior(n, prior_mean, prior_sd, control_mean, prob_treatment, nsims)
  prob_positive <- pnorm(0, mean = p$M, sd = p$S, lower.tail = FALSE)
  mean(ifelse(prob_positive > cutoff, p$true_lift, 0))
}

analytic_decisions <- function(n = 5000, cutoff = 0.95, prior_mean = 0, prior_sd = 0.05,
                                control_mean = 2.0, prob_treatment = 0.5) {
  se_lift <- compute_se_lift(n, control_mean, prob_treatment)
  post_sd_inv <- sqrt(1 / prior_sd^2 + 1 / se_lift^2)
  lambda_crit <- qnorm(cutoff) * se_lift^2 * post_sd_inv
  analytic_impact_from_crit(lambda_crit, se_lift, prior_mean, prior_sd)
}

# =============================================================================
# Decision Rule 2: Ship if lower 95% credible interval > 0
# =============================================================================

sim_decisions_ci <- function(n = 5000, ci_level = 0.95, prior_mean = 0, prior_sd = 0.05,
                              control_mean = 2.0, prob_treatment = 0.5, nsims = 100000) {
  p <- sim_posterior(n, prior_mean, prior_sd, control_mean, prob_treatment, nsims)
  lower_ci <- qnorm((1 - ci_level) / 2, mean = p$M, sd = p$S)
  mean(ifelse(lower_ci > 0, p$true_lift, 0))
}

analytic_decisions_ci <- function(n = 5000, ci_level = 0.95, prior_mean = 0, prior_sd = 0.05,
                                   control_mean = 2.0, prob_treatment = 0.5) {
  se_lift <- compute_se_lift(n, control_mean, prob_treatment)
  post_sd_inv <- sqrt(1 / prior_sd^2 + 1 / se_lift^2)
  lambda_crit <- qnorm((1 + ci_level) / 2) * se_lift^2 * post_sd_inv
  analytic_impact_from_crit(lambda_crit, se_lift, prior_mean, prior_sd)
}

# =============================================================================
# Decision Rule 3: Ship if M > 0
# =============================================================================

sim_decisions_M <- function(n = 5000, prior_mean = 0, prior_sd = 0.05,
                             control_mean = 2.0, prob_treatment = 0.5, nsims = 100000) {
  p <- sim_posterior(n, prior_mean, prior_sd, control_mean, prob_treatment, nsims)
  mean(ifelse(p$M > 0, p$true_lift, 0))
}

analytic_decisions_M <- function(n = 5000, prior_mean = 0, prior_sd = 0.05,
                                  control_mean = 2.0, prob_treatment = 0.5) {
  se_lift <- compute_se_lift(n, control_mean, prob_treatment)
  lambda_crit <- -prior_mean * se_lift^2 / prior_sd^2
  analytic_impact_from_crit(lambda_crit, se_lift, prior_mean, prior_sd)
}

```



```{r example-procedure}
#| echo: false
#| fig-cap: "Distribution of shipped lifts under the probability to beat control decision rule ($c = 0.95$) for increasing sample sizes. Prior is $\\operatorname{Normal}(0, 0.05^2)$, $\\sigma = 0.5$, and equal allocation ($p = 0.5$). At small $N$, most simulations result in no-ship (the spike at 0). As $N$ grows, the posterior tightens and more true positives are shipped."

sigma <- 0.5
prob_treatment <- 0.5
prior_mean <- 0
prior_sd <- 0.05
control_mean <- 1
nsims <- 1e6
cutoff <- 0.95

brks <- seq(-0.2, 0.3, 0.0125)
sample_sizes <- c(1000, 10000, 100000, 1000000)

# First pass: compute all histograms to find shared y-axis
hists <- lapply(sample_sizes, function(n) {
  se_c <- sigma / sqrt(n * (1 - prob_treatment))
  se_t <- sigma / sqrt(n * prob_treatment)

  true_lift <- rnorm(nsims, prior_mean, prior_sd)
  control   <- rnorm(nsims, control_mean, se_c)
  treatment <- rnorm(nsims, control_mean * (1 + true_lift), se_t)

  estimated_lift    <- (treatment / control) - 1
  estimated_lift_se <- sqrt(se_t^2 / control^2 + se_c^2 * treatment^2 / control^4)

  V <- 1 / (1 / estimated_lift_se^2 + 1 / prior_sd^2)
  M <- V * (estimated_lift / estimated_lift_se^2 + prior_mean / prior_sd^2)
  S <- sqrt(V)

  prob_positive <- pnorm(0, mean = M, sd = S, lower.tail = FALSE)
  impact <- ifelse(prob_positive > cutoff, true_lift, 0)
  hist(impact, breaks = brks, plot = FALSE)
})

ymax <- max(sapply(hists, \(h) max(h$counts)))

# Second pass: plot with shared y-axis
par(mfrow = c(2, 2))
for (i in seq_along(sample_sizes)) {
  plot(hists[[i]], xlim = c(-0.1, 0.25), ylim = c(0, ymax),
       main = stringr::str_c("N=", scales::comma(sample_sizes[i])),
       xlab = "impact", ylab = "Frequency")
}
```


Below is a plot of the mean of this distribution as a function of sample size, and you can see this limiting behavior quite clearly.  The proposal is to base your run time on the expectation of the Clairvoyant distribution. From the plot, running A/B tests too short means the impact is going to be very small on average, but running longer means that (under the prior) you will impact the product more.  There comes a point where running the test longer is not helpful because you encounter diminishing returns.  This is a nice way to think about run time in my opinion! You know what your average impact would be were you to have perfect information, and you can collect enough samples so that you are sufficiently close to this without wasting time obtaining additional samples.  I think that is pretty elegant!

```{r example-mde-curve}
#| fig-cap: "Average impact to the product as a function of sample size under the probability to beat control rule ($c = 0.95$). The curve flattens as $N$ grows, reflecting diminishing returns from longer experiments. The loess-smoothed curve is based on 100,000 simulations at each sample size."
lns <- seq(2, 5, 0.25)
ns <- 10^lns
sim_prob <- sapply(ns, \(x) sim_decisions(n = x))
smoothed_sim_prob <- loess(sim_prob ~ lns)

plot(lns, predict(smoothed_sim_prob), yaxt='n', xlab=expression(log[10](N)), ylab='Expected Impact', type='l')
axis(2, at = axTicks(2), labels = scales::percent(axTicks(2)))
```


Note that I've just shown simulations for probability to beat, but the simulation approach is sufficiently flexible to accommodate any decision rule (expected loss, Bayesian p values, or whatever you can cook up).  If you're an analyst, you can do this fairly readily on your computer.  What if you wanted to make this a product many people can use, and use with reproducible results?  Simulating is fine in my opinion, generating random numbers and counting should be pretty cheap, but it would be cool to have a formula for the expected value of this distribution as a function of the total sample size.

## An Analytic Solution

Let's stick with probability to beat for a moment.  The ship decision is, equivalently, 

$$ c \lt \Phi\left(\dfrac{M}{\sqrt{V}}\right) \>, $$

or equivalently

$$ \Phi^{-1}(c) \lt \dfrac{M}{\sqrt{V}} \>. $$

Here, $\Phi$ is the CDF of a standard normal distribution. The discourse and evidence across the A/B testing suggests that most A/B tests fail to move metrics, meaning that we should expect $\mu \approx 0$ as our prior mean.  As a consequence, our ship condition would be

$$ \Phi^{-1}(c) \lt  \dfrac{\hat \lambda}{s^2_\lambda\sqrt{\dfrac{1}{s^2_\lambda} + \dfrac{1}{\tau^2}}} \>.$$

Additionally, I personally think that an appropriate prior standard deviation is going to be small since we don't routinely detect enormous hurt/help to the product.  As a consequence of this, this would mean that $\lambda \approx 0$, or that the average outcomes in groups are going to be similar, at least to a first order approximation!  This means that the sampling variance of the lift can be approximated as 

$$ s^2_\lambda \approx \left( \dfrac{s^2_t + s^2_c}{\bar y^2_c} \right) \>. $$

Here, the subscripts indicate estimates for treatment, $t$, or control, $c$. Note that this is where the sample size, and hence time, enters the equation.  The sampling variability is a function of $N$, though I've not written this out explicitly. I'll add a dependency on $N$ going forward, and we can re-write our decision criteria solely as a function of the estimated lift as

$$  S(N) = \Phi^{-1}(c) \cdot s^2_\lambda \cdot \sqrt{\dfrac{1}{s^2_\lambda} + \dfrac{1}{\tau^2}} \lt \hat \lambda \>. $$

If we know the true lift $\lambda$, we could calculate the probability that this criteria would be met by $\hat \lambda$.  It would be

$$ \Pr(S(N) \lt \hat \lambda \mid \lambda) = 1 - \Phi\left(\dfrac{S(N)-\lambda}{s_\lambda} \right)$$


but of course we don't know $\lambda$.  We do however have a prior over $\lambda$, so we can compute the expected impact as

$$ E[\mbox{impact} \mid N] = \int_{-\infty}^{\infty}  \lambda \cdot \left(1 - \Phi\left(\dfrac{S(N)-\lambda}{s_\lambda} \right) \right) f(\lambda) d\lambda \>. $$

Here, $f$ is the prior density of $\lambda$. Now, this looks like a right hairy bastard, but it is actually quite manageable for R or other numerical integration since the integrand is nice and smooth.  Shown below are the simulated and analytic results for the probability to beat control decision rule. The approximation isn't bad!

```{r prob-to-beat-comparison}
#| fig-cap: "Simulated vs analytic expected impact for the probability to beat control rule ($c = 0.95$). The close agreement confirms the analytic solution is a good approximation, at least under the proposed parameters. Prior is $\\operatorname{Normal}(0, 0.05^2)$, $\\sigma = 0.5$, and equal allocation ($p = 0.5$)."
lns <- seq(2, 5, 0.25)
ns <- 10^lns

sim_prob      <- sapply(ns, \(x) sim_decisions(n = x))
analytic_prob <- sapply(ns, \(x) analytic_decisions(n = x))
f_prob <- loess(sim_prob ~ lns)

col_sim <- rgb(1, 0, 0, 0.75)
col_ana <- rgb(0, 0, 1, 0.75)

plot(lns, predict(f_prob), type = 'l', col = col_sim, lwd = 2,
     xlab = expression(log[10](N)), ylab = 'Expected Impact', yaxt = 'n',
     ylim = range(0, analytic_prob))
axis(2, at = axTicks(2), labels = scales::percent(axTicks(2)))
grid()
lines(lns, analytic_prob, col = col_ana, lwd = 2)
legend('bottomright',
       legend = c('Simulation', 'Analytic'),
       col = c(col_sim, col_ana), lwd = 2)
```

This sort of exercise can be done for the remaining two decision rules.  Shown below is a plot comparing the analytic formulae and simulations for all three decision rules.


```{r comparisons}
#| fig-cap: "Simulated (red) and analytic (blue) expected impact curves for three decision rules: probability to beat control (solid, $c = 0.95$), 95% credible interval excluding zero (dashed), and M > 0 (dotted, ship if posterior mean is positive). The M > 0 rule is the most permissive, shipping more often and yielding higher average impact at each sample size."
lns <- seq(2, 5, 0.25)
ns <- 10^lns

# Compute all 6 expected impact curves
sim_prob      <- sapply(ns, \(x) sim_decisions(n = x))
analytic_prob <- sapply(ns, \(x) analytic_decisions(n = x))
sim_ci        <- sapply(ns, \(x) sim_decisions_ci(n = x))
analytic_ci   <- sapply(ns, \(x) analytic_decisions_ci(n = x))
sim_M         <- sapply(ns, \(x) sim_decisions_M(n = x))
analytic_M    <- sapply(ns, \(x) analytic_decisions_M(n = x))

# Smooth simulation curves
f_prob <- loess(sim_prob ~ lns)
f_ci   <- loess(sim_ci ~ lns)
f_M    <- loess(sim_M ~ lns)

# Colors: red = simulation, blue = analytic
# Linetype: solid = P(lift>0) rule, dashed = CI rule, dotted = M > 0
col_sim <- rgb(1, 0, 0, 0.75)
col_ana <- rgb(0, 0, 1, 0.75)

ylim <- range(0, analytic_prob, analytic_ci, analytic_M)

plot(lns, predict(f_prob), type = 'l', col = col_sim, lwd = 2, lty = 1,
     xlab = expression(log[10](N)), ylab = 'Expected Impact', yaxt = 'n', ylim = ylim)
axis(2, at = axTicks(2), labels = scales::percent(axTicks(2)))
grid()
lines(lns, analytic_prob, col = col_ana, lwd = 2, lty = 1)
lines(lns, predict(f_ci),  col = col_sim, lwd = 2, lty = 2)
lines(lns, analytic_ci,    col = col_ana, lwd = 2, lty = 2)
lines(lns, predict(f_M),   col = col_sim, lwd = 2, lty = 3)
lines(lns, analytic_M,     col = col_ana, lwd = 2, lty = 3)
legend('bottomright',
       legend = c(expression("Simulation, " * P(lambda > 0 ~ "|" ~ hat(lambda)) > 0.95),
                  expression("Analytic, " * P(lambda > 0 ~ "|" ~ hat(lambda)) > 0.95),
                  expression("Simulation, 95% CI for " * lambda > 0),
                  expression("Analytic, 95% CI for " * lambda > 0),
                  expression("Simulation, M > 0"),
                  expression("Analytic, M > 0")),
       col = c(col_sim, col_ana, col_sim, col_ana, col_sim, col_ana),
       lty = c(1, 1, 2, 2, 3, 3), lwd = 2)
```

