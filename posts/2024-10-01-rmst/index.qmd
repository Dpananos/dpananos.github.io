---
title: Survival Analysis in SQL (No, Really)
date: "2024-10-01"
code-fold: true
echo: true
fig-cap-location: top
categories: []
number-sections: false
draft: true
cache: true
---

Remember [Bootstrapping in SQL](https://dpananos.github.io/posts/2022-11-16-bootstrapping-in-sql/)?  That was fun.  Maybe we should do more statistics stuff in SQL.  God knows tech people are doing it:

* [Here is Evan Miller talking about how Eppo estimates design matrices for regression in SQL](https://www.youtube.com/watch?v=iyH8GPXzBpk)
* [Here is a paper in which data scientists from some experimentation heavy orgs describe how to estimate fixed effects models in SQL](https://arxiv.org/abs/2410.09952)

I've been on a survival analysis kick, so let's estimate the Kaplan Meir estimator in SQL.  


## Simulate Some Data

To start, we need some data.  Most experiments run in tech can measure the timing of events very accurately, but I imagine that most KPIs really care about changes on the level of days.  It doesn't matter if your users pay you 12 minutes sooner (except maybe in edge cases), but if they pay you 2 days sooner than they would otherwise then that is great news. As a consequence, I think it makes sense to use interval censoring on the day level.  

Here is some code to simulate some timestamps for an experiment.  In short, an experiment is run for 30 days.  Every hour, some number of users enter the experiment and are randomly assigned to either treatment or control.  The users are then monitored over time -- some for more time, some for less time -- and all users who do not get the outcome under study are administratively censored 30 days after the experiment.  This means that if I entered the experiment on day 29, I would only really be observed for 24 hours.  Where as if I entered on day 2 of the experiment then I would be observed for 28 days.  The survival approach means that we can properly account for the censoring as opposed to extending the experiment 30 additional days so as to give everyone the same length of time at risk.

Here is the code.

```{r}
#| code-fold: true
library(tidyverse)
library(survival)
library(duckdb)

set.seed(9)

users_per_hour <- 97
experiment_days <- 30
start_time <- ymd_hms('2024-10-01 00:00:00')
end_time <- start_time + days(experiment_days)

# simulate observations of entry into an experiment
dttm_rng <- seq(start_time, end_time - days(1), by = 'hour')

experiment_data <- map_dfr(dttm_rng, ~{
  
  # Simulate how many users enter the experiment
  n_exposed <- rpois(1, users_per_hour)
  # Assign them to treatment or control randomly
  treatment <- rbinom(n_exposed, 1, 0.5)
  # Simulate their exposure time.  They enter the experiment uniformly
  # between midnight and 11:59:59 pm the same day
  exposure_time <- .x + runif(n=n_exposed, min=0, max=60*60*24-1)
  # Simulate their event time.
  rt = exp(-0.1*treatment)/(60*60*24*180)
  latent_event_time <- rexp(n_exposed, rate = rt) + exposure_time
  observed_time <- if_else(latent_event_time>end_time, NA, latent_event_time)
  
  tibble(
    treatment, exposure_time, observed_time, latent_event_time
  )
  
  
}) %>% 
  mutate(userid = seq_along(treatment),
         treatment = if_else(treatment == 1, "Treatment","Control"))

```

I'm going to write these data to duckdb where I can write the necessary SQL.

```{r}
#| code-fold: true
con <- DBI::dbConnect(duckdb(), ':memory:')
dbWriteTable(con, 'experiment_000', experiment_data)

```


## Survival Analysis in SQL

```{sql, connection=con, output.var = 'dd'}
SET VARIABLE EXPERIMENT_START_DATE = TIMESTAMP '2024-10-01 00:00:00';
SET VARIABLE EXPERIMENT_END_DATE = TIMESTAMP '2024-10-31 00:00:00';


with prep_cleaned_exposures as (
  select distinct
    userid, 
    treatment,
    min(exposure_time) over (partition by userid) as exposure_time,
    (min(treatment) over (partition by userid)) <> (max(treatment) over (partition by userid)) as multiple_exposures
  from experiment_000
  where 
    (
    exposure_time <= getvariable('EXPERIMENT_END_DATE')
    and
    exposure_time >= getvariable('EXPERIMENT_START_DATE')
    )
  order by treatment, userid
  
)

, cleaned_exposures as (select * from prep_cleaned_exposures where not multiple_exposures  )

, exposure_balance as (select treatment, count(distinct userid) n_exposed from cleaned_exposures group by 1)


, prep_experiment_outcomes as (
  select 
    a.userid, 
    a.treatment, 
    a.exposure_time, 
    b.observed_time as event_time, 
    c.n_exposed as at_risk
  from cleaned_exposures as a
  left join experiment_000 as b 
    on a.userid = b.userid and a.exposure_time <  coalesce(b.observed_time, getvariable('EXPERIMENT_END_DATE'))
  left join exposure_balance c on a.treatment = c.treatment
)

, cleaned_experiment_outcomes as (
  select
    userid, 
    treatment, 
    exposure_time, 
    event_time is not NULL as is_observed, 
    at_risk,
    coalesce(event_time, getvariable('EXPERIMENT_END_DATE') ) as observed_time,
    datediff('seconds', exposure_time, coalesce(event_time, getvariable('EXPERIMENT_END_DATE') )) * 1.0 / (86400) as event_time_days,
  from prep_experiment_outcomes
  order by treatment, userid
)

, prep_lifetable_1 as (

select
  
  treatment, 
  case when not is_observed then floor(event_time_days) else floor(event_time_days) + 0.5 end as event_time,
  coalesce(count(distinct case when  is_observed then userid end), 0) as n_event,
  coalesce(count(distinct case when not is_observed then userid end), 0) as n_censor,
  at_risk as n_exposed
  
from cleaned_experiment_outcomes
group by 1, 2, 5
order by 1, 2

)

, life_table as (

select 
*, 
coalesce(lag(at_risk1, 1) over (partition by treatment order by event_time), n_exposed) as at_risk,
n_event::float / at_risk::float as hazard
from(
  select 
    *, 
    n_exposed - (sum(n_event + n_censor) over (partition by treatment order by event_time rows between unbounded preceding and current row)) as at_risk1
  from prep_lifetable_1
  order by 1, 2
)

)

select 
  *, 
  exp(sum(ln(1-hazard)) over (partition by treatment order by event_time rows between unbounded preceding and current row)) as survival_probability
from life_table
order by 1, 2

```


```{r}
experiment_data %>% 
  filter(
    exposure_time>=start_time, 
    exposure_time <=end_time
  ) %>% 
  transmute(
    treatment = treatment, 
    event = if_else(is.na(observed_time), 0, 3),
    time = floor(interval(exposure_time, coalesce(observed_time, end_time))/days(1)),
    time2 = ceiling(interval(exposure_time, observed_time)/days(1))
  ) -> d


# Interval censoring.
survfit(Surv(time=time, time2=time2, event=event, type='interval') ~ strata(treatment), data=d) -> f


broom::tidy(f) %>% 
  ggplot(aes(time, estimate, color=strata)) + 
  geom_step() 
```
```{r}

dd  %>% 
  ggplot(aes(event_time, survival_probability, color=factor(treatment))) + 
  geom_step() + 
  geom_point(data=broom::tidy(f), aes(time, estimate, color=strata))
```
