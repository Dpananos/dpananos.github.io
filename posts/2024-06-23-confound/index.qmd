---
title: What Is Confounding?
date: "2024-06-26"
code-fold: true
echo: true
fig-cap-location: top
categories: []
number-sections: false
draft: false
---


## Introduction

Thinking back to my Introduction to Epidemiology class, confounding was one of the topics of central interest.  However, it took the remainder of my PhD (and then some) to really get _what confounding was_.  Confounding was introduced as exposure and outcome having common cause (you have to have seen this before)

```{r}
#| echo: false
#| code-fold: false
library(dagitty)
library(ggdag)
library(tidyverse)


dag <- dagify(
  y ~ x + w,
  x ~ w,
  exposure = "x",
  outcome = "y",
  coords = list(x = c(x = 0, w = 1, y = 2),
                y = c(x = 0, w = 1, y = 0))
)

ggdag(dag) + 
  theme_void()
```


So yes, the effect of $x$ on $y$ is confounded.  But like...why?  DAG's aside, what is happening here? That much isn't answered in introductory classes, and I think it would behoove most people to learn the answer.  As a digression, I know the simple difference in means can be decomposed into ATE + Selection Bias + Heterogeneous treatment effect, but I don't think that really anwers what I want since that decomposition is in terms of potential outcomes.  I want answers which, in part, depend on the data and how we choose to summarize it.

Granted, we need a little but of math, but _only_ a little.  If you understand the law of total expectation, and can handle conditional distributions, you're going to be fine.  Let's look at what really happens in this DAG using an example.

## An Example

Suppose a new drug $D$ is introduced to prevent death.  In truth, the drug decreases the risk of death in both men and women, but men are more likely to take the drug and men are more likely to die.  Hence, the effect of the drug is confounded by sex.

I'm going to simulate this data using the following data generating process

$$ S \sim \operatorname{Bern}(0.5) $$
$$ D \mid S \sim \operatorname{Bern}(0.6 + 0.3S) $$

$$ Y \mid D, S \sim \operatorname{Bern}(0.4 - 0.1D + 0.4S) $$

I'll simulate a million observations so that our precision is big enough to not worry about sampling variability. In R, that can be done with

```{r}
#| code-fold: false
sim_data <- function(x, n=1000){
  withr::with_seed(x, {
    s <- rbinom(n, 1, 0.5)
    d <- rbinom(n, 1, 0.6 + 0.3*s)
    y <- rbinom(n, 1, 0.4 - 0.1*d + 0.4*s)
  })
  data.frame(s, d, y)
}
```


## The Right Way

Open up a book like ["_What If_"](https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/) and you'll find that the way to correctly estimate the effect of $D$ on $Y$ would be to first estimate the risk of death within strata defined by sex and then weight the stratified estimates by the prevalence of the strata.  This can be done using `{marginaleffects}` pretty easily


```{r}
#| code-fold: false

library(marginaleffects)

model <- glm(y~d*s, data=sim_data(0, 1e6), family = binomial())

# If I didn't know P(S) maybe I wouldn't use datagrid in this way
nd <- datagrid(model = model, d=0:1, s=0:1)

avg_comparisons(model, variables = 'd', newdata = nd)
```



The estimate is very close to the actual effect of the drug; a reduction in risk of death of 10 percentage points. Why exactly this procedure works relies on the ignorability assumption and the law of total expectation.  Briefly, if we have ignorability then $E[Y(d) \mid S=s] = E[Y\mid D=d, S=s]$, and by the law of total expectation

$$ E[Y(d)] = \sum_{s} E[Y(d) \mid S=s] P(S=s) =\sum_{s} E[Y \mid D=d, S=s] P(S=s) $$
which is a weighted average of within strata estimates.   Now, let's take an extended look at the wrong way of doing things.

## The Wrong Way

So we know the right uses the law of total expectation to get the right weights for the within strata estimates.  What does the wrong way estimate then?  First, let's see what the wrong way would estimate for this data generating process.

```{r}

model <- glm(y~d, data=sim_data(0, 1e6), family = binomial())

nd <- datagrid(model = model, d=0:1)

avg_comparisons(model, variables = 'd', newdata = nd)

```

Interesting, the wrong answer is _really wrong_.  Without adjusting for sex, we would conclude that the drug increases risk of death by near 6\%. Of course, this is a result of the fact that men are more likely to take the drug and more likely to die.

The wrong analysis simply estimates the within exposure estimates and contrasts them.  Now, because we are marginalizing over $S$ to do this, it would behoove us to appeal to the law of total expectation since we know this is the key to getting the right answer.

In the wrong approach, we compute

$$ E[Y \mid D=d] \>. $$

Let's write out how $S$ plays a part in this computation.  Again, $E[Y \mid D=d]$ is a weighted average of within strata estimates

$$ E[Y \mid D=d] = \sum_{s} E[Y \mid D=d, S=s] P(S=s\mid D=d) $$

Aha!  Our first clue into what confounding _really is_.  We see that $E[Y \mid D=d, S=s]$ appears in our weighted average, but the weights are wrong!  They should be $P(S=s)$ and instead they are $P(S=s\mid D=d)$!  Confounding is (partially) an improper weighting of expected values.


We can actually compute what the wrong approach would produce in the limit of infinite data if we just leverage Bayes rule to rewrite $P(S=s\mid D=d)$ as 

$$P(S=s\mid D=d) = \dfrac{P(D=d \mid S=s) P(S=s)}{P(D=d)} $$

```{r}
#| code-fold: false
#| 
Ey <- function(d, s){
  0.4 - 0.1*d + 0.4*s
}

Ed <- function(s){
  0.6 + 0.3*s  
}

Pd1 <- Ed(1)*0.5 + Ed(0)*0.5

Eyd1 <- Ey(1, 1)*Ed(1)*0.5/Pd1 + Ey(1, 0)*Ed(0)*0.5/Pd1
Eyd0 <- Ey(0, 1)*(1-Ed(1))*0.5/(1-Pd1) + Ey(0, 0)*(1-Ed(0))*0.5/(1-Pd1)

wrong_answer <- Eyd1 - Eyd0
#0.06
```


This really drives the point home that confounding is not a matter of more data, rather its a matter of the right data.


## More Wrong

So confounding is partially the wrong weights applied to the right estimates.  Earlier, I mentioned that the simple difference in means can be decomposed into selection bias and heteogrenous treatment effects.  Now, I've designed this example to have a homoegenous treatment effect, so we know that the error comes from selection bias.  Let's determine exactly selection bias lives in the wrong weights applied to the right estimates,  $P(S=s\mid D=d)$.

According to [Causal Inference: The Mixed Tape](https://mixtape.scunning.com/04-potential_outcomes#simple-difference-in-means-decomposition), the selection bias in the simple difference in means is 

$$ E[Y(0) \mid D=1] - E[Y(0) \mid D=0] \>. $$

Let's use the law of total expectation to re-write $E[Y(d^\prime) \mid D=d]$ as a marginalization over $S$.

$$ \begin{align*}E[Y(d^\prime) \mid D=d] &= \sum_{s} E[Y(d^\prime) \mid D=d, S=s]P(S=s\mid D=d) \\ &= \sum_{s}E[Y(d^\prime) \mid D=d, S=s]\frac{P(D=d \mid S=s) P(S=s)}{P(D=d)} \end{align*}$$

Its easier to spot where the bias comes from (mathematically) if you consider what the implication would be were exposure to be independent of sex.  If exposure were independent of sex, as would be the case in a randomized experiment, then $P(D=d \mid S=s) = P(D=d)$.  The weight in this sum then collapses to $P(S=s)$.  However, in this example, $D$ is not independent of $S$ and so $P(D=d \mid S=s) \neq P(D=d)$!  This probability has a name, its called _the propensity score_, and that this is not the same between strata defined by sex is the reason that the selection bias is non zero in our example.  If the propensity score was the same within strata, then strata defined by $D$ would be exchangeable and $E[Y(d^\prime) \mid D=d] = E[Y(d^\prime)]$.




















































