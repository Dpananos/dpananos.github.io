{
  "hash": "851cc1a8f95849c95796d256842f6afd",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Survival Analysis in SQL (No, Really)\ndate: \"2024-10-01\"\ncode-fold: true\necho: true\nfig-cap-location: top\ncategories: []\nnumber-sections: false\ndraft: true\ncache: true\n---\n\n\nRemember [Bootstrapping in SQL](https://dpananos.github.io/posts/2022-11-16-bootstrapping-in-sql/)?  That was fun.  Maybe we should do more statistics stuff in SQL.  God knows tech people are doing it:\n\n* [Here is Evan Miller talking about how Eppo estimates design matrices for regression in SQL](https://www.youtube.com/watch?v=iyH8GPXzBpk)\n* [Here is a paper in which data scientists from some experimentation heavy orgs describe how to estimate fixed effects models in SQL](https://arxiv.org/abs/2410.09952)\n\nI've been on a survival analysis kick, so let's estimate the Kaplan Meir estimator in SQL.  \n\n\n## Simulate Some Data\n\nTo start, we need some data.  Most experiments run in tech can measure the timing of events very accurately, but I imagine that most KPIs really care about changes on the level of days.  It doesn't matter if your users pay you 12 minutes sooner (except maybe in edge cases), but if they pay you 2 days sooner than they would otherwise then that is great news. As a consequence, I think it makes sense to use interval censoring on the day level.  \n\nHere is some code to simulate some timestamps for an experiment.  In short, an experiment is run for 30 days.  Every hour, some number of users enter the experiment and are randomly assigned to either treatment or control.  The users are then monitored over time -- some for more time, some for less time -- and all users who do not get the outcome under study are administratively censored 30 days after the experiment.  This means that if I entered the experiment on day 29, I would only really be observed for 24 hours.  Where as if I entered on day 2 of the experiment then I would be observed for 28 days.  The survival approach means that we can properly account for the censoring as opposed to extending the experiment 30 additional days so as to give everyone the same length of time at risk.\n\nHere is the code.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse)\nlibrary(survival)\nlibrary(duckdb)\n\nset.seed(9)\n\nusers_per_hour <- 97\nexperiment_days <- 30\nstart_time <- ymd_hms('2024-10-01 00:00:00')\nend_time <- start_time + days(experiment_days)\n\n# simulate observations of entry into an experiment\ndttm_rng <- seq(start_time, end_time - days(1), by = 'hour')\n\nexperiment_data <- map_dfr(dttm_rng, ~{\n  \n  # Simulate how many users enter the experiment\n  n_exposed <- rpois(1, users_per_hour)\n  # Assign them to treatment or control randomly\n  treatment <- rbinom(n_exposed, 1, 0.5)\n  # Simulate their exposure time.  They enter the experiment uniformly\n  # between midnight and 11:59:59 pm the same day\n  exposure_time <- .x + runif(n=n_exposed, min=0, max=60*60*24-1)\n  # Simulate their event time.\n  rt = exp(-0.1*treatment)/(60*60*24*180)\n  latent_event_time <- rexp(n_exposed, rate = rt) + exposure_time\n  observed_time <- if_else(latent_event_time>end_time, NA, latent_event_time)\n  \n  tibble(\n    treatment, exposure_time, observed_time, latent_event_time\n  )\n  \n  \n}) %>% \n  mutate(userid = seq_along(treatment),\n         treatment = if_else(treatment == 1, \"Treatment\",\"Control\"))\n```\n:::\n\n\nI'm going to write these data to duckdb where I can write the necessary SQL.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ncon <- DBI::dbConnect(duckdb(), ':memory:')\ndbWriteTable(con, 'experiment_000', experiment_data)\n```\n:::\n\n\n\n## Survival Analysis in SQL\n\n\n::: {.cell output.var='dd'}\n\n```{.sql .cell-code}\nSET VARIABLE EXPERIMENT_START_DATE = TIMESTAMP '2024-10-01 00:00:00';\nSET VARIABLE EXPERIMENT_END_DATE = TIMESTAMP '2024-10-31 00:00:00';\n\n\nwith prep_cleaned_exposures as (\n  select distinct\n    userid, \n    treatment,\n    min(exposure_time) over (partition by userid) as exposure_time,\n    (min(treatment) over (partition by userid)) <> (max(treatment) over (partition by userid)) as multiple_exposures\n  from experiment_000\n  where \n    (\n    exposure_time <= getvariable('EXPERIMENT_END_DATE')\n    and\n    exposure_time >= getvariable('EXPERIMENT_START_DATE')\n    )\n  order by treatment, userid\n  \n)\n\n, cleaned_exposures as (select * from prep_cleaned_exposures where not multiple_exposures  )\n\n, exposure_balance as (select treatment, count(distinct userid) n_exposed from cleaned_exposures group by 1)\n\n\n, prep_experiment_outcomes as (\n  select \n    a.userid, \n    a.treatment, \n    a.exposure_time, \n    b.observed_time as event_time, \n    c.n_exposed as at_risk\n  from cleaned_exposures as a\n  left join experiment_000 as b \n    on a.userid = b.userid and a.exposure_time <  coalesce(b.observed_time, getvariable('EXPERIMENT_END_DATE'))\n  left join exposure_balance c on a.treatment = c.treatment\n)\n\n, cleaned_experiment_outcomes as (\n  select\n    userid, \n    treatment, \n    exposure_time, \n    event_time is not NULL as is_observed, \n    at_risk,\n    coalesce(event_time, getvariable('EXPERIMENT_END_DATE') ) as observed_time,\n    datediff('seconds', exposure_time, coalesce(event_time, getvariable('EXPERIMENT_END_DATE') )) * 1.0 / (86400) as event_time_days,\n  from prep_experiment_outcomes\n  order by treatment, userid\n)\n\n, prep_lifetable_1 as (\n\nselect\n  \n  treatment, \n  case when not is_observed then floor(event_time_days) else floor(event_time_days) + 0.5 end as event_time,\n  coalesce(count(distinct case when  is_observed then userid end), 0) as n_event,\n  coalesce(count(distinct case when not is_observed then userid end), 0) as n_censor,\n  at_risk as n_exposed\n  \nfrom cleaned_experiment_outcomes\ngroup by 1, 2, 5\norder by 1, 2\n\n)\n\n, life_table as (\n\nselect \n*, \ncoalesce(lag(at_risk1, 1) over (partition by treatment order by event_time), n_exposed) as at_risk,\nn_event::float / at_risk::float as hazard\nfrom(\n  select \n    *, \n    n_exposed - (sum(n_event + n_censor) over (partition by treatment order by event_time rows between unbounded preceding and current row)) as at_risk1\n  from prep_lifetable_1\n  order by 1, 2\n)\n\n)\n\nselect \n  *, \n  exp(sum(ln(1-hazard)) over (partition by treatment order by event_time rows between unbounded preceding and current row)) as survival_probability\nfrom life_table\norder by 1, 2\n\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nexperiment_data %>% \n  filter(\n    exposure_time>=start_time, \n    exposure_time <=end_time\n  ) %>% \n  transmute(\n    treatment = treatment, \n    event = if_else(is.na(observed_time), 0, 3),\n    time = floor(interval(exposure_time, coalesce(observed_time, end_time))/days(1)),\n    time2 = ceiling(interval(exposure_time, observed_time)/days(1))\n  ) -> d\n\n\n# Interval censoring.\nsurvfit(Surv(time=time, time2=time2, event=event, type='interval') ~ strata(treatment), data=d) -> f\n\n\nbroom::tidy(f) %>% \n  ggplot(aes(time, estimate, color=strata)) + \n  geom_step() \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndd  %>% \n  ggplot(aes(event_time, survival_probability, color=factor(treatment))) + \n  geom_step() + \n  geom_point(data=broom::tidy(f), aes(time, estimate, color=strata))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}