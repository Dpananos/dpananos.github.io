{
  "hash": "a67b85822f378251a03c5d706689fd28",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: IRLS Estimation in Compute Constrained Environments\ndate: 2025-07-19\ncode-fold: true\necho: true\nfig-cap-location: top\ncategories: []\nnumber-sections: false\ndraft: false\ncache: true\n---\n\n\n# Introduction\n\nVariance reduction is table stakes for online experimentation.  Variance reduction improves sensitivity of experiments, meaning you can achieve the same statistical power with fewer experimental subjects, meaning experiments run faster.\n\nDepending on the scale of your data, and the number of adjustment variables you want in your regression, it may not be feasible to extract all the data into a R or Python process and run the regression. Evan Miller, former Eppo Statisitcs Engineer, has spoken about just this kind of problem [here](https://www.infoq.com/presentations/eppo-warehouse/). I highly recommend watching that video in its entirety, but the gist is Eppo used to run CUPED by sucking data into a python process wherein it would estimate the required regression using statsmodels' implementation of OLS.  However, this caused a scaling issue at Eppo.\n\nThe solution was to create the requisite the required matrices, $X^TX$ and $X^Ty$, in SQL!  Then, pull those matrices from out of the warehouse, and estimate the coefficients of the regression in typescript.  Pretty impressive stuff.\n\nBut, why stop at linear regression?  Why not implement IRLS so that other regressions can be estimated?  Why not implement robust standard errors too! In this blog post, I extend some of Evan's thinking (marginally) so as to implement Poisson regression and robust covariance estimation.  Why those two things?  Poisson regression's coefficients estimate what we would call \"the lift\" (a multiplicative effect).  However, the variance structure of Poisson regression is too restrictive, and would give inappropriate coverage if we used the standard errors from the model as fit.  We'll need robust covariance estimation so as to have better standard errors.\n\n# An Example\n\nI've got a duckdb database that has so much data, it would not fit on a [t3.micro](https://aws.amazon.com/ec2/instance-types/) instance on AWS.  That is one of the cheaper instances, so we could either spend more money or use Evan's appraoch.  I've set up a docker container to mimic the contraints of the t3.micro.  Let's try running this simple python script in that container with the large dataset I've created.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n``` python\nimport pandas as pd\nimport duckdb\nfrom pyfixest import feols\n\n\ndef main():\n    conn = duckdb.connect('your_cloud_warehouse.db')\n    df = conn.execute(\"SELECT * FROM regression_data\").fetch_df()\n    model = feols('y ~ trt + x2 + x3 + x4_A + x4_B + x4_C + x4_D + x4_E', data=df)\n\n    print(model.summary())\n\nif __name__ == \"__main__\":\n    main()\n```\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n#0 building with \"desktop-linux\" instance using docker driver\n\n#1 [internal] load .dockerignore\n#1 transferring context: 2B done\n#1 DONE 0.0s\n\n#2 [internal] load build definition from Dockerfile\n#2 transferring dockerfile: 1.46kB done\n#2 DONE 0.0s\n\n#3 [internal] load metadata for docker.io/library/python:3.12-slim\n#3 DONE 0.3s\n\n#4 [ 1/11] FROM docker.io/library/python:3.12-slim@sha256:4600f71648e110b005bf7bca92dbb335e549e6b27f2e83fceee5e11b3e1a4d01\n#4 DONE 0.0s\n\n#5 [internal] load build context\n#5 transferring context: 325B done\n#5 DONE 0.0s\n\n#6 [ 9/11] RUN chmod 666 your_cloud_warehouse.db\n#6 CACHED\n\n#7 [10/11] RUN echo \"ulimit -v 1048576\" >> /etc/profile\n#7 CACHED\n\n#8 [ 2/11] RUN echo 'ulimit -v 1048576' >> /etc/bash.bashrc\n#8 CACHED\n\n#9 [ 3/11] WORKDIR /app\n#9 CACHED\n\n#10 [ 8/11] COPY your_cloud_warehouse.db .\n#10 CACHED\n\n#11 [ 7/11] COPY pythonscripts/perform_python_regression.py ./pythonscripts/\n#11 CACHED\n\n#12 [ 6/11] RUN uv pip install --system -e .\n#12 CACHED\n\n#13 [ 4/11] COPY pyproject.toml .\n#13 CACHED\n\n#14 [ 5/11] RUN pip install uv\n#14 CACHED\n\n#15 [11/11] RUN echo '#!/bin/bash\\nset -e\\necho \"Starting with memory limit of 1GB (t3.micro equivalent)\"\\nulimit -v 1048576\\nulimit -m 1048576\\necho \"Memory limits set:\"\\nulimit -a | grep -E \"(virtual memory|resident set size)\"\\necho \"Starting Python regression...\"\\nexec python pythonscripts/perform_python_regression.py\\n' > /app/run.sh && chmod +x /app/run.sh\n#15 CACHED\n\n#16 exporting to image\n#16 exporting layers done\n#16 writing image sha256:96d846f2cfa5ecf8ac67ab6f4e7cd4013c4efb21fa4dc27c7e5061628be62dce done\n#16 naming to docker.io/library/ols-regression done\n#16 DONE 0.0s\n\nView build details: docker-desktop://dashboard/build/desktop-linux/desktop-linux/p9agvxyulg4be27oawiupxjyu\n\nWhat's Next?\n  1. Sign in to your Docker account â†’ docker login\n  2. View a summary of image vulnerabilities and recommendations â†’ docker scout quickview\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.bash .cell-code  code-fold=\"true\"}\ndocker run --rm --memory=1g --memory-swap=1g ols-regression\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStarting with memory limit of 1GB (t3.micro equivalent)\nMemory limits set:\nvirtual memory              (kbytes, -v) 1048576\nStarting Python regression...\nDatabase connection established successfully\nAttempting to load data into memory...\n\n============================================================\nðŸš¨ MEMORY ERROR DETECTED ðŸš¨\n============================================================\nThe container has run out of memory as expected!\nThis demonstrates the memory constraints of a t3.micro instance.\nError details: cannot allocate memory for array\nContainer is exiting gracefully...\n============================================================\n```\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nDeleted Images:\nuntagged: ols-regression:latest\ndeleted: sha256:1b62be1380ba5c91c8cf864a47bef05b31403b37a5c22a068a2b7ff290739a50\n\nDeleted build cache objects:\nbu9wd0rlrf84jt8puaudhqrfm\nvqo7ndvv94d4x3dmq9fez3ttx\nslkh4uzqz0c79j8xpthc3aerr\n6p0wdfp3n20ng7wxsrhtcfvws\nnaekdlkl2pweahw876aa5htbp\nnetrc36fshjbjrq7f59c0o0mo\nayh6j5ng349i6mqbjuoxpal6d\ns0f9vy75gzwfbjplu0fzlcl6w\neq70rganzm3ik8dm066vr87k7\nsotxalxytujir3zpyribl7y6x\nyc8jqovd2dnksg892ck7q2xri\nm03vsiktatfcwhz9ekjaiqgjt\nfzpl7fuhptclw4nz5ksen1szo\ngchjwhmpn4l29r8fcfu01xn58\n4sik8vwfr32kyjmbjy7165d7g\nica6wo2wf2mt1hdvdz6dsckv5\nhbstr0nty4grkefss3519wim2\n\nTotal reclaimed space: 17.14GB\n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}