{
  "hash": "bd99399a2488cd01deb4eca96297a49e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Kuethe's Criterion and A/B Tests\ndate: 2025-11-21\necho: true\nfig-cap-location: top\ncategories: []\nnumber-sections: false\ndraft: false\ncache: false\n---\n\n\n\n## On The Validity of a Normal Approximation to The Ratio of Means\n\nA/B tests often target the \"lift\" in the metric as an estimand.  As a reminder, the lift is\n\n$$ \\lambda = \\dfrac{E[Y(1)] - E[Y(0)]}{E[Y(0)]} $$\nand is interpreted as the percent improvement treatment had over control. Clearly, the lift can be problematic when $E[Y(0)]$ -- the expected value of the outcome under no treatment -- is too close to 0.  This is not a revolutionary insight and has been a popular critique of the estimand.\n\nEppo (By Datadog) is familiar with the problem with lift, and as a consequence will not show the lift and associated confidence intervals when the control group's average outcome is within 10 standard errors of 0.  \n\nBut why 10 standard errors? Ten is a nice number, but feels somewhat arbitrary. That decision came from a paper entitled [\"On the existence of a normal approximation to the distribution of the ratio of two independent normal random variables\"](https://www.researchgate.net/profile/F-Rubio/publication/257406150_On_the_existence_of_a_normal_approximation_to_the_distribution_of_the_ratio_of_two_independent_normal_random_variables/links/53d7a18a0cf2e38c632ddabc/On-the-existence-of-a-normal-approximation-to-the-distribution-of-the-ratio-of-two-independent-normal-random-variables.pdf) by Eloisa Diaz-Frances and Francisco J. Rubio. I'll give you the short version of the justification and leave you to read the paper if you like.\n\nIn brief, Diaz-Frances and Rubio study the sampling distribution of the ratio of two normals with positive means.  Note, that is is related to A/B testing because a) the lift is a shifted version of such a random variable, and b) the CLT gives us good justification for treating the sample means as Gaussian random variables [^1]. Diaz-Frances and Rubio reference a paper by Kuethe and colleagues called [\"Imaging obstructed ventilation with NMR using inert fluorinated gases\"](https://pubmed.ncbi.nlm.nih.gov/10846046/) who also studied normal approximations such random variables and concluded that when the denominator's coefficient of variation is smaller than 0.1 (or, when the mean is 10 standard deviations away from 0) then there is a really small probability of observing a negative value from the normal approximation to the true sampling distribution.  This is what we call \"Kuethe's criterion\".  In A/B testing, the denominator's standard deviation is the standard error, and this is where our 10 standard error rule came from.  In short, we consider the normal approximation to the lift's sampling distribution valid only when Kuethe's criterion is met.\n\n[^1]: Assuming we have enough data that this is a good assumption.  Berry-Esseen theorem applies here, but that is another blog post altogether.\n\nKuethe and colleagues report that the probability of observing a negative value from the normal approximation is something on the order of $10^-24$ which seems excessively small.  Also, I don't care about the probability of a negative value, I care much more about the coverage of my resulting confidence interval.  How does coverage change when Kuethe's criterion is not met?\n\n## A Very Short Simulation of Coverage\n\nIt should be straight forward to determine the impact of beign $M$ standard errors away from the mean.  Consider a random variable $y$ such that\n\n$$ y \\sim \\mathcal{N}(M, 1^2 )\\>. $$\n\nNote that the coefficient of variation for $y$ is $1/M$ and so when $M\\geq10$, then Kuethe's criterion is satisfied.  To simulate what happens to coverage, I just need to simulate a bunch of these random variables for treatment and control, compute the standard error via the delta method, and determine coverage.  The simulation is actually pretty short.  \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbetween <- function(x, low, hi) (x<=hi) & (low <= x)\n\nsim_coverage <- function(Mc){\n  \n  actual_lift <- 0.0\n  Mt <- Mc * (actual_lift+1)\n  \n  # With this many simulations, I can be certain about the first 2 decimal places\n  Nsims <- 100000\n  \n  # The following random variables are guarenteed to have a coefficient of variation <= 0.1\n  yc <- rnorm(Nsims, Mc, 1)\n  yt <- rnorm(Nsims, Mt, 1)\n  lift <- yt/yc-1\n  \n  # Because we know their standard deviation, we can apply the delta method for the lift\n  se <- sqrt((Mt/Mc)^2 * (1/Mc^2 + 1/Mt^2))\n  \n  # Now, we can determine coverage\n  conf_lower <- lift - 1.96*se\n  conf_upper <- lift + 1.96*se\n  coverage <- mean(between(actual_lift, conf_lower, conf_upper))\n  \n  return(coverage)\n}\n\n\nM <- seq(2, 30, 0.25)\n\ncoverage <- sapply(M, sim_coverage)\nf <- loess(coverage ~ M)\n\n# First plot normally\nplot(M, predict(f), ylim = c(0.8, 1), type='l',\n     xlab = \"Standard Deviations Away From 0\",\n     col='red', ylab = \"Coverage of 95% CI\")\n\n# Shade vertical region where M >= 10\nusr <- par(\"usr\")  # xmin, xmax, ymin, ymax\n\nrect(xleft = 10,\n     ybottom = usr[3],\n     xright = usr[2],\n     ytop = usr[4],\n     col = adjustcolor(\"lightgray\", alpha.f = 0.3),\n     border = NA)\n\n# Redraw the line so it appears on top\nlines(M, predict(f), col='red')\n\ntext(20, 0.975, \"Kuethe's Criterion\\nMet\")\ntext(5, 0.975, \"Kuethe's Criterion\\nNot Met\")\ngrid()\nabline(h = 0.95)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}