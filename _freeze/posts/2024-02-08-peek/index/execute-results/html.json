{
  "hash": "7db3fd6f3382fa99e95f20c7bd03a0d6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Peeking Sets You Up For Dissapointment\ndate: \"2024-02-08\"\ncode-fold: true\necho: true\nfig-cap-location: top\ncategories: []\nnumber-sections: true\n---\n\n\n\nPeeking (looking for significance in an AB test before the experiment has enough samples to reach desired power) is a \"no no\".  Rationales for not peeking typically mention inflated type 1 error rate.\n\nUnless you're just randomizing into two groups and not changing anything, the null is unlikely to be true.  So inflated type one error rate is really not the primary concern.  Rather, if we peek then we are setting ourselves up for disappointment.  Detected effects from peeking will typically not generalize, and we will be overstating out impact.  The reason why is [fairly clear when considering the Winner's Curse](https://dpananos.github.io/posts/2024-01-22-curse/). \n\n\nThe long and the short of it is as follows:\n\n* When you peak, your tests are under powered.\n* Statistically significant results from under powered tests generally over estimate the truth (see my post on the Winner's curse for why).\n* So when you detect an effect from peeking, you are very likely over estimating your impact.  When you roll out the change globally, you're probably not going to see the impact you expected.  This can lead to disappointment (and a lot of questions from everyone when they don't see changes to the numbers on a dashboard).\n\n\n\n\n::: {.cell}\n\n:::\n\n\nAs a concrete example, say you design an experiment to detect a 3% lift in a binary outcome, and say your intervention truly does improve the outcome by 3%.  The baseline is 10%, and you design your experiment to have 80% power with a 5% false positive rate.  You're going to need a total of 318,132 users in your experiment...which sounds like a lot.  What if we instead checked for statistical significance each time the groups hit a multiple of 20, 000. Depending on how fast we can accrue users into the experiment, this could save time...right?\n\nShown below is a line plot of the relative error between what you would expect to detect at each peek and the true lift. The plot speaks for itself; conditional on finding an effect early, you're likely over estimating the truth. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- c(seq(20000, 2*N, 20000), 2*N)\nlog_rr_mean <-  log(pt) - log(pc)\nlog_rr_var <-  (1-pt)/(pt*n) + (1-pc)/(pc*n)\nlog_rr_sd <-  sqrt(log_rr_var)\n\nz <- log_rr_mean - log_rr_sd * qnorm(0.8)\ntrunc <- log_rr_mean + log_rr_sd * dnorm((z - log_rr_mean)/ log_rr_sd) / (1 - pnorm((z - log_rr_mean)/ log_rr_sd))\n\n# Plotting with formatted axes ticks\nplot(n, abs( log_rr_mean-trunc)/log_rr_mean, type='l', xlab='Sample Size', ylab='Relative Error')\n\n\npoints(n, abs(log_rr_mean-trunc)/log_rr_mean, pch=19)\n```\n\n::: {.cell-output-display}\n![Relative error in estimating the true impact as a function of the total sample size.  Detecting a statistically significant difference early may save you time, but it comes at the expense of over estimating the true impact you have made by nearly 35\\%.  Since we don't know the true impact of our interventions, it is difficult to know if we are over estimating on any one experiment.  However, peeking as a practice will almost surely result in inflated expectations and great dissapointments.](index_files/figure-html/unnamed-chunk-2-1.png){width=768}\n:::\n:::\n\n\n\nNow, keep in mind that a 35% relative error means you would estimate the lift to be 4% instead of 3%.  Maybe that doesn't sound so bad, and for a single experiment it isn't.  But _habitual_ peeking means this error compounds experiment over experiment.  Maybe something to talk to your PMs about.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}