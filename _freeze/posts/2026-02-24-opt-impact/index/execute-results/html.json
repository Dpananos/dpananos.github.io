{
  "hash": "76dcab91e850bb602f166e42b119777d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Optimal Experiment Sample Sizes for Bayesian A/B Tests\ndate: 2026-02-24\ncode-fold: false\necho: false\nfig-cap-location: top\ncategories: []\nnumber-sections: false\ndraft: true\ncache: false\n---\n\n\n\n\nThis post is a spiritual successor to an [early post on this blog](../2023-03-31-optimal-mde/index.qmd) and a fast follow here from [this post about A/B test run times for Bayesians](../2026-02-20-time/index.qmd).  From that post, we are able to obtain an analytic approximation to the expected impact from a single experiment using a Bayesian analysis and a given decision rule.  Below is a plot like the one found in my other post, but this time perhaps with a more realistic prior.\n\nThese curves tell me what my expected impact is going to be at given sample sizes, but not what sample size I should choose. Obviously, I want to impact the product as much as possible, but I don't want to spend months and months on a single experiment.  \n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Simulated (open markers) and closed-form analytic (black lines) expected impact for all three decision rules. The red horizontal line is the Clairvoyant upper bound. Prior is $\\operatorname{Normal}(0.005, 0.01^2)$, $\\sigma = 0.5$, equal allocation.](index_files/figure-html/optimistic-prior-comparison-1.png){width=672}\n:::\n:::\n\n\n\nLet's assume I can accrue $N$ total subjects per week.  I can either choose to run more experiments at a lower expected impact, or run fewer experiments with higher impact.  Remember, not every experiment is going to be a \"ship\" decision, this expected impact is over all experiments. There should be some sweet spot where I am running sufficiently many experiments with sufficiently high impact.  Intuitively, this should be the sample size we choose for our A/B tests.  For the curves above, the plot I describe looks something like the one shown below.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Estimated cumulative quarterly impact as a function of experiment size, assuming 2,500 subjects per week and experiments run back to back. Larger experiments yield higher per-experiment impact but fewer experiments per quarter. Prior is $\\operatorname{Normal}(0.005, 0.01^2)$.](index_files/figure-html/quarterly-impact-1.png){width=672}\n:::\n:::\n\n\nFrom this plot, we learn a few things:  First, the decision rule to ship only if the credible interval excludes 0 is dominated by the probability to beat decision rule in both quarterly expected impact (at least for the parameters I've used here), and we can achieve higher impact with fewer samples per experiment (roughly 780 fewer assuming the optima are at $\\log_{10}(N)=3.25$ and 3.5 respectively).  The one thing this plot is missing is some sort of uncertainty for these curves -- remember, these are expectations and the true cumulative impact is going to vary about these points, perhaps drastically so!  I think that is a better job for simulation.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Simulated quarterly impact distributions (shaded bands: 5thâ€“95th percentile; open markers: mean and median) overlaid on the closed-form curves (lines) from the previous plot. Simulation uses 10,000 quarters per sample size on a coarser grid. At large $N$, fewer than one experiment fits in a quarter, so impact collapses to zero.](index_files/figure-html/quarterly-impact-sim-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}