{
  "hash": "54feb4a1663767c6046253acb317d134",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Pre Randomized Experiment Differences Don't Mean DiD is Needed\ndate: 2025-08-16\ncode-fold: true\necho: true\nfig-cap-location: top\ncategories: []\nnumber-sections: false\ndraft: false\ncache: true\n---\n\n\n[Take a look at this interaction](https://stats.stackexchange.com/a/669596/111259) I had with user jginestet on Cross Validated.  In short, jginestet suggests that a randomized experiment assessing percent changes in tumor volume should adjust for pre-experiment differences (at this point, I'm nodding yes) and therefore the most appropriate method is DiD -- difference in differences -- (at this point, I'm shaking my head no).\n\nUser jginestet is right, pre-experiment differences should be adjusted for, but he is wrong when he suggests DiD is the way to do it.  Let me explain\n\n## Point 1: Before Intervention, Treatment and Control are Samples From The Same Population\n\nBy virtue of this point, the two groups have the same pre-experiment mean outcome _in expectation_.  The sample means are going to differ because of sampling variability.  If $\\bar{Y}_i$ is the sample mean in group $i \\in \\left\\{t, c\\right\\}$, then mathematically this means that *before intervention* $E[\\bar{Y}_t] = E[\\bar{Y}_c] = E[Y]$.  The same can't be said for post intervention, and that is why we run the experiment.  What does this mean for DiD?\n\n\nLet $\\bar{Y}_{i, j}$ indicate sample mean for group $i \\in \\left\\{ t, c \\right\\}$ in time period   $j \\in \\left\\{ 0, 1 \\right\\}$ (here 0 means before intervention, 1 means after intervention).  DiD estimates the following quantity\n\n$$ \\widehat{\\Delta} = (\\bar{Y}_{t, 1} - \\bar{Y}_{t, 0}) - (\\bar{Y}_{c, 1} - \\bar{Y}_{c, 0})  $$\nIf the treatment assignment is randomized, something nice happens now. Taking expectations\n\n$$ \\begin{align} \nE[\\widehat{\\Delta}] &= E[(\\bar{Y}_{t, 1} - \\bar{Y}_{t, 0}) - (\\bar{Y}_{c, 1} - \\bar{Y}_{c, 0})] \\\\ \n&= E[\\bar{Y}_{t, 1}] - E[\\bar{Y}_{t, 0}] - E[\\bar{Y}_{c, 1}] + E[\\bar{Y}_{c, 0}] \\\\ \n&= E[\\bar{Y}_{t, 1}] - E[\\bar{Y}_{0}] - E[\\bar{Y}_{c, 1}] + E[\\bar{Y}_{0}]\\\\\n&= E[\\bar{Y}_{t, 1}] - \\cancel{E[\\bar{Y}_{0}]} - E[\\bar{Y}_{c, 1}] + \\cancel{E[\\bar{Y}_{0}]} \\\\\n&= E[\\bar{Y}_{t, 1}] - E[\\bar{Y}_{c, 1}]\n\\end{align}$$\n\nWhich is the difference in means after the intervention.  DiD is not needed in randomized experiments because the pre-treatment difference is 0 in expectation. We don't need to adjust for pre-experiment systematic differences because there are none.  Both treatment and control are samples from the same population.  They therefore share population parameters.  This is unlike most applications of DiD where groups do not share population parameters.\n\n\n## Point 2: If You Have Pre Experiment Data, You Should Condition On It!\n\nThat being said, there is value to adjusting for pre-experiment data, just not with DiD.  Consider the following simulation:\n\n\n* I simulate a randomized experiment.\n* I have a highly correlated, pre-experiment outcome (e.g. could be weight in an intervention intended to help people lose weight).  This is important since if the pre-experiment outcome was not correlated with post experiment outcome, the whole point of using, or even checking, pre-experiment outcomes would be moot.  \n* I fit a model using treatment indication with and without the pre-experiment data.\n* I compare coverage of the confidence intervals, hoping that they would have nominal coverage.\n\nAnd, if you run this simulation, you find they do indeed have nominal coverage.  But something interesting happens when you examine coverage as a function of the pre experiment differences.  See the plot below.\n\nThis plot shows the coverage of the 95% confidence interval for the treatment effect within deciles of the standardized pre-experiment difference (essentially the absolute value of the z score for the difference).  Note that for ANCOVA, the coverage is always at the nominal level, regardless of how big the pre-experiment differences are.  But the difference in means has larger than expected coverage when pre-experiment differences are \"small\", and wildly low coverage in cases where pre experiment differences are big.  Despite this, _each method maintains nominal coverage across all simulated experiments_.\n\nWhy does this matter?  This plot is telling me that, so long as the outcomes are highly correlated, then pre-experiment differences should be adjusted for ... with ANCOVA.  These points are nicely summarized at a higher level in the section titled \"Balance of prognostic factors is necessary for valid inference\" from Senn's [\"Seven myths of randomization in clinical trials\"](https://onlinelibrary.wiley.com/doi/10.1002/sim.5713).  \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nr <- map_dfr(1:10000, ~{\n  \n  n <- 1000\n  treatment <- rbinom(n, 1, 0.5)\n  Y <- MASS::mvrnorm(n, c(0, 0), matrix(c(1, 0.85, 0.85, 1), nrow=2))\n  \n  y_post <- Y[,1]\n  y_pre <- Y[,2]\n  \n  pre_experiment_z <- t.test(y_pre ~ treatment)$statistic\n  \n  fit_ols <- lm(y_post ~ treatment)\n  fit_ancova <- lm(y_post ~ treatment + y_pre)\n  models <- list('Diff in Means' = fit_ols, 'ANCOVA' = fit_ancova)\n  \n  models %>% \n    imap_dfr(~{\n      broom::tidy(.x, conf.int=T) %>% \n        filter(term=='treatment')} %>% \n        mutate(pre_experiment_z = pre_experiment_z, \n               cover = !(sign(conf.low) == sign(conf.high)), \n               model=.y)\n      )\n  \n  \n})\n\nr %>% \n  group_by(\n    model, \n    decile = ntile(abs(pre_experiment_z), 10)\n  ) %>% \n  summarise(\n    coverage = mean(cover),\n    .groups = \"drop\"\n  ) %>% \n  ggplot(aes(x = decile, y = coverage, color = model)) + \n  geom_line(size = 1) +\n  geom_hline(yintercept = 0.95, linetype = \"dashed\", color = \"gray50\") +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(\n    title = \"Coverage by Pre-Experiment Score Decile\",\n    x = \"Decile of Pre-Experiment Difference in Means (|Z score|)\",\n    y = \"Coverage\",\n    color = \"Model\"\n  ) +\n  scale_y_continuous(labels = scales::percent) + \n  scale_x_continuous(breaks = 1:10) + \n  theme_minimal(base_size = 14) +\n  theme(\n    legend.position = \"top\"\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\n## Conclusion\n\nSo, should pre experiment imbalance be adjusted for?  Well yes.  We've seen that coverage in this case can be quite small when imbalance is big and pre experiment outcomes are highly correlated with post experiment outcomes. If you know the imbalance is big then presumably you have the data to demonstrate that, so you can adjust for it with ANCOVA.\n\nCould you use DiD?  Yes, you could, but there is no need to do so since the pre experiment population means are the same in expectation.\n\nThose who insist on DiD in the randomized case are right -- that we should adjust for pre experiment imbalance -- for the wrong reasons -- because I presume they think there is a systematic difference between groups; there isn't.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}