[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Quarto Blog",
    "section": "",
    "text": "I’ve moved my blog to Quarto! It will take some time to migrate my old posts here so please be patient."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Blog About Statistics, Mathematics, and Other Stuff",
    "section": "",
    "text": "Statistics\n\n\nMachine Learning\n\n\nPython\n\n\nScikit-Learn\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\nDemetri Pananos\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 22, 2022\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNews\n\n\n\n\n\n\n\n\n\n\n\nJun 19, 2022\n\n\nDemetri Pananos\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nBayes\n\n\nStan\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2022\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2021\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2020\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nProbability\n\n\n\n\n\n\n\n\n\n\n\nAug 31, 2018\n\n\nDemetri Pananos\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPython\n\n\nRiddler\n\n\n\n\n\n\n\n\n\n\n\nDec 29, 2017\n\n\nDemetri Pananos\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "PhDemetri",
    "section": "",
    "text": "I’m a Senior Data Scientist at Zapier focusing on improving A/B Tests and experimentation."
  },
  {
    "objectID": "posts/2017-12-29-coins/index.html",
    "href": "posts/2017-12-29-coins/index.html",
    "title": "Coins and Factors",
    "section": "",
    "text": "I love Fivethirtyeight’s Riddler column. Usually, I can solve the problem with computation, but on some rare occasions I can do some interesting math to get the solution without having to code. Here is the first puzzle I ever solved. It is a simple puzzle, yet it has an elegant computational and analytic solution. Let’s take a look.\nThe puzzle says:"
  },
  {
    "objectID": "posts/2017-12-29-coins/index.html#computing-the-solution",
    "href": "posts/2017-12-29-coins/index.html#computing-the-solution",
    "title": "Coins and Factors",
    "section": "Computing the Solution",
    "text": "Computing the Solution\nThis is really easy to program. Here is a little python script to compute the solution:\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom itertools import product\n\n\n# Array of 100 True.  True is Heads up\nNcoins = 100\ncoins = np.ones(Ncoins,dtype = bool)\nindex = np.arange(1,Ncoins+1)\n\n#Go through the coins\nfor N in range(1,Ncoins+1):\n    \n    coins[index%N==0] = ~coins[index%N==0]  #Flip the coin.  \nShown below is the solution. In dark blue are the coins face down (I’ve arranged them in a 10 by 10 grid and annotated them with their position for clarity). When we take a look at the coins variable, we see that those coins in positions which are perfect squares pop out. That is an interesting result, but what is more interesting is reasoning out the solution without doing any computation at all!\n\n\n\n\n\nFigure 1: Coins after flipping. Dark squares are face down coins."
  },
  {
    "objectID": "posts/2017-12-29-coins/index.html#reasoning-out-the-solution.",
    "href": "posts/2017-12-29-coins/index.html#reasoning-out-the-solution.",
    "title": "Coins and Factors",
    "section": "Reasoning Out the Solution.",
    "text": "Reasoning Out the Solution.\nFirst, let’s think why a coin would end up face down. If all coins start heads up, then it would take an odd number of flips for the coin to end up face down. Since coins are only flipped when we pass a factor of a coin’s position, then those coins in positions with an odd number of factors will be heads down at the end.\nSo 9 would end up heads down because it has factors 1 (flip face down), 3 (flip face up), and 9 (flip face down), while 6 would be heads up because it has factors 1, 2, 3, and 6.\nSo which numbers have an odd number of factors? Here is where we get to do some interesting math. The Fundamental Theorem of Arithmetic says that every integer \\(N>1\\) is either prime or can be uniquely factored as a product of primes\n\\[N = \\prod_{j} p_j^{a_j} \\>.\\]\nIf \\(N\\) can be factored like this, that means it has\n\\[\\prod_{j} (a_j +1)\\]\nunique factors.\nIt is straight forward to argue that a composite odd number must be the product of odd numbers, so we know that the \\(a_j+1\\) must be odd \\(\\forall j\\), and so that means the \\(a_j\\) are even and can be written as \\(a_j = 2n_j\\). Thus, our factorization becomes\n\\[N = \\prod_j p_j^{2n_j} = \\prod_j (p_j^{n_j})^2 = \\left(\\prod_j p_j^{n_j} \\right)^2 \\>,\\]\nwhich means that if \\(N\\) has an odd number of factors, it must be a perfect square! All done.\nI love trying to solve the Riddler’s puzzle without coding. It makes me draw upon knowledge I haven’t used in a while, and may force me to learn something new."
  },
  {
    "objectID": "posts/2018-08-31-combinatorics/index.html",
    "href": "posts/2018-08-31-combinatorics/index.html",
    "title": "Neat Litle Combinatorics Problem",
    "section": "",
    "text": "We could always just sample form the set to estimate the expected value. Here is a python script to do just that.\n\nimport numpy as np\nx = np.array([49, 8, 48, 15, 47, 4, 16, 23, 43, 44, 42, 45, 46])\n\nmins = []\nfor _ in range(1000):\n    mins.append(np.random.choice(x,size = 6, replace = False).min())\n\nprint(np.mean(mins))\n\n9.098\n\n\nBut that is estimating the mean. We can do better and directly compute it. Here is some python code to create all subsets from \\(S\\) of size 6. Then, we simply take out the minimum from each subset and compute the mean.\n\nimport numpy as np\nfrom itertools import combinations, groupby\n\nx = np.array([49, 8, 48, 15, 47, 4, 16, 23, 43, 44, 42, 45, 46])\nx = np.sort(x)\n\nc = list(combinations(x,6))\n\nmins = list(map(lambda x: x[0], c))\n\ns = 0\nfor k, g in groupby(sorted(mins)):\n    s+=k*(len(list(g))/len(mins))\n\nprint( s )\n\n8.818181818181818\n\n\nThe script returns 8.18 repeating. Great, but we can do even better! If we can compute the probability density function, we can compute the mean analytically. Let’s consider a smaller problem to outline the solution.\nLet our set in question be \\((1,2,3,4,5)\\). Let the minimum of a sample of 3 numbers from this set be the random variable \\(z\\). Now, note there are \\(\\binom{5}{3} = 10\\) ways to choose 3 elements from a set of 5.\nHow many subsets exist where the minimum is 1? Well, if I sampled 1, then I would still have to pick 2 numbers from a possible 4 numbers larger than 1. There are \\(\\binom{4}{2}\\) ways to do this. So \\(p(z=1) = \\binom{4}{2} / \\binom{5}{3}\\).\nIn a similar fashion, there are \\(\\binom{3}{2}\\) subsets where 2 is the minimum, and \\(\\binom{2}{2}\\) subsets where 3 is the minimum. There are no subsets where 4 or 5 are the minimum (why?). So that means the expected minimum value for this set would be\n\\[\\operatorname{E}(z) = \\dfrac{ \\sum_{k = 1}^{3} k\\binom{5-k}{2} }{\\binom{5}{3}}  \\]\nWhatever that sum happens to be. Here is how you could code up the analytic solution to our problem.\n\nimport numpy as np\nfrom scipy.special import binom\n\nx = np.array([ 4, 8, 15, 16, 23, 42, 43, 44, 45, 46, 47, 48, 49])\nx = np.sort(x)\n\nsample_size =6\nsample_space = x[:-(sample_size-1)]\nE = 0\nfor i,s in enumerate(sample_space,start = 1):\n\n    E+= s*binom(x.size-i,sample_size-1)\n\nprint(E/binom(x.size, sample_size))\n\n8.818181818181818\n\n\nFull disclosure, this was on a job application (literally, on the job application), so sorry KiK for putting the answer out there, but the question was too fun not to write up!"
  },
  {
    "objectID": "posts/2020-10-06/index.html",
    "href": "posts/2020-10-06/index.html",
    "title": "Log Link vs. Log(y)",
    "section": "",
    "text": "set.seed(0)\nN = 1000\ny = rlnorm(N, 0.5, 0.5)\n\nand explain why glm(y ~ 1, family = gaussian(link=log) and lm(log(y)~1) produce different estimates of the coefficients. In case you don’t have an R terminal, here are the outputs\n\nlog_lm = lm(log(y) ~ 1)\nsummary(log_lm)\n\n\nCall:\nlm(formula = log(y) ~ 1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.61028 -0.34631 -0.02152  0.35173  1.64112 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.49209    0.01578   31.18   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.499 on 999 degrees of freedom\n\n\n\nglm_mod = glm(y ~ 1 , family = gaussian(link=log))\nsummary(glm_mod)\n\n\nCall:\nglm(formula = y ~ 1, family = gaussian(link = log))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.5282  -0.6981  -0.2541   0.4702   6.5869  \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.61791    0.01698    36.4   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.9918425)\n\n    Null deviance: 990.85  on 999  degrees of freedom\nResidual deviance: 990.85  on 999  degrees of freedom\nAIC: 2832.7\n\nNumber of Fisher Scoring iterations: 5\n\n\nAnswer is the same as the difference between \\(E(g(X))\\) and \\(g(E(X))\\) which are not always the same. Let me explain.\nFirst, let’s start with the lognormal random variable. \\(y \\sim \\operatorname{Lognormal}(\\mu, \\sigma)\\) means \\(\\log(y) \\sim \\operatorname{Normal}(\\mu, \\sigma)\\). So \\(\\mu, \\sigma\\) are the parameters of the underlying normal distribution. When we do lm(log(y) ~ 1), we are modelling \\(E(\\log(y)) = \\beta_0\\). So \\(\\beta_0\\) is an estimate of \\(\\mu\\) and \\(\\exp(\\mu)\\) is an estimate of the median of the lognormal. That is an easy check\n\nmedian(y)\n\n[1] 1.600898\n\n\n\n#Meh, close enough\nexp(coef(log_lm))\n\n(Intercept) \n   1.635723 \n\n\nIf I wanted an estimate of the mean of the lognormal, I would need to add \\(\\sigma^2/2\\) to my estimate of \\(\\mu\\).\n\nmean(y)\n\n[1] 1.855038\n\n\n\n#Meh, close enough\nsigma = var(log_lm$residuals)\nexp(coef(log_lm) + sigma/2)\n\n(Intercept) \n   1.852594 \n\n\nOk, onto the glm now. When we use the glm, we model \\(\\log(E(y)) = \\beta_0\\), so we model the mean of the lognormal directly. Case in point\n\nmean(y)\n\n[1] 1.855038\n\n\n\nexp(coef(glm_mod))\n\n(Intercept) \n   1.855038 \n\n\nand if I wanted the median, I would need to consider the extra factor of \\(\\exp(\\sigma^2/2)\\)\n\nmedian(y)\n\n[1] 1.600898\n\n\n\nexp(coef(glm_mod) - sigma/2)\n\n(Intercept) \n   1.637881 \n\n\nLog link vs. log outcome can be tricky. Just be sure to know what you’re modelling when you use either."
  },
  {
    "objectID": "posts/2021-04-03-confidence-intervals/index.html",
    "href": "posts/2021-04-03-confidence-intervals/index.html",
    "title": "On Interpretations of Confidence Intervals",
    "section": "",
    "text": "The 95% in 95% confidence interval refers not to the probability that any one interval contains the estimand, but rather to the long term relative frequency of the estimator containing the estimand in an infinite sequence of replicated experiments under ideal conditions.\nNow, if this were twitter I would get ratioed so hard I might have to take a break and walk it off. Luckily, this is my blog and not yours so I can say whatever I want with impunity. But, rather than shout my opinions and demand people listen, I thought it might be a good exercise to explain to you why I think this and perhaps why people might disagree. Let’s for a moment ignore the fact that the interpretation I use above is the de jure definition of a confidence interval and instead start where a good proportion of statistical learning starts; with a deck of shuffled cards.\nI present to you a shuffled deck. Its a regular deck of cards, no funny business with the cards or the shuffling. What is the probability the top card of this deck an ace? I’d wager a good portion of people would say 4/52. If you, dear reader, said 4/52 then I believe you have made a benign mistake, but a mistake all the same. And I suspect the reason you’ve made this mistake is because you’ve swapped a hard question (the question about this deck) for an easier question (a question about the long term relative frequencies of coming to shuffled decks with no funny business and finding aces).\nSwapping hard questions for easy questions is not a new observation. Daniel Khaneman writes about it in Thinking Fast and Slow and provides numerous examples. I’ll repeat some examples from the book here. We might swap the question:\nThe book Thinking Fast and Slow explains why we do this, or better yet why we have no control over this. I won’t explain it here. But it is important to know that this is something we do, mostly unconsciously.\nSo back to the deck of cards. Questions about the deck in front of you are hard. Its either an ace or not, but you can’t tell! The card is face down and there is no other information you could use to make the decision. So, you answer an easier one using information that you do know, namely the number of aces in the deck, the number of cards in the deck, the information that each card is equally likely to be on top given the fact there is no funny business with the cards or the shuffling, and the basic rules of probability you might have learned in high school if not elsewhere. But the answer you give is for a fundamentally different question, namely “If I were to observe a long sequence of well shuffled decks with no funny business, what fraction of them have an ace on top?”. Your answer is about that long sequence of shuffled decks. It isn’t about any one particular deck, and certainly not the one in front of you.\nI think the same thing happens with confidence intervals. The estimator has the property that 95% of the time it is constructed (under ideal circumstances) it will contain the estimand. But any one interval does or does not contain the estimand. And unlike the deck of cards which can easily be examined, we can’t ever know for certain if the interval successfully captured the estimand. There is no moment where we get to verify the estimand is in the confidence interval, and so we are sort of left guessing thus prompting us to offer a probability that we are right.\nThe mistake is benign. It hurts no one to think about confidence intervals as having a 95% probability of containing the estimand. Your company will not lose money, your paper will (hopefully) not be rejected, and the world will not end. That being said, it is unfortunately incorrect if not by appealing to the definition, then perhaps for other reasons.\nI’ll start with an appeal to authority. Sander Greenland and coauthors (who include notable epidemiologist Ken Rothman and motherfucking Doug Altman) include interpretation of a confidence interval as having 95% probability of containing the true effect as misconception 19 in this amazing paper. They note ” It is possible to compute an interval that can be interpreted as having 95% probability of containing the true value” but go on to say that this results in us doing a Bayesian analysis and computing a credible interval. If these guys are wrong, I don’t want to be right.\nAdditionally, when I say “The probability of a coin being flipped heads is 0.5” that references a long term frequency. I could, in principle, demonstrate that frequency by flipping a coin a lot and computing the empirical frequency of heads, which assuming the coin is fair and the number of flips large enough, will be within an acceptable range 0.5. To those people who say “This interval contains the estimand with 95% probability” I say “prove it”. Demonstrate to me via simulation or otherwise this long term relative frequency. I can’t imagine how this could be demonstrated because any fixed dataset will yield same answer over and over. Perhaps what supporters of this perspective mean is something closer to the Bayesian interpretation of probability (where probability is akin to strength in a belief). If so, the debate is decidedly answered because probability in frequentism is not about belief strength but about frequencies. Additionally, what is the random component in this probability? The data from the experiment are fixed, to allow these to vary is to appeal to my interpretation of the interval. If the estimand is random, then we are in another realm all together as frequentism assumes fixed parameters and random data. Maybe they mean something else which I just can’t think of. If there is something else, please let me know.\nI’ve gotten flack about confidence intervals on twitter."
  },
  {
    "objectID": "posts/2021-04-03-confidence-intervals/index.html#flack-1-framing-it-as-a-bet",
    "href": "posts/2021-04-03-confidence-intervals/index.html#flack-1-framing-it-as-a-bet",
    "title": "On Interpretations of Confidence Intervals",
    "section": "Flack 1: Framing It As A Bet",
    "text": "Flack 1: Framing It As A Bet\nYou present to me a shuffled deck with no funny business and offer me a bet in which I win X0,000 dollars if the card is an ace and lose X0 dollars if the card is not. “Aha Demetri! If you think the probability of the card on top being an ace is 0 or 1 you are either a fool for not taking the bet or are a fool for being so over confident! Your position is indefensible!” one person on twitter said to me (ok, they didn’t say it verbatim like this, but that was the intent).\nWell, not so fast. Nothing about my interpretation precludes me from using the answer to a simpler question to make decisions (I would argue statistics is the practice of doing jus that, but I digress). The top card is still an ace or not, but I can still think about an infinite sequence of shuffled decks anyway. In most of those scenarios, the card on top is an ace. Thus, I take the bet and hope the top card is an ace (much like I hope the confidence interval captures the true estimand, even though I know it either does or does not)."
  },
  {
    "objectID": "posts/2021-04-03-confidence-intervals/index.html#flack-2-my-next-interval-has-95-probability",
    "href": "posts/2021-04-03-confidence-intervals/index.html#flack-2-my-next-interval-has-95-probability",
    "title": "On Interpretations of Confidence Intervals",
    "section": "Flack 2: My Next Interval Has 95% Probability",
    "text": "Flack 2: My Next Interval Has 95% Probability\n“But Demetri, if 95% refers to the frequency of intervals containing the estimand, then surely my next interval has 95% probability of capturing the estimand prior to seeing data. Hence, individual intervals do have 95% probability of containing the estimand”.\nI get this sometimes, but don’t fully understand how it is supposed to be convincing. I see no problem with saying “the next interval has 95% probability” just like I have no problem with saying “If you shuffle those cards, the probability an ace is on top is 4/52” or “My next Roll Up The Rim cup has a 1 in 6 chance of winning”. This is starting to get more philosophical than I care it to, but those all reference non-existent things. Once they are brought into existence, it would be silly to think that they retain these properties. My cup is either winner or loser, even if I don’t roll it."
  },
  {
    "objectID": "posts/2021-04-03-confidence-intervals/index.html#flack-3-but-schrödingers-cat",
    "href": "posts/2021-04-03-confidence-intervals/index.html#flack-3-but-schrödingers-cat",
    "title": "On Interpretations of Confidence Intervals",
    "section": "Flack 3: But Schrödinger’s Cat…",
    "text": "Flack 3: But Schrödinger’s Cat…\nNo. Stop. This is not relevant in the least. I’m talking about cards and coins, not quarks or electrons. The Wikipedia article even says “Schrödinger did not wish to promote the idea of dead-and-live cats as a serious possibility; on the contrary, he intended the example to illustrate the absurdity of the existing view of quantum mechanics”. Cards can’t be and not-be aces until flipped. Get out of here."
  },
  {
    "objectID": "posts/2021-04-03-confidence-intervals/index.html#wrapping-up-dont-me",
    "href": "posts/2021-04-03-confidence-intervals/index.html#wrapping-up-dont-me",
    "title": "On Interpretations of Confidence Intervals",
    "section": "Wrapping Up, Don’t @ Me",
    "text": "Wrapping Up, Don’t @ Me\nTo be completely fair, I think the question about the cards I’ve presented to you is unfair. The question asks for a probability, and while 0 and 1 are valid probabilities, the question is phrased in a way so that you are prompted for a number between 0 and 1. Likewise, the name “95% confidence interval” begs for the wrong interpretation. That is the problem we face when we use language, which is naturally imprecise and full of shortcuts and ambiguity, to talk about things as precise as mathematics. It is a seminal case study in what I like to call the precision-usefulness trade off; precise statements are not useful. It is by, interpreting them and communicating them in common language that they become useful and that usefulness comes at the cost of precision (note, this explanation of the trade off is itself susceptible to the trade off). The important part is that we use confidence intervals to convey uncertainty in the estimate for which they are derived from. It isn’t important what you or I think about it, as the confidence interval is merely a means to an end.\nAS I noted, the mistake is benign, and these arguments are mostly a mental exercise than a fight against a method which may induce harm. Were it not for COVID19, I would encourage us all to go out for a beer and have these conversations rather than do it over twitter. Anyway, if you promise not to @ me anymore about this and I promise not to tweet about it anymore."
  },
  {
    "objectID": "posts/2021-11-23-bootstrap/index.html",
    "href": "posts/2021-11-23-bootstrap/index.html",
    "title": "Hacking Sklearn To Do The Optimism Corrected Bootstrap",
    "section": "",
    "text": "Its late, I can’t sleep, so I’m writing a blog post about the optimism corrected bootstrap.\nIn case you don’t know, epidemiology/biostatistics people working on prediction models like to validate their models in a slightly different way than your run-in-the-mill data scientist. Now, it should be unsurprising that this has generated some discussion between ML people and epi/biostats people, but I’m going to ignore this for now. I’m going to assume you have good reason for wanting to do the optimism corrected bootstrap in python, especially with sklearn, and if you don’t and want to discuss the pros and cons fo the method instead then lalalalalala I can’t hear you."
  },
  {
    "objectID": "posts/2021-11-23-bootstrap/index.html#the-optimism-corrected-bootstrap-in-7-steps",
    "href": "posts/2021-11-23-bootstrap/index.html#the-optimism-corrected-bootstrap-in-7-steps",
    "title": "Hacking Sklearn To Do The Optimism Corrected Bootstrap",
    "section": "The Optimism Corrected Bootstrap in 7 Steps",
    "text": "The Optimism Corrected Bootstrap in 7 Steps\nAs a primer, you might want to tread Alex Hayes’ pretty good blog post about variants of the bootstrap for predictive performance. It is more mathy than I care to be right now and in R should that be your thing.\nTo do the optimism corrected bootstrap, follow these 7 steps as found in Ewout W. Steyerberg’s Clinical Prediction Models.\n\nConstruct a model in the original sample; determine the apparent performance on the data from the sample used to construct the model.\nDraw a bootstrap sample (Sample*) with replacement from the original sample.\nConstruct a model (Model) in Sample, replaying every step that was done in the original sample, especially model specification steps such as selection of predictors. Determine the bootstrap performance as the apparent performance of Model* in Sample.\nApply Model* to the original sample without any modification to determine the test performance.\nCalculate the optimism as the difference between bootstrap performance and test performance.\nRepeat steps 1–4 many times, at least 200, to obtain a stable mean estimate of the optimism.\nSubtract the mean optimism estimate (step 6) from the apparent performance (step 1) to obtain the optimism-corrected performance estimate.\n\nThis procedure is very straight forward, and could easily be coded up from scratch, but I want to use as much existing code as I can and put sklearn on my resume, so let’s talk about what tools exist in sklearn to do cross validation and how we could use them to perform these steps."
  },
  {
    "objectID": "posts/2021-11-23-bootstrap/index.html#cross-validation-in-sklearn",
    "href": "posts/2021-11-23-bootstrap/index.html#cross-validation-in-sklearn",
    "title": "Hacking Sklearn To Do The Optimism Corrected Bootstrap",
    "section": "Cross Validation in Sklearn",
    "text": "Cross Validation in Sklearn\nWhen you pass arguments like cv=5 in sklearn’s many functions, what you’re really doing is passing 5 to sklearn.model_selection.KFold. See sklearn.model_selection.cross_validate which calls a function called ‘check_cv’ to verify this. KFold.split returns a generator, which when passed to next yields a pair of train and test indicides. The inner workings of KFold might look something like\nfor _ in range(number_folds):\n    train_ix = make_train_ix()\n    test_ix = make_test_ix()\n    yield (trian_ix, test_ix)\nThose incidies are used to slice X and y to do the cross validation. So, if we are going to hack sklearn to do the optimisim corrected bootstrap for us, we really just need to write a generator to give me a bunch of indicies. According to step 2 and 3 above, the train indicies need to be resamples of np.arange(len(X)) (ask yourself “why?”). According to step 4, the test indicies need to be np.arnge(len(X)) (again….”why?“).\nOnce we have a generator to do give us our indicies, we can use sklearn.model_selection.cross_validate to fit models on the resampled data and predict on the original sample (step 4). If we pass return_train_score=True to cross_validate we can get the bootstrap performances as well as the test performances (step 5). All we need to do then is calculate the average difference between the two (step 6) and then add this quantity to the apparent performance we got from step 1.\nThat all sounds very complex, but the code is decieptively simple."
  },
  {
    "objectID": "posts/2021-11-23-bootstrap/index.html#the-code-i-know-you-skipped-here-dont-lie",
    "href": "posts/2021-11-23-bootstrap/index.html#the-code-i-know-you-skipped-here-dont-lie",
    "title": "Hacking Sklearn To Do The Optimism Corrected Bootstrap",
    "section": "The Code (I Know You Skipped Here, Don’t Lie)",
    "text": "The Code (I Know You Skipped Here, Don’t Lie)\n\nimport numpy as np\nfrom numpy.core.fromnumeric import mean\nfrom sklearn.model_selection import cross_validate, RepeatedKFold\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.utils import resample\n\n# Need some data to predict with\ndata = load_diabetes()\nX, y = data['data'], data['target']\n\nclass OptimisimBootstrap():\n\n    def __init__(self, n_bootstraps):\n\n        self.n_bootstraps = n_bootstraps\n\n    def split(self, X, y,*_):\n\n        n = len(X)\n        test_ix = np.arange(n)\n\n        for _ in range(self.n_bootstraps):\n            train_ix = resample(test_ix)\n            yield (train_ix, test_ix)\n\n# Optimism Corrected\nmodel = LinearRegression()\nmodel.fit(X, y)\napparent_performance = mean_squared_error(y, model.predict(X))\n\nopt_cv = OptimisimBootstrap(n_bootstraps=250)\nmse = make_scorer(mean_squared_error)\ncv = cross_validate(model, X, y, cv=opt_cv, scoring=mse, return_train_score=True)\noptimism = cv['test_score'] - cv['train_score']\noptimism_corrected = apparent_performance + optimism.mean()\nprint(f'Optimism Corrected: {optimism_corrected:.2f}')\n\n# Compare against regular cv\ncv = cross_validate(model, X, y, cv = 10, scoring=mse)['test_score'].mean()\nprint(f'regular cv: {cv:.2f}')\n\n# Compare against repeated cv\ncv = cross_validate(model, X, y, cv = RepeatedKFold(n_splits=10, n_repeats=100), scoring=mse)['test_score'].mean()\nprint(f'regular cv: {cv:.2f}')\n\nOptimism Corrected: 2998.79\nregular cv: 3000.38\n\n\nregular cv: 3012.67\n\n\nThe two estimates (optimism corrected and 10 fold) should be reasonably close together, but uh don’t run this code multiple times. You might see that the optimism corrected estimate is quite noisy meaning I’m either wrong or that twitter thread I linked to might have some merit."
  },
  {
    "objectID": "posts/2022-05-07-flippin/index.html",
    "href": "posts/2022-05-07-flippin/index.html",
    "title": "Flippin’ Fun!",
    "section": "",
    "text": "I wrote an answer to a question about sequences of coin flips a couple days back that I was quite chuffed with. In short, the question asked for statistical ways to determine if a sequence of coin flips was from an unbiased coin or a human trying to appear random. The resulting model turned into a fun game on twitter centered around determining if people who follow me could simulate a sequence of coin flips that looked random (without using a real RNG, or some funny workaround. I have a lot of faith in my twitter followers…maybe too much).\nAnyway, then I thought “I should fit a hierarchical model to this data”. So that’s what I’m doing"
  },
  {
    "objectID": "posts/2022-05-07-flippin/index.html#initial-model",
    "href": "posts/2022-05-07-flippin/index.html#initial-model",
    "title": "Flippin’ Fun!",
    "section": "Initial Model",
    "text": "Initial Model\nTo understand the hierarchical model, we first need to understand the model I initially built. Let me give you a quick rundown on that.\nLet \\(S\\) be a sequence of bernoulli experiments so that \\(S_i\\) is either 1 or a 0 (a heads or a tails if you wish). The question I answered concerns detecting if a given sequence \\(S\\) could have been created by a human (or by a non-random process posing as random). I interpreted that as a call to estimate the lag-1 autocorrelation of the flips. The hypothesis being that humans probably perceive long streaks of one outcome or the other as signs of non-randomness and will intentionally switch the outcome if they feel the run is too long. I initially chose a Bayesian approach because I’m a glutton for punishment and someone else already gave a pretty good answer.\nThe model is quite straight forward to write down. Let \\(\\rho\\) be the correlation between \\(S_i\\) and \\(S_{i+1}\\), and let \\(q\\) be the expected number of heads in the sequence. We can write down the conditional probabilities that we see a 0/1 given the last element in the sequence was a 1/0. Those conditional probabilities are derived here and they are…\n\\[ P(1 \\vert 1) = q + \\rho(1-q) \\]\n\\[ P(1 \\vert 0) = q(1-\\rho) \\]\n\\[ P(0 \\vert 1) = (1-q)(1-\\rho) \\]\n\\[ P(0 \\vert 0) = 1 - q + \\rho \\cdot q \\]\nThe trick is to then count the subsequences of (1, 1), (1, 0), (0, 1), and (0, 0). Let \\(p_{i\\vert j} = P(i \\vert j)\\). We can then consider the count of each subsequence as multinomial\n\\[ y \\sim \\mbox{Multinomial}(\\theta) \\>. \\]\nHere, \\(\\theta\\) is the multinomial parameter, wherein each element is \\(\\theta = [p_{1 \\vert 1}, p_{1\\vert 0}, p_{0 \\vert 1}, p_{0\\vert 0}]\\). Equip this with a uniform prior on both \\(\\rho\\) and \\(q\\) and you’ve got yourself a model."
  },
  {
    "objectID": "posts/2022-05-07-flippin/index.html#the-stan-code",
    "href": "posts/2022-05-07-flippin/index.html#the-stan-code",
    "title": "Flippin’ Fun!",
    "section": "The Stan Code",
    "text": "The Stan Code\nThe Stan code for this model is quite easy to understand. The data block will consist of the counts of each type of subsequence. We can then concatenate those counts into an int of length 4 via the transformed data block. The concatenated counts will be what we pass to the multinomial likelihood.\ndata{\n  int y_1_1; //number of concurrent 1s\n  int y_0_1; //number of 0,1 occurences\n  int y_1_0; //number of 1,0 occurences\n  int y_0_0; //number of concurrent 0s\n}\ntransformed data{\n    int y[4] = {y_1_1, y_0_1, y_1_0, y_0_0};\n}\nThe only parameters we are interested in estimating are the autocorrelation rho and the coin’s bias q.\nparameters{\n  real<lower=-1, upper=1> rho;\n  real<lower=0, upper=1> q;\n}\nWe can derive the probabilities we need via the equations above, and that is a job for the transformed parameters block. We can then concatenate the conditional probabilities into a simplex data type object, theta to pass to the multinomial likelihood. Be careful though, we need to multiply theta by 0.5 since we are working with conditional probabilities. Note \\(p_{1\\vert 1} + p_{0 \\vert 1 } + p_{1\\vert 0} + p_{0 \\vert 0 } = 2\\), hence the scaling factor.\ntransformed parameters{\n  real<lower=0, upper=1> prob_1_1 = q + rho*(1-q);\n  real<lower=0, upper=1> prob_0_1 = (1-q)*(1-rho);\n  real<lower=0, upper=1> prob_1_0 = q*(1-rho);\n  real<lower=0, upper=1> prob_0_0 = 1 - q + rho*q;\n  simplex[4] theta = 0.5*[prob_1_1, prob_0_1, prob_1_0, prob_0_0 ]';\n}\nThe model call is then quite simple\nmodel{\n  q ~ beta(1, 1);\n  rho ~ uniform(-1, 1);\n  y ~ multinomial(theta); \n}\nand we can even generate new sequences based off the estimated parameters as a sort of posterior predictive check.\ngenerated quantities{\n    vector[300] yppc;\n    \n    yppc[1] = bernoulli_rng(q);\n    \n    for(i in 2:300){\n        if(yppc[i-1]==1){\n            yppc[i] = bernoulli_rng(prob_1_1);\n        }\n        else{\n        yppc[i] = bernoulli_rng(prob_1_0);\n        }\n    }\n}\nAll in all the model is\ndata{\n\n  int y_1_1; //number of concurrent 1s\n  int y_0_1; //number of 0,1 occurences\n  int y_1_0; //number of 1,0 occurences\n  int y_0_0; //number of concurrent 0s\n  \n}\ntransformed data{\n    int y[4] = {y_1_1, y_0_1, y_1_0, y_0_0};\n}\nparameters{\n  real<lower=-1, upper=1> rho;\n  real<lower=0, upper=1> q;\n}\ntransformed parameters{\n  real<lower=0, upper=1> prob_1_1 = q + rho*(1-q);\n  real<lower=0, upper=1> prob_0_1 = (1-q)*(1-rho);\n  real<lower=0, upper=1> prob_1_0 = q*(1-rho);\n  real<lower=0, upper=1> prob_0_0 = 1 - q + rho*q;\n  simplex[4] theta = 0.5*[prob_1_1, prob_0_1, prob_1_0, prob_0_0 ]';\n}\nmodel{\n  q ~ beta(1, 1);\n  rho ~ uniform(-1, 1);\n  y ~ multinomial(theta);\n  \n}\ngenerated quantities{\n    vector[300] yppc;\n    \n    yppc[1] = bernoulli_rng(q);\n    \n    for(i in 2:300){\n        if(yppc[i-1]==1){\n            yppc[i] = bernoulli_rng(prob_1_1);\n        }\n        else{\n        yppc[i] = bernoulli_rng(prob_1_0);\n        }\n    }\n}"
  },
  {
    "objectID": "posts/2022-05-07-flippin/index.html#run-from-python",
    "href": "posts/2022-05-07-flippin/index.html#run-from-python",
    "title": "Flippin’ Fun!",
    "section": "Run From Python",
    "text": "Run From Python\nWith the model written down, all we need to do is add some python code to create the counts of each subsequence and then run the stan model. Here is teh python code I used to create the response tweets for that game I ran on twitter.\n\nimport cmdstanpy\nimport matplotlib.pyplot as plt\n\ny_1_1 = 0 # count of (1, 1)\ny_0_0 = 0 # count of (0, 0)\ny_0_1 = 0 # count of (0, 1)\ny_1_0 = 0 # count of (1, 0)\n\nsequence = list('TTHHTHTTHTTTHTTTHTTTHTTHTHHTHHTHTHHTTTHHTHTHTTHTHHTTHTHHTHTTTHHTTHHTTHHHTHHTHTTHTHTTHHTHHHTTHTHTTTHHTTHTHTHTHTHTTHTHTHHHTTHTHTHHTHHHTHTHTTHTTHHTHTHTHTTHHTTHTHTTHHHTHTHTHTTHTTHHTTHTHHTHHHTTHHTHTTHTHTHTHTHTHTHHHTHTHTHTHHTHHTHTHTTHTTTHHTHTTTHTHHTHHHHTTTHHTHTHTHTHHHTTHHTHTTTHTHHTHTHTHHTHTTHTTHTHHTHTHTTT'.upper())\n\n# Do a rolling window trick I saw somewhere on twitter.\n# This implements a rollowing window of 2\n# In python 3.10, this would be a great use case for match\nfor pairs in zip(sequence[:-1], sequence[1:]):\n    if pairs == ('H','H'):\n        y_1_1 +=1\n    elif pairs == ('T','H'):\n        y_0_1 +=1\n    elif pairs == ('H', 'T'):\n        y_1_0 +=1\n    else:\n        y_0_0 +=1\n\n# Write the stan model as a string.  We will then write it to a file\nstan_code = '''\ndata{\n\n  int y_1_1; //number of concurrent 1s\n  int y_0_1; //number of 0,1 occurences\n  int y_1_0; //number of 1,0 occurences\n  int y_0_0; //number of concurrent 0s\n  \n}\ntransformed data{\n    int y[4] = {y_1_1, y_0_1, y_1_0, y_0_0};\n}\nparameters{\n  real<lower=-1, upper=1> rho;\n  real<lower=0, upper=1> q;\n}\ntransformed parameters{\n  real<lower=0, upper=1> prob_1_1 = q + rho*(1-q);\n  real<lower=0, upper=1> prob_0_1 = (1-q)*(1-rho);\n  real<lower=0, upper=1> prob_1_0 = q*(1-rho);\n  real<lower=0, upper=1> prob_0_0 = 1 - q + rho*q;\n  simplex[4] theta = 0.5*[prob_1_1, prob_0_1, prob_1_0, prob_0_0 ]';\n}\nmodel{\n  q ~ beta(1, 1);\n  rho ~ uniform(-1, 1);\n  y ~ multinomial(theta);\n  \n}\ngenerated quantities{\n    vector[300] yppc;\n    \n    yppc[1] = bernoulli_rng(q);\n    \n    for(i in 2:300){\n        if(yppc[i-1]==1){\n            yppc[i] = bernoulli_rng(prob_1_1);\n        }\n        else{\n        yppc[i] = bernoulli_rng(prob_1_0);\n        }\n    }\n}\n'''\n\n\n\n# Write the model to a temp file\nwith open('model_file.stan', 'w') as model_file:\n    model_file.write(stan_code)\n    \n# Compile the model\nmodel = cmdstanpy.CmdStanModel(stan_file='model_file.stan')\n\n# data to pass to Stan\ndata = dict(y_1_1 = y_1_1, y_0_0 = y_0_0, y_0_1 = y_0_1, y_1_0 = y_1_0)\n\n# Plotting stuff.\nfig, ax = plt.subplots(dpi = 120, ncols=2, figsize = (15, 5))\n\nax[0].set_title('Auto-correlation')\nax[1].set_title('Bias')\n\nax[0].set_xlim(-1, 1)\nax[1].set_xlim(0, 1)\n\nax[0].axvline(0, color = 'red')\nax[1].axvline(0.5, color = 'red')\n\nax[0].annotate('Uncorrelated Flips', xy=(0.475, 0.5), xycoords='axes fraction', rotation = 90)\nax[1].annotate('Unbiased Flips', xy=(0.475, 0.5), xycoords='axes fraction', rotation = 90)\n\n# MCMC go brrrr\nfit = model.sample(data)\n\nax[0].hist(fit.stan_variable('rho'), edgecolor='k', alpha = 0.5)\nax[1].hist(fit.stan_variable('q'), edgecolor='k', alpha = 0.5)\n\nautocorr = fit.stan_variable('rho').mean()\nbias = fit.stan_variable('q').mean()\n\ntweet = f\"Your flips have an expected correlation of {autocorr:.2f} and your coin's bias is about {bias:.2f}\"\n\nprint(f\"Your sequence was {''.join(sequence)}\")\nprint(tweet)\n\nINFO:cmdstanpy:compiling stan file /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file.stan to exe file /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file\n\n\nINFO:cmdstanpy:compiled model executable: /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file\n\n\nWARNING:cmdstanpy:Stan compiler has produced 1 warnings:\n\n\nWARNING:cmdstanpy:\n--- Translating Stan model to C++ code ---\nbin/stanc  --o=/Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file.hpp /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file.stan\nWarning in '/Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file.stan', line 11, column 4: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.32.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\n\n--- Compiling, linking C++ code ---\nclang++ -std=c++1y -Wno-unknown-warning-option -Wno-tautological-compare -Wno-sign-compare -D_REENTRANT -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.3.9 -I stan/lib/stan_math/lib/boost_1.75.0 -I stan/lib/stan_math/lib/sundials_6.0.0/include -I stan/lib/stan_math/lib/sundials_6.0.0/src/sundials    -DBOOST_DISABLE_ASSERTS          -c -include-pch stan/src/stan/model/model_header.hpp.gch -x c++ -o /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file.o /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file.hpp\nclang++ -std=c++1y -Wno-unknown-warning-option -Wno-tautological-compare -Wno-sign-compare -D_REENTRANT -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.3.9 -I stan/lib/stan_math/lib/boost_1.75.0 -I stan/lib/stan_math/lib/sundials_6.0.0/include -I stan/lib/stan_math/lib/sundials_6.0.0/src/sundials    -DBOOST_DISABLE_ASSERTS                -Wl,-L,\"/Users/demetri/.cmdstan/cmdstan-2.29.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/Users/demetri/.cmdstan/cmdstan-2.29.2/stan/lib/stan_math/lib/tbb\"      /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file.o src/cmdstan/main.o        -Wl,-L,\"/Users/demetri/.cmdstan/cmdstan-2.29.2/stan/lib/stan_math/lib/tbb\" -Wl,-rpath,\"/Users/demetri/.cmdstan/cmdstan-2.29.2/stan/lib/stan_math/lib/tbb\"   stan/lib/stan_math/lib/sundials_6.0.0/lib/libsundials_nvecserial.a stan/lib/stan_math/lib/sundials_6.0.0/lib/libsundials_cvodes.a stan/lib/stan_math/lib/sundials_6.0.0/lib/libsundials_idas.a stan/lib/stan_math/lib/sundials_6.0.0/lib/libsundials_kinsol.a  stan/lib/stan_math/lib/tbb/libtbb.dylib stan/lib/stan_math/lib/tbb/libtbbmalloc.dylib stan/lib/stan_math/lib/tbb/libtbbmalloc_proxy.dylib -o /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file\nrm -f /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file.o\n\n\n\nINFO:cmdstanpy:CmdStan start processing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                \n\n\n                                                                                \n\n\n                                                                                \n\n\n                                                                                \n\n\nINFO:cmdstanpy:CmdStan done processing.\n\n\n\n\n\nYour sequence was TTHHTHTTHTTTHTTTHTTTHTTHTHHTHHTHTHHTTTHHTHTHTTHTHHTTHTHHTHTTTHHTTHHTTHHHTHHTHTTHTHTTHHTHHHTTHTHTTTHHTTHTHTHTHTHTTHTHTHHHTTHTHTHHTHHHTHTHTTHTTHHTHTHTHTTHHTTHTHTTHHHTHTHTHTTHTTHHTTHTHHTHHHTTHHTHTTHTHTHTHTHTHTHHHTHTHTHTHHTHHTHTHTTHTTTHHTHTTTHTHHTHHHHTTTHHTHTHTHTHHHTTHHTHTTTHTHHTHTHTHHTHTTHTTHTHHTHTHTTT\nYour flips have an expected correlation of -0.36 and your coin's bias is about 0.49"
  },
  {
    "objectID": "posts/2022-05-07-flippin/index.html#stack-layerswait-thats-a-deep-learning-thing",
    "href": "posts/2022-05-07-flippin/index.html#stack-layerswait-thats-a-deep-learning-thing",
    "title": "Flippin’ Fun!",
    "section": "Stack Layers…Wait, That’s a Deep Learning Thing",
    "text": "Stack Layers…Wait, That’s a Deep Learning Thing\nNow it’s time to write a hierarchical model, and for that we need to be a little more careful. I initially thought I could just put priors on the population level autocorrelation and bias, but I quickly ran into a problem there, which I will illustrate below.\nSuppose the coin’s bias is 0 (we always get tails). Could the autocorrelation be -1? No, it couldn’t be, because that would mean our next flip would have to be a 1, but the coin’s bias is 0! This illustrates some nuance to the problem I had failed to consider but luckily did not suffer from. The autocorrelation for two binary random variables, \\(X, Y\\), is defined as\n\\[ \\rho = \\dfrac{\\alpha - q}{q(1-q)} \\]\nWhere \\(E(XY) = \\alpha\\). You see, \\(\\alpha\\) can only be so big depending on the value of \\(q\\), and if you place uniform priors on both \\(\\rho\\) and \\(q\\) you can quickly get conditional probabilities outside the unit interval. That’s exactly what happened, and I was banging my head against the wall for a night trying to figure out the bounds on \\(\\rho\\) given \\(q\\) and it quickly became a mess.\nThere is another way. Rather than place priors on \\(\\rho\\) and \\(q\\), we could place priors on the multinomial parameter and then do algebra (two equations, two unknowns) to find out expressions for \\(q\\) and \\(\\rho\\) in terms of \\(p_{1\\vert1}\\) and \\(p_{1\\vert0}\\). This isn’t ideal, because I have very good prior information on what \\(\\rho\\) and \\(q\\) should be, not on what \\(\\theta\\) should be. Whatever, let’s proceed and see how our priors look like with a prior predictive check.\nThe model is actually simpler to write in Stan than the previous model. We will place a Dirichlet prior on the multinomial parameters (one for each person who responded with a sequence), and then each sequence is multinomial with that multinomial parameter.\n\\[ \\theta_j \\sim \\mbox{Dirichlet}(\\alpha) \\]\n\\[ y_j \\sim \\mbox{Multinomial}(\\theta_j)  \\]\nThe quantities we care about can be expressed in terms of \\(\\theta\\)\n\\[ \\rho = p_{1\\vert 1} - p_{1\\vert 0}  \\]\n\\[ q = \\dfrac{2p_{1\\vert 0}}{1 - p_{1\\vert 1} + p_{1\\vert 0}}\\]\nHere is the model in Stan\ndata{\n\n    int N; //How many sequences do we have\n    int y[N, 4]; // matrix of counts of co-occurences of (1,1), (1,0), (0, 1), (0,0)\n    int do_sample; // Flag to do a prior predictive check\n\n}\nparameters{\n    vector<lower=0>[4] a;\n    simplex[4] theta[N];\n}\nmodel{\n  \n  a ~ cauchy(0, 2.5);\n  \n  if(do_sample>0){\n      for(i in 1:N){\n          theta[i] ~ dirichlet(a);\n          y[i] ~ multinomial(theta[i]);\n      }\n    }\n  \n}\ngenerated quantities{\n\n    vector[4] theta_ppc = dirichlet_rng(a);\n    real rho = theta_ppc[1] - theta_ppc[2];\n    real q = 2* theta_ppc[2]/(1 - theta_ppc[1] + theta_ppc[2]); \n    \n    real yppc[N, 4];\n    \n    for(i in 1:N){\n        yppc[i] = multinomial_rng(theta[i], sum(y[i]));\n    }\n}\nShown below are the priors for the autocorrelation and bias based on the priors I’ve used. They are a little too uncertain for my liking. Humans are pretty smart, and I don’t expect for the population average bias to be very far from 0.5. I would prefer that the prior for \\(q\\) be very tightly centered around 0.5, and that the prior for \\(\\rho\\) be tightly centered on 0, but that’s life. The model runs the 76 sequences in about 12 seconds (4 chains, 1000 warmups, 1000 samples) and diagnostics don’t indicate any pathological behavior. Let’s look at the joint posterior.\n\n\n\n\n\n\n\n(a) Priors\n\n\n\n\n\n\n\n(b) Posteriors\n\n\n\n\nFigure 1: Prior and Posterior distributions for the model.\n\n\nThe take home here is that the sequences are largely consistent with an unbiased and uncorrelated coin. The expected correlation is negative (-0.05) meaning humans are more likely to switch from heads to tails, or tails to heads, and the expected bias is 0.53 meaning people seem to favor heads for some reason. The results are largely unsurprising, and I really wish I could place priors on \\(\\rho\\) and \\(q\\) directly so that my model really does reflect the state of my knowledge, but this is good enough for now."
  },
  {
    "objectID": "posts/2022-05-07-flippin/index.html#conclusion",
    "href": "posts/2022-05-07-flippin/index.html#conclusion",
    "title": "Flippin’ Fun!",
    "section": "Conclusion",
    "text": "Conclusion\nI’ve been thinking about this problem for a while after I watched a talk by Gelman where he mentioned estimating the autocorrelation between bernoulli experiments in passing (he was talking about model criticism and offering examples of other things to check our model with). I’m moderately happy with the hierarchical model, and the results make a lot of sense. Humans are pretty smart, and we have an intuitive sense for what random looks like. I’m willing to bet that if people submitted longer sequences, we would have a more precise estimate of the bias/correlation.\nOne thing I’ve shirked is really detailed model criticism, though I have an idea of how I would do that. In that answer on cross validated, COOLSerdash posts a REALLY COOL way to visualize the sequence data. They plot the number of runs (sequences of consecutive heads or tails) against the longest run in the sequence. I think this would make for an excellent way to check that our model has learned the correct autocorrelation for each individual who participated in the game (though I think the sequences were too short to have a precise estimate)."
  }
]